{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed03097d",
   "metadata": {},
   "source": [
    "## JAX "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcf6f1f",
   "metadata": {},
   "source": [
    "### For now:\n",
    "\n",
    "* This feature is currently experimental and undergoing verification.\n",
    "* Benchmarks indicate significant speedups compared to the NumPy backend.\n",
    "* Full GPU support and optimization are planned but not yet finalized.\n",
    "* Requirement: Your objective function ($f$) must use jax.numpy instead of standard numpy to avoid errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c673c43",
   "metadata": {},
   "source": [
    "### Other backends:\n",
    "\n",
    "* The JAX is chosen as the primary acceleration backend due to its high compatibility and similar syntax to NumPy. \n",
    "* The core calculation logic is planned to be ported to other backends like PyTorch or TensorFlow to provide native GPU/TPU support and broader ecosystem integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b159b",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "* Compare with: `examples/optimization/2026-Jung/main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import specular\n",
    "import specular.jax as sjax\n",
    "from specular.optimization.step_size import StepSize\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "# ==========================================\n",
    "# 1. Objective function for only JAX\n",
    "# ==========================================\n",
    "def run_jax_experiment(trials=10, iteration=100, m=500, n=100, lambda1=0.1, lambda2=1.0):\n",
    "    \n",
    "    methods = [\"SPEG\", \"S-SPEG\", \"H-SPEG\"]\n",
    "    all_results = {m: [] for m in methods}\n",
    "    all_times = {m: [] for m in methods}\n",
    "\n",
    "    for t in tqdm(range(trials), desc=\"Trials\"):\n",
    "        key = jax.random.PRNGKey(t)\n",
    "        k1, k2, k3 = jax.random.split(key, 3)\n",
    "        \n",
    "        A = jax.random.normal(k1, (m, n))\n",
    "        b = jax.random.normal(k2, (m,))\n",
    "        x_0 = jax.random.normal(k3, (n,))\n",
    "\n",
    "        def f(x):\n",
    "            residual = jnp.dot(A, x) - b\n",
    "            loss = (1/(2*m)) * jnp.sum(residual**2)\n",
    "            reg = (lambda2/2) * jnp.sum(x**2) + lambda1 * jnp.sum(jnp.abs(x))\n",
    "            return loss + reg\n",
    "\n",
    "        # Stochastic Component\n",
    "        def f_j(x, idx):\n",
    "            term_data = (jnp.dot(A[idx], x) - b[idx])**2\n",
    "            term_reg = (lambda2/2) * jnp.sum(x**2) + lambda1 * jnp.sum(jnp.abs(x))\n",
    "            return 0.5 * term_data + term_reg\n",
    "\n",
    "        step_size = StepSize('square_summable_not_summable', [4.0, 0.0])\n",
    "\n",
    "        # --- 1. SPEG (Deterministic) ---\n",
    "        res = sjax.gradient_method(f, x_0, step_size, form='specular gradient', max_iter=iteration)\n",
    "        _, hist_f, runtime = res.history()\n",
    "        all_results[\"SPEG\"].append(np.array(hist_f))\n",
    "        all_times[\"SPEG\"].append(runtime)\n",
    "\n",
    "        # --- 2. S-SPEG (Stochastic) ---\n",
    "        res_s = sjax.gradient_method(f, x_0, step_size, form='stochastic', f_j=f_j, m=m, max_iter=iteration, seed=t)\n",
    "        _, hist_f_s, runtime_s = res_s.history()\n",
    "        all_results[\"S-SPEG\"].append(np.array(hist_f_s))\n",
    "        all_times[\"S-SPEG\"].append(runtime_s)\n",
    "\n",
    "        # --- 3. H-SPEG (Hybrid) ---\n",
    "        res_h = sjax.gradient_method(f, x_0, step_size, form='hybrid', f_j=f_j, m=m, switch_iter=10, max_iter=iteration, seed=t)\n",
    "        _, hist_f_h, runtime_h = res_h.history()\n",
    "        all_results[\"H-SPEG\"].append(np.array(hist_f_h))\n",
    "        all_times[\"H-SPEG\"].append(runtime_h)\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. Results\n",
    "    # ==========================================\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for name in methods:\n",
    "        data = np.array(all_results[name])\n",
    "        mean_curve = np.mean(data, axis=0)\n",
    "        std_curve = np.std(data, axis=0)\n",
    "        \n",
    "        x_axis = np.arange(len(mean_curve))\n",
    "        plt.plot(x_axis, mean_curve, label=name)\n",
    "        plt.fill_between(x_axis, mean_curve-std_curve, mean_curve+std_curve, alpha=0.2)\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel('Objective Value')\n",
    "    plt.title(f'JAX Backend Optimization (m={m}, n={n})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n[Avg Running Time]\")\n",
    "    for name in methods:\n",
    "        print(f\"{name}: {np.mean(all_times[name]):.4f}s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_jax_experiment(trials=20, iteration=1000, m=500, n=100, lambda1=100.0, lambda2=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
