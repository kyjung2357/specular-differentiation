{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Specular Differentiation","text":"<p>The Python package <code>specular</code> implements specular differentiation which generalizes classical differentiation. This implementation strictly follows the definitions, notations, and results in [1] and [2].</p> <p>A specular derivative (the red line) can be understood as the average of the inclination angles of the right and left derivatives.  In contrast, a symmetric derivative (the purple line) is the average of the right and left derivatives. Their difference is illustrated as in the following figure.</p> <p></p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Applications</li> <li>Documentation</li> <li>LaTeX macro</li> <li>Citing specular-differentiation</li> <li>References</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#requirements","title":"Requirements","text":"<p><code>specular-differentiation</code> requires:</p> <ul> <li>Python &gt;= 3.11</li> <li><code>ipython</code> &gt;= 8.12.3</li> <li><code>matplotlib</code> &gt;= 3.10.8</li> <li><code>numpy</code> &gt;= 2.4.0</li> <li><code>pandas</code> &gt;= 2.3.3</li> <li><code>tqdm</code> &gt;= 4.67.1</li> </ul>"},{"location":"#user-installation","title":"User installation","text":"<p>Standard Installation (NumPy backend)</p> <pre><code>pip install specular-differentiation\n</code></pre> <p>Advanced Installation (JAX backend)</p> <pre><code>pip install \"specular-differentiation[jax]\"\n</code></pre> <p>See the documentation for advanced installation (JAX backend, Pytest).</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>The following simple example calculates the specular derivative of the ReLU function $f(x) = max(0, x)$ at the origin.</p> <pre><code>import specular\n\nReLU = lambda x: max(x, 0)\nspecular.derivative(ReLU, x=0)\n# Output: 0.41421356237309515\n</code></pre>"},{"location":"#applications","title":"Applications","text":"<p>Specular differentiation is defined in normed vector spaces, allowing for applications in higher-dimensional Euclidean spaces.  The <code>specular</code> package includes the following applications.</p>"},{"location":"#ordinary-differential-equation","title":"Ordinary differential equation","text":"<ul> <li>Directory: <code>examples/ode/</code></li> <li>References: [1], [3], [4]</li> </ul> <p>In [1], seven schemes are proposed for solving ODEs numerically:</p> <ul> <li>the specular Euler scheme of Type 1~6</li> <li>the specular trigonometric scheme</li> </ul> <p>The following example shows that the specular Euler schemes of Type 5 and 6 yield more accurate numerical solutions than classical schemes: the explicit and implicit Euler schemes and the Crank-Nicolson scheme.</p> <p></p>"},{"location":"#optimization","title":"Optimization","text":"<ul> <li>Directory: <code>examples/optimization/</code></li> <li>References: [2], [5]</li> </ul> <p>In [2], three methods are proposed for optimizing nonsmooth convex objective functions:</p> <ul> <li>the specular gradient (SPEG) method</li> <li>the stochastic specular gradient (S-SPEG) method</li> <li>the hybrid specular gradient (H-SPEG) method</li> </ul> <p>The following example compares the three proposed methods with the classical methods: gradient descent (GD), Adaptive Moment Estimation (Adam), and Broyden-Fletcher-Goldfarb-Shanno (BFGS).</p> <p></p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>1. Getting Started</li> <li>2. API Reference</li> <li>3. Examples</li> </ul>"},{"location":"#latex-macro","title":"LaTeX macro","text":"<p>To use the specular differentiation symbol in your LaTeX document, add the following code to your preamble (before <code>\\begin{document}</code>):</p> <pre><code>% Required packages\n\\usepackage{graphicx}\n\\usepackage{bm}\n\n% Definition of Specular Differentiation symbol\n\\newcommand\\sd[1][.5]{\\mathbin{\\vcenter{\\hbox{\\scalebox{#1}{\\,$\\bm{\\wedge}$}}}}}\n</code></pre>"},{"location":"#usage-examples","title":"Usage examples","text":"<p>Use the symbol in your document (after <code>\\begin{document}</code>):</p> <pre><code>% A specular derivative in the one-dimensional Euclidean space\n$f^{\\sd}(x)$\n\n% A specular directional derivative in normed vector spaces\n$\\partial^{\\sd}_v f(x)$\n</code></pre>"},{"location":"#citing-specular-differentiation","title":"Citing specular-differentiation","text":"<p>To cite this repository:</p> <pre><code>@software{specular2026,\n  author  = {Kiyuob Jung},\n  title   = {specular-differentiation},\n  url     = {https://github.com/your-id/specular-differentiation},\n  version = {0.13.0},\n  year    = {2026},\n  doi     = {Pending},\n}\n</code></pre>"},{"location":"#references","title":"References","text":"<p>[1] K. Jung. Nonlinear numerical schemes using specular differentiation for initial value problems of first-order ordinary differential equations. arXiv preprint arXiv:????.?????, TBA.</p> <p>[2] K. Jung. Specular differentiation in normed vector spaces and its applications to nonsmooth convex optimization. arXiv preprint arXiv:????.?????, TBA. </p> <p>[3] K. Jung and J. Oh. The specular derivative. arXiv preprint arXiv:2210.06062, 2022.</p> <p>[4] K. Jung and J. Oh. The wave equation with specular derivatives. arXiv preprint arXiv:2210.06933, 2022.</p> <p>[5] K. Jung and J. Oh. Nonsmooth convex optimization using the specular gradient method with root-linear convergence. arXiv preprint arXiv:2210.06933, 2024.</p>"},{"location":"started/","title":"1. Getting Started","text":""},{"location":"started/#11-user-installation","title":"1.1. User installation","text":"<p>Standard Installation (NumPy backend)</p> <p>The package is available on PyPI:</p> <pre><code>pip install specular-differentiation\n</code></pre> <p>Check the version:</p> <pre><code>import specular\n\nprint(\"version: \", specular.__version__)\n# Output: version:  1.0.0\n</code></pre> <p>Advanced Installation (JAX backend)</p> <p>By default, the package uses the NumPy backend (CPU).  To enable hardware acceleration, you can install the package with the JAX backend (GPU/TPU).  This adds the following dependencies:</p> <ul> <li>JAX (<code>jax</code>, <code>jaxlib</code> &gt;= 0.4):</li> </ul> <pre><code>pip install \"specular-differentiation[jax]\"\n</code></pre> <p>Note</p> <p>This feature is experimental for now. See 2.4 JAX backend.</p> <p>Developer installation</p> <p>To install all dependencies including tests, docs, and examples. This adds the following dependencies:</p> <ul> <li>JAX (<code>jax</code>, <code>jaxlib</code> &gt;= 0.4):</li> <li>SciPy (<code>scipy</code> &gt;= 1.10.0)</li> <li>PyTorch (<code>torch</code> &gt;= 2.0.0)</li> <li>Pytest (<code>pytest</code> &gt;= 7.0)</li> </ul> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"started/#12-quick-start","title":"1.2. Quick start","text":"<p>The following simple example calculates the specular derivative of the ReLU function $f(x) = max(0, x)$ at the origin.</p> <pre><code>import specular\n\nReLU = lambda x: max(x, 0)\nspecular.derivative(ReLU, x=0)\n# Output: 0.41421356237309515\n</code></pre>"},{"location":"started/#13-jax-backend-usage","title":"1.3. JAX Backend Usage","text":"<p>To leverage JAX for hardware acceleration instead of the standard NumPy backend, import <code>specular.jax</code>:</p> <pre><code>import specular.jax as sjax\n\nReLU = lambda x: jax.numpy.maximum(x, 0)\nsjax.derivative(ReLU, 0.0)\n# Output: Array(0.41421354, dtype=float32)\n</code></pre> <p>To enable 64-bit precision (double precision), update the JAX configuration as follows:</p> <pre><code>import jax\njax.config.update(\"jax_enable_x64\", True)\n\nimport specular.jax as sjax\n\nReLU = lambda x: jax.numpy.maximum(x, 0)\nsjax.derivative(ReLU, 0.0)\n# Output: Array(0.41421356, dtype=float64)\n</code></pre>"},{"location":"api/","title":"2. API Reference","text":"<ul> <li> <p>2.1. Calculation     ---</p> <ul> <li><code>specular.calculation</code> </li> <li>2.1.2 the $n$-dimensional Euclidean space ($n&gt;1$)</li> </ul> </li> <li> <p>2.2 ODE     ---     \uc0c1\ubbf8\ubd84 \ubc29\uc815\uc2dd \ud480\uc774 \uad00\ub828 API\uc785\ub2c8\ub2e4.</p> </li> <li> <p>2.3 Optimization     ---     \ucd5c\uc801\ud654 \uc54c\uace0\ub9ac\uc998 \ubaa8\uc74c\uc785\ub2c8\ub2e4.</p> </li> <li> <p>2.4 JAX Backend     ---     JAX \uac00\uc18d \uae30\ub2a5\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.</p> </li> </ul>"},{"location":"api/calculation/","title":"2.1. Calculation","text":"<p>The <code>calculation</code> module provides five primary functions to calculate specular differentiation, depending on the dimension of input.</p>"},{"location":"api/calculation/#specular.ode.solver","title":"<code>specular.ode.solver</code>","text":"<p>============================================================== Numerical methods for solving ordinary differential equations ==============================================================</p> <p>Let the source function F:[t_0, T]xR -&gt; R be given, and the initial data u_0:R -&gt; R be given.  Consider the initial value problem: (IVP)              u'(t) = F(t, u(t)) with the initial condition u(t_0) = u_0(t_0). To solve (IVP) numerically, this module provides implementations of the specular Euler schemes, the Crank-Nicolson scheme, and the specular trigonometric scheme.</p>"},{"location":"api/calculation/#specular.ode.solver.Euler_scheme","title":"<code>Euler_scheme(of_Type, F, t_0, u_0, T, h=1e-06, u_1=False, tol=1e-06, zero_tol=1e-08, max_iter=100)</code>","text":"<p>Solves an initial value problem (IVP) using the specular Euler scheme of Type 1, 2, 3, 4, 5, and 6.</p>"},{"location":"api/calculation/#specular.ode.solver.Euler_scheme--parameters","title":"Parameters","text":"<p>of_Type : int | str     The type of the specular Euler scheme.     Options: <code>1</code>, <code>'1'</code>, <code>2</code>, <code>'2'</code>, <code>3</code>, <code>'3'</code>, <code>4</code>, <code>'4'</code>, <code>5</code>, <code>'5'</code>, <code>6</code>, <code>'6'</code>. F : callable     The given source function <code>F</code> in (IVP).     The calling signature should be <code>F(t, u)</code> where <code>t</code> and <code>u</code> are scalars. t_0 : float     The starting time of the simulation. u_0 : callable     The given initial condition <code>u_0</code> in (IVP). T : float     The end time of the simulation. h : float, optional     Mesh size used in the finite difference approximation. Must be positive.     Default: <code>1e-6</code>. u_1 : callable | float | bool     The numerical solution at the time <code>t_1 = t_0 + h</code> for Types 1, 2, and 3.     If a float or callable is provided, it is used as the exact value.     If False, the explicit Euler scheme is applied.     Default: <code>False</code>. tol : float | optional     Tolerance for fixed-point iteration     Used for Types 3, 4, 5, and 6. zero_tol : float | np.floating     A small threshold used to determine if the denominator (alpha + beta) is close to zero for numerical stability.     Default: <code>1e-6</code>. max_iter : int | optional     Max iterations for fixed-point solver.</p>"},{"location":"api/calculation/#specular.ode.solver.Euler_scheme--returns","title":"Returns","text":"<p>ODEResult     An object containing <code>(t, u)</code> data and the scheme name.</p> Source code in <code>specular\\ode\\solver.py</code> <pre><code>def Euler_scheme(\n    of_Type: int | str,\n    F: Callable[[float, float], float],\n    t_0: float, \n    u_0: Callable[[float], float] | float,\n    T: float, \n    h: float = 1e-6,\n    u_1: Callable[[float], float] | float | bool = False,\n    tol: float = 1e-6, \n    zero_tol: float = 1e-8,\n    max_iter: int = 100\n) -&gt; ODEResult:\n    \"\"\"\n    Solves an initial value problem (IVP) using the specular Euler scheme of Type 1, 2, 3, 4, 5, and 6.\n\n    Parameters\n    ----------\n    of_Type : int | str\n        The type of the specular Euler scheme.\n        Options: ``1``, ``'1'``, ``2``, ``'2'``, ``3``, ``'3'``, ``4``, ``'4'``, ``5``, ``'5'``, ``6``, ``'6'``.\n    F : callable\n        The given source function ``F`` in (IVP).\n        The calling signature should be ``F(t, u)`` where ``t`` and ``u`` are scalars.\n    t_0 : float\n        The starting time of the simulation.\n    u_0 : callable\n        The given initial condition ``u_0`` in (IVP).\n    T : float\n        The end time of the simulation.\n    h : float, optional\n        Mesh size used in the finite difference approximation. Must be positive.\n        Default: ``1e-6``.\n    u_1 : callable | float | bool\n        The numerical solution at the time ``t_1 = t_0 + h`` for Types 1, 2, and 3.\n        If a float or callable is provided, it is used as the exact value.\n        If False, the explicit Euler scheme is applied.\n        Default: ``False``.\n    tol : float | optional\n        Tolerance for fixed-point iteration\n        Used for Types 3, 4, 5, and 6.\n    zero_tol : float | np.floating\n        A small threshold used to determine if the denominator (alpha + beta) is close to zero for numerical stability.\n        Default: ``1e-6``.\n    max_iter : int | optional\n        Max iterations for fixed-point solver.\n\n    Returns\n    -------\n    ODEResult\n        An object containing ``(t, u)`` data and the scheme name.\n    \"\"\"\n    Type = str(of_Type)\n\n    scheme = 'specular Euler scheme of Type ' + Type\n    steps = int((T - t_0) / h)\n\n    all_history = {}\n    t_history = []\n    u_history = []\n\n    if Type in ['1', '2', '3']:\n        t_prev = t_0\n        u_prev = u_0(t_0) if callable(u_0) else u_0\n\n        t_history.append(t_prev)\n        u_history.append(u_prev)\n\n        t_curr = t_prev + h\n\n        if u_1 == False:\n            # explicit Euler to get u_1\n            u_curr = u_prev + h * F(t_prev, u_prev)\n        else:\n            u_curr = u_1\n\n        t_history.append(t_curr)\n        u_history.append(u_curr)\n\n        if Type == '1':\n            _of_Type_1(F, h, zero_tol, steps, t_prev, t_curr, u_prev, u_curr, t_history, u_history)\n\n        elif Type == '2':\n            _of_Type_2(F, h, zero_tol, steps, t_prev, t_curr, u_prev, u_curr, t_history, u_history)\n\n        elif Type == '3':\n            _of_Type_3(F, h, tol, zero_tol, max_iter, steps, t_prev, t_curr, u_prev, u_curr, t_history, u_history)\n\n    elif Type in ['4', '5', '6']:\n        t_curr = t_0\n        u_curr = u_0(t_0) if callable(u_0) else u_0  \n\n        t_history.append(t_curr)\n        u_history.append(u_curr)\n\n        if Type == '4':\n            _of_Type_4(F, h, tol, zero_tol, max_iter, steps, t_curr, u_curr, t_history, u_history)\n\n        elif Type == '5':\n            _of_Type_5(F, h, tol, zero_tol, max_iter, steps, t_curr, u_curr, t_history, u_history)\n\n        elif Type == '6':\n            _of_Type_6(F, h, tol, zero_tol, max_iter, steps, t_curr, u_curr, t_history, u_history)\n\n    else:\n        raise ValueError(f\"Unknown type. Got {of_Type}. Supported types: '1', '2', '3', '4', '5', and '6'\")\n\n    all_history[\"variables\"] = np.array(t_history)\n    all_history[\"values\"] = np.array(u_history)\n\n    return ODEResult(\n        scheme=scheme,\n        h=h,\n        all_history=all_history\n    )\n</code></pre>"},{"location":"api/calculation/#specular.ode.solver.classical_scheme","title":"<code>classical_scheme(F, t_0, u_0, T, h=1e-06, form='explicit Euler', tol=1e-06, max_iter=100)</code>","text":"<p>Solves an initial value problem (IVP) using classical numerical schemes. Supported forms: explicit Euler, implicit Euler, and Crank-Nicolson.</p>"},{"location":"api/calculation/#specular.ode.solver.classical_scheme--parameters","title":"Parameters","text":"<p>F : callable     The given source function <code>F</code> in (IVP).     The calling signature should be <code>F(t, u)</code> where <code>t</code> and <code>u</code> are scalars. t_0 : float     The starting time of the simulation. u_0 : callable     The given initial condition <code>u_0</code> in (IVP). T : float     The end time of the simulation. h : float, optional     Mesh size used in the finite difference approximation. Must be positive.     Default: <code>1e-6</code>. form : str | optional     The form of the numerical scheme.      Options: <code>'explicit_Euler'</code>, <code>'implicit_Euler'</code>, <code>'Crank-Nicolson'</code>.     Default: <code>'explicit_Euler'</code>. tol : float | optional     Tolerance for fixed-point iteration.     Used for implicit Euler and Crank-Nicolson schemes. max_iter : int | optional     Max iterations for fixed-point solver.</p>"},{"location":"api/calculation/#specular.ode.solver.classical_scheme--returns","title":"Returns","text":"<p>ODEResult     An object containing <code>(t, u)</code> data and the scheme name.</p> Source code in <code>specular\\ode\\solver.py</code> <pre><code>def classical_scheme(\n    F: Callable[[float, float], float], \n    t_0: float, \n    u_0: Callable[[float], float] | float,\n    T: float, \n    h: float = 1e-6,\n    form: str = 'explicit Euler',\n    tol: float = 1e-6, \n    max_iter: int = 100\n) -&gt; ODEResult:\n    \"\"\"\n    Solves an initial value problem (IVP) using classical numerical schemes.\n    Supported forms: explicit Euler, implicit Euler, and Crank-Nicolson.\n\n    Parameters\n    ----------\n    F : callable\n        The given source function ``F`` in (IVP).\n        The calling signature should be ``F(t, u)`` where ``t`` and ``u`` are scalars.\n    t_0 : float\n        The starting time of the simulation.\n    u_0 : callable\n        The given initial condition ``u_0`` in (IVP).\n    T : float\n        The end time of the simulation.\n    h : float, optional\n        Mesh size used in the finite difference approximation. Must be positive.\n        Default: ``1e-6``.\n    form : str | optional\n        The form of the numerical scheme. \n        Options: ``'explicit_Euler'``, ``'implicit_Euler'``, ``'Crank-Nicolson'``.\n        Default: ``'explicit_Euler'``.\n    tol : float | optional\n        Tolerance for fixed-point iteration.\n        Used for implicit Euler and Crank-Nicolson schemes.\n    max_iter : int | optional\n        Max iterations for fixed-point solver.\n\n    Returns\n    -------\n    ODEResult\n        An object containing ``(t, u)`` data and the scheme name.\n    \"\"\"\n    t_curr = t_0\n    u_curr = u_0(t_0) if callable(u_0) else u_0 \n\n    all_history = {}\n    t_history = [t_curr]\n    u_history = [u_curr]\n\n    steps = int((T - t_0) / h)\n\n    if form == \"explicit Euler\":\n        for _ in tqdm(range(steps), desc=\"Running the explicit Euler scheme\"):\n            t_curr, u_curr = t_curr + h, u_curr + h*F(t_curr, u_curr) # type: ignore\n\n            t_history.append(t_curr) \n            u_history.append(u_curr) \n\n    elif form == \"implicit Euler\":\n        for k in tqdm(range(steps), desc=\"Running the implicit Euler scheme\"):\n            t_next = t_curr + h\n\n            # Initial guess: explicit Euler \n            u_temp = u_curr + h*F(t_curr, u_curr) # type: ignore\n            u_guess = u_temp\n\n            # Fixed-point iteration\n            for _ in range(max_iter):\n                u_guess = u_curr + h*F(t_next, u_temp)\n                if np.linalg.norm(u_guess - u_temp) &lt; tol:\n                    break\n                u_temp = u_guess\n            else:\n                print(f\"Warning: step {k+1} did not converge.\")\n\n            t_curr, u_curr = t_next, u_guess  \n            t_history.append(t_curr)\n            u_history.append(u_curr)\n\n    elif form == \"Crank-Nicolson\":\n        for k in tqdm(range(steps), desc=\"Running Crank-Nicolson scheme\"):\n            t_next = t_curr + h\n\n            F_curr = F(t_curr, u_curr)\n\n            # Initial guess: explicit Euler\n            u_temp = u_curr + h * F_curr\n            u_guess = u_temp\n\n            # Fixed-point iteration\n            for _ in range(max_iter):\n                f_guess = F(t_next, u_temp)\n                u_guess = u_curr + 0.5 * h * (F_curr + f_guess)\n\n                if np.linalg.norm(u_guess - u_temp) &lt; tol:\n                    break\n\n                u_temp = u_guess\n            else:\n                print(f\"Warning: step {k+1} did not converge.\")\n\n            t_curr, u_curr = t_next, u_guess\n            t_history.append(t_curr)\n            u_history.append(u_curr)\n\n    else:\n        raise ValueError(f\"Unknown form '{form}'. Supported forms: {SUPPORTED_SCHEMES}\")\n\n    all_history[\"variables\"] = np.array(t_history)\n    all_history[\"values\"] = np.array(u_history)\n\n    return ODEResult(\n        scheme= form + \" scheme\",\n        h=h,\n        all_history=all_history\n    )\n</code></pre>"},{"location":"api/calculation/#specular.ode.solver.trigonometric_scheme","title":"<code>trigonometric_scheme(F, t_0, u_0, u_1, T, h=1e-06)</code>","text":"<p>Solves an initial value problem (IVP) using the specular trigonometric scheme.</p>"},{"location":"api/calculation/#specular.ode.solver.trigonometric_scheme--parameters","title":"Parameters","text":"<p>F : callable     The given source function <code>F</code> in (IVP).     The calling signature should be <code>F(t, u)</code> where <code>t</code> and <code>u</code> are scalars. t_0 : float     The starting time of the simulation. u_0 : callable     The given initial condition <code>u_0</code> in (IVP). u_1 : callable | float | bool     The numerical solution at the time <code>t_1 = t_0 + h</code> for Types 1, 2, and 3.     If a float or callable is provided, it is used as the exact value.     If False, the explicit Euler scheme is applied.     Default: <code>False</code>. T : float     The end time of the simulation. h : float, optional     Mesh size used in the finite difference approximation. Must be positive.     Default: <code>1e-6</code>.</p>"},{"location":"api/calculation/#specular.ode.solver.trigonometric_scheme--returns","title":"Returns","text":"<p>ODEResult     An object containing <code>(t, u)</code> data and the scheme name.</p> Source code in <code>specular\\ode\\solver.py</code> <pre><code>def trigonometric_scheme(\n    F: Callable[[float], float],\n    t_0: float, \n    u_0: Callable[[float], float] | float,\n    u_1: Callable[[float], float] | float,\n    T: float, \n    h: float = 1e-6\n) -&gt; ODEResult:\n    \"\"\"\n    Solves an initial value problem (IVP) using the specular trigonometric scheme.\n\n    Parameters\n    ----------\n    F : callable\n        The given source function ``F`` in (IVP).\n        The calling signature should be ``F(t, u)`` where ``t`` and ``u`` are scalars.\n    t_0 : float\n        The starting time of the simulation.\n    u_0 : callable\n        The given initial condition ``u_0`` in (IVP).\n    u_1 : callable | float | bool\n        The numerical solution at the time ``t_1 = t_0 + h`` for Types 1, 2, and 3.\n        If a float or callable is provided, it is used as the exact value.\n        If False, the explicit Euler scheme is applied.\n        Default: ``False``.\n    T : float\n        The end time of the simulation.\n    h : float, optional\n        Mesh size used in the finite difference approximation. Must be positive.\n        Default: ``1e-6``.\n\n    Returns\n    -------\n    ODEResult\n        An object containing ``(t, u)`` data and the scheme name.\n    \"\"\"\n    t_prev = t_0\n    u_prev = u_0(t_0) if callable(u_0) else u_0\n\n    t_curr = t_0 + h\n    u_curr = u_1(t_curr) if callable(u_1) else u_1\n\n    all_history = {}\n    t_history = [t_prev, t_curr]\n    u_history = [u_prev, u_curr]\n\n    steps = int((T - t_0) / h)\n\n    for m in tqdm(range(steps - 1), desc=\"Running specular trigonometric scheme\"):\n        t_next = t_curr + h\n        u_next = u_curr + h*math.tan(2*math.atan(F(t_curr, u_curr)) - math.atan((u_curr - u_prev) / h)) # type: ignore\n\n        t_history.append(t_next)\n        u_history.append(u_next)\n\n        t_prev, u_prev = t_curr, u_curr\n        t_curr, u_curr = t_next, u_next\n\n    all_history[\"variables\"] = np.array(t_history)\n    all_history[\"values\"] = np.array(u_history)\n\n    return ODEResult(\n        scheme=\"specular trigonometric scheme\",\n        h=h,\n        all_history=all_history\n    )\n</code></pre>"},{"location":"api/calculation/#quick-reference","title":"Quick Reference","text":"Function Space Description Input Type Output Type <code>derivative</code> $\\mathbb{R} \\to \\mathbb{R}^m$ specular derivative <code>float</code> <code>float</code>, <code>np.ndarray</code> <code>directional_derivative</code> $\\mathbb{R}^n \\to \\mathbb{R}$ specular directional derivative in direction $v \\in \\mathbb{R}^n$ <code>np.ndarry</code> <code>float</code> <code>partial_derivative</code> $\\mathbb{R}^n \\to \\mathbb{R}$ specular partial derivative w.r.t. $v = x_i$ <code>np.ndarray</code> <code>float</code> <code>gradient</code> $\\mathbb{R}^n \\to \\mathbb{R}$ specular gradient vector <code>np.ndarray</code> <code>np.ndarray</code> <code>jacobian</code> $\\mathbb{R}^n \\to \\mathbb{R}^m$ specular jacobian matrix <code>np.ndarray</code> <code>np.ndarray</code>"},{"location":"api/calculation/#211-one-dimensional-euclidean-space-n1","title":"2.1.1 One-dimensional Euclidean Space ($n=1$)","text":"<p>In $\u211d$, the specular derivative can be calculated using the function <code>derivative</code>.</p> <pre><code>import specular\n\ndef f(x):\n    return max(x, 0.0)\n\nspecular.derivative(f, x=0.0)\n# Output: 0.41421356237309515\n</code></pre>"},{"location":"api/calculation/#212-the-n-dimensional-euclidean-space-n1","title":"2.1.2 the $n$-dimensional Euclidean space ($n&gt;1$)","text":"<p>In $\u211d^n$, the specular directional derivative of a function $f: \u211d^n \\to \u211d$ at a point $x \\in \u211d^n$ in the direction $v \\in \u211d^n$ can be calculated using the function <code>directional_derivative</code>.</p> <pre><code>import specular\nimport math \n\nf = lambda x: math.sqrt(x[0]**2 + x[1]**2 + x[2]**2)\nspecular.directional_derivative(f, x=[0.0, 0.1, -0.1], v=[1.0, -1.0, 2.0])\n# Output: -2.1213203434708223\n</code></pre> <p>Let $e_1, e_2, \\ldots, e_n$ be the standard basis of $\u211d^n$. For each $i \\in \u2115$ with $1 \\leq i \\leq n$, the specular partial derivative with respect to a variable $x_i$ can be calculated using the function <code>partial_derivative</code>, which yields the same result as <code>directional_derivative</code> with direction $v=e_i$.</p> <pre><code>import specular\nimport math\n\ndef f(x):\n    return math.sqrt(x[0]**2 + x[1]**2 + x[2]**2)\n\nspecular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=2)\n# Output: 0.8859268982863702\nspecular.directional_derivative(f, x=[0.1, 2.3, -1.2], v=[0.0, 1.0, 0.0])\n# Output: 0.8859268982863702\n</code></pre> <p>Also, the specular gradient can be calculated using <code>gradient</code>.</p> <pre><code>import specular\nimport numpy as np\n\ndef f(x):\n    return np.linalg.norm(x)\n\nspecular.gradient(f, x=[0.1, 2.3, -1.2])\n# Output: [ 0.03851856  0.8859269  -0.46222273]\nspecular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=1)\n# Output: 0.03851856078540371\nspecular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=2)\n# Output: 0.8859268982863702\nspecular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=3)\n# Output: -0.4622227292028128\n</code></pre>"},{"location":"api/jax/","title":"2.4. JAX backend","text":"<p>See the official homepage of JAX.</p>"},{"location":"api/jax/#241-for-now","title":"2.4.1 For now","text":"<ul> <li>This feature is currently experimental and undergoing verification.</li> <li>Benchmarks indicate significant speedups compared to the NumPy backend.</li> <li>Full GPU support and optimization are planned but not yet finalized.</li> <li>Requirement: Your objective function must use <code>jax.numpy</code> instead of standard <code>numpy</code> to avoid errors.</li> </ul>"},{"location":"api/jax/#242-why-jax","title":"2.4.2 Why JAX?","text":"<ul> <li>The JAX is chosen as the primary acceleration backend due to its high compatibility and similar syntax to NumPy. </li> <li>The core calculation logic is planned to be ported to other backends like PyTorch or TensorFlow to provide native GPU/TPU support and broader ecosystem integration.</li> </ul>"},{"location":"api/jax/#243-example","title":"2.4.3 Example","text":"<p>For a detailed comparison of the algorithms, please refer to the following scripts:</p> <ul> <li> <p><code>docs/api/jax_example.py</code>: A basic implementation using the JAX backend.</p> </li> <li> <p><code>examples/optimization/2026-Jung/main.py</code>: The full experimental setup used in the paper.</p> </li> </ul> <p>Note</p> <p>Preliminary tests show that the computation time is shorter than BFGS; however, the exact theoretical reasons for this performance gain are still being investigated and have not yet been fully proven.</p>"},{"location":"api/ode/","title":"2.2. Ordinary differential equations","text":"<p>Let the source function $F:[t_0, T] \\times \u211d \\to \u211d$ be given, and the initial data $u_0:\u211d \\to \u211d$ be given.  Consider the initial value problem:</p> <p>$$ u'(t) = F(t, u(t)) $$ </p> <p>with the initial condition $u(t_0) = u_0(t_0)$. To solve the problem numerically, the subpackage <code>ode</code> provides the following methods:</p> <ul> <li>the specular Euler scheme (Type 1 ~ 6)</li> <li>the specular trigonometric scheme</li> <li>the explicit Euler scheme</li> <li>the implicit Euler scheme</li> <li>the Crank-Nicolson scheme</li> </ul>"},{"location":"api/ode/#221-specular-euler-scheme","title":"2.2.1 Specular Euler scheme","text":"<p>All functions return an instance of the <code>ODEResult</code> class that encapsulates the numerical results.</p> <pre><code>import specular\n\ndef F(t, u):\n    return -2*u \n\nspecular.Euler_scheme(of_Type='1', F=F, t_0=0.0, u_0=1.0, T=2.5, h=0.1)\n# Output: Running the specular Euler scheme of Type 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00&lt;?, ?it/s]\n# Output: &lt;specular.ode.result.ODEResult at 0x1765982d8d0&gt;\n</code></pre> <p>To access the numerical results, call <code>.history()</code>. It returns a tuple containing the time grid and the numerical solution.</p> <pre><code>import specular\n\ndef F(t, u):\n    return -2*u \n\nspecular.Euler_scheme(of_Type=1, F=F, t_0=0.0, u_0=1.0, T=2.5, h=0.1).history()\n# Output: Running the specular Euler scheme of Type 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00&lt;?, ?it/s]\n# Output: (array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2,\n#        1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5]),\n# Output:  array([1.        , 0.8       , 0.62169432, 0.48101574, 0.37172557,\n#        0.2870388 , 0.22149069, 0.17081087, 0.13166787, 0.1014624 ,\n#        0.07816953, 0.06021577, 0.04638162, 0.0357239 , 0.02751427,\n#        0.02119088, 0.01632056, 0.0125695 , 0.00968054, 0.00745555,\n#        0.00574195, 0.00442221, 0.00340579, 0.00262299, 0.00202011,\n#        0.0015558 ]))\n</code></pre> <p>To visualize the numerical results, call <code>.visualization()</code>.</p> <pre><code>import specular\nimport numpy as np\n\ndef F(t, u):\n   return -2*u \n\ndef exact_sol(t):\n    return np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nspecular.Euler_scheme(of_Type='1', F=F, t_0=0.0, u_0=u_0, T=2.5, h=0.1).visualization(exact_sol=exact_sol, save_path=\"specular-Euler-scheme-of-Type-1\")\n# Output: Running the specular Euler scheme of Type 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00&lt;?, ?it/s]\n# Output: Figure saved: figures\\specular-Euler-scheme-of-Type-1\n</code></pre> <p></p> <p>To obtain the table of the numerical results, call <code>.table()</code>. </p> <pre><code>import specular\nimport numpy as np\n\ndef F(t, u):\n    return -2*u\n\ndef exact_sol(t):\n    return np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nspecular.Euler_scheme(of_Type=4, F=F, t_0=0.0, u_0=u_0, T=2.5, h=0.1).table(exact_sol=exact_sol, save_path=\"specular-Euler-scheme-of-type-4\")\n# Output: Running the specular Euler scheme of Type 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00&lt;?, ?it/s]\n# Output: Table saved: tables\\specular-Euler-scheme-of-type-4.csv\n</code></pre> <p><code>.visualization()</code> and <code>.table()</code> are are chainable.</p> <pre><code>import specular\nimport numpy as np\n\ndef F(t, u):\n    return -2*u\n\ndef exact_sol(t):\nreturn np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nspecular.Euler_scheme(of_Type=4, F=F, t_0=0.0, u_0=u_0, T=2.5, h=0.1).visualization(exact_sol=exact_sol).table(exact_sol=exact_sol)\n# Output: Running the specular Euler scheme of Type 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00&lt;?, ?it/s]\n</code></pre> <p>To compute the total error of the numerical results, call <code>.total_error()</code>. The exact solution is required. The norm can be <code>max</code>, <code>l1</code>, or <code>l2</code>.</p> <pre><code>def F(t, u):\n    return -2*u \n\ndef exact_sol(t):\n    return np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nspecular.Euler_scheme(of_Type=5, F=F, t_0=0.0, u_0=u_0, T=10.0, h=0.1).total_error(exact_sol=exact_sol, norm='max')\n# Output: Running the specular Euler scheme of Type 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 300882.64it/s]\n# Output: 0.0011409613137273178\n</code></pre>"},{"location":"api/ode/#222-specular-trigonometric-scheme","title":"2.2.2 Specular trigonometric scheme","text":"<pre><code>import specular\n\ndef F(t, u):\n    return -2*u \n\ndef exact_sol(t):\n    return np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nu_1 = exact_sol(t_0 + h)\n\nspecular.trigonometric_scheme(F=F, t_0=0.0, u_0=u_0, u_1=u_1, T=2.5, h=0.1).visualization(exact_sol=exact_sol, save_path=\"specular-trigonometric\")\n# Output: Running specular trigonometric scheme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00&lt;?, ?it/s]\n# Output: Figure saved: figures\\specular-trigonometric\n</code></pre>"},{"location":"api/ode/#223-classical-schemes","title":"2.2.3 Classical schemes","text":"<p>The three classical schemes are available: the explicit Euler, the implicit Euler, and the Crank-Nicolson schemes.</p> <pre><code>import specular\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef F(t, u):\n    return -(t*u)/(1-t**2)\ndef exact_sol(t):\n    return np.sqrt(1 - t**2)\ndef u_0(t_0):\n    return exact_sol(t_0)\nt_0 = 0.0\nT = 0.9\nh = 0.05\n\nresult_EE = specular.ode.solver.classical_scheme(F=F, t_0=t_0, u_0=u_0, T=T, h=h, scheme=\"explicit Euler\").history()\nresult_IE = specular.ode.solver.classical_scheme(F=F, t_0=t_0, u_0=u_0, T=T, h=h, scheme=\"implicit Euler\").history()\nresult_CN = specular.ode.solver.classical_scheme(F=F, t_0=t_0, u_0=u_0, T=T, h=h, scheme=\"Crank-Nicolson\").history()\nexact_values = np.array([exact_sol(t) for t in result_EE[0]])\n\nplt.figure(figsize=(5.5, 2.5))\n\nplt.plot(result_EE[0], exact_values, color='black', label='Exact solution')\nplt.plot(result_EE[0], result_EE[1],  marker='x', linestyle='None', markerfacecolor='none', markeredgecolor='red', label='Explicit Euler') \nplt.plot(result_IE[0], result_IE[1],  marker='x', linestyle='None', markerfacecolor='none', markeredgecolor='blue', label='Implicit Euler') \nplt.plot(result_CN[0], result_CN[1],  marker='x', linestyle='None', markerfacecolor='none', markeredgecolor='purple', label='Crank-Nicolson')\n\nplt.xlabel(r\"Time\", fontsize=10)\nplt.ylabel(r\"Solution\", fontsize=10)\nplt.grid(True)\nplt.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), borderaxespad=0., fontsize=10)\nplt.savefig('figures/classical-schemes.png', dpi=1000, bbox_inches='tight')\nplt.show()\n# Output: Running the explicit Euler scheme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00&lt;?, ?it/s]\n# Output: Running the implicit Euler scheme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00&lt;?, ?it/s]\n# Output: Running Crank-Nicolson scheme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00&lt;00:00, 17988.44it/s]\n</code></pre> <p></p>"},{"location":"api/optimization/","title":"2.3. Optimization","text":"<p>Consider the optimization problem:</p> <p>$$ \\min_{x \\in \\mathbb{R}^n} f(x), $$</p> <p>where $f:\\mathbb{R}^n \\to \\mathbb{R}$ is convex. To solve the problem numerically, the module The <code>optimization</code> subpackage  provides the following methods:</p> <ul> <li>the specular gradient (SPEG) method</li> <li>the stochastic specular gradient (S-SPEG) method</li> <li>the hybrid specular gradient (H-SPEG) method</li> </ul> <p>Given an initial point $x_0 \\in \\mathbb{R}^n$, method takes the form:  $$ x_{k+1} = x_k - h_k s_k, $$ where $h_k &gt; 0$ is the step size and $s_k$ is the specular gradient for each $k \\in \\mathbb{N}$.</p>"},{"location":"api/optimization/#231-step-size-rules-stepsize","title":"2.3.1 Step Size Rules (<code>StepSize</code>)","text":"<p>The <code>StepSize</code> class defines how the step size $h_k$ evolves during optimization: $x_{k+1} = x_k - h_k s_k$</p>"},{"location":"api/optimization/#available-options","title":"Available Options","text":"Name Rule (Formula) Type Input Description <code>constant</code> $h_k = a$ <code>float</code> <code>a</code> Fixed step size for all $k$. <code>not_summable</code> $h_k = a / \\sqrt{k}$ <code>float</code> <code>a</code> $\\lim_{k \\to \\infty }h_k = 0$, but $\\sum h_k = \\infty$. <code>square_summable_not_summable</code> $h_k = a / (b + k)$ <code>list</code> <code>[a, b]</code> $\\sum h_k^2 &lt; \\infty$ and $\\sum h_k = \\infty$. <code>geometric_series</code> $h_k = a \\cdot r^k$ <code>list</code> <code>[a, r]</code> Exponentially decaying step size. <code>user_defined</code> Custom <code>Callable</code> <code>f(k)</code> User-provided function of iteration $k$."},{"location":"api/optimization/#test","title":"TEST","text":"<p>Step size rules for optimization methods.</p> Source code in <code>specular\\optimization\\step_size.py</code> <pre><code>class StepSize:\n    \"\"\"\n    Step size rules for optimization methods.\n    \"\"\"\n    __options__ = [\n        'constant',\n        'not_summable',\n        'square_summable_not_summable',\n        'geometric_series',\n        'user_defined'\n    ]\n\n    def __init__(\n        self,\n        name: str,\n        parameters: float | np.floating | int | Tuple | list | np.ndarray | Callable\n    ):\n        \"\"\"\n        The step size rules for optimization methods:\n\n        :math:`x_{k+1} = x_k - h_k s_k`,\n\n        where :math:`s_k` is the search direction and :math:`h_k &gt; 0` is the step size at iteration `k &gt;= 1`.\n\n        Parameters\n        ----------\n        name : str\n            Options: 'constant', 'not_summable', 'square_summable_not_summable', 'geometric_series', 'user_defined'\n        parameters : float | int | tuple | list | np.ndarray | Callable\n            The parameters required for the selected step size rule:\n\n            * 'constant': float or int\n\n                A number `a &gt; 0` for the rule :math:`h_k = a` for each `k`.\n\n            * 'not_summable': float or int\n\n                A number `a &gt; 0` for the rule :math:`h_k = a / sqrt{k}` for each `k`.\n\n            * 'square_summable_not_summable': list or tuple\n\n                A pair of numbers `[a, b]`, where `a &gt; 0` and `b &gt;= 0`, for the rule :math:`h_k = a / (b + k)` for each `k`.\n\n            * 'geometric_series': list or tuple\n\n                A pair of numbers `[a, r]`, where `a &gt; 0` and `0 &lt; r &lt; 1`, for the rule :math:`h_k = a * r^k` for each `k`.\n\n            * 'user_defined': Callable\n\n                A function that takes the current iteration `k` as input and returns the step size (float).\n\n        Examples\n        --------\n        &gt;&gt;&gt; from specular.optimization.step_size import StepSize\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'constant': h_k = a\n        &gt;&gt;&gt; step = StepSize(name='constant', parameters=0.5)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'not_summable' rule: h_k = a / sqrt(k)\n        &gt;&gt;&gt; # a = 2.0\n        &gt;&gt;&gt; step = StepSize(name='not_summable', parameters=2.0)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'square_summable_not_summable' rule: h_k = a / (b + k\n        &gt;&gt;&gt; # a = 10, b = 2\n        &gt;&gt;&gt; step = StepSize(name='square_summable_not_summable', parameters=[10.0, 2.0])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'geometric_series' rule: h_k = a * r^k\n        &gt;&gt;&gt; # a = 1.0, r = 0.5\n        &gt;&gt;&gt; step = StepSize(name='geometric_series', parameters=[1.0, 0.5])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'user_defined' callable.\n        &gt;&gt;&gt; # Custom rule: h_k = 1 / k^2\n        &gt;&gt;&gt; custom_rule = lambda k: 1.0 / (k**2)\n        &gt;&gt;&gt; step = StepSize(name='user_defined', parameters=custom_rule)\n        \"\"\"\n        self.step_size = name\n        self.parameters = parameters\n\n        init_methods = {\n            'constant': self._init_constant,\n            'not_summable': self._init_not_summable,\n            'square_summable_not_summable': self._init_square_summable,\n            'geometric_series': self._init_geometric,\n            'user_defined': self._init_user_defined\n        }\n\n        if name not in init_methods:\n             raise ValueError(f\"Invalid step size '{name}'. Options: {self.__options__}\")\n\n        init_methods[name]()\n\n    def __call__(self, k: int) -&gt; float:\n        \"\"\"\n        Returns the step size at iteration k.\n        \"\"\"\n        return self._rule(k)\n\n    # ==== Initialization Methods ====\n    def _init_constant(self):\n        if not isinstance(self.parameters, (float, int, np.floating)):\n            raise TypeError(f\"Invalid type: number required. Got {type(self.parameters)}\")\n\n        if self.parameters &lt;= 0:\n            raise ValueError(f\"Invalid value: positive number required. Got {self.parameters}\")\n\n        self.a = float(self.parameters)\n        self._rule = self._calc_constant\n\n    def _init_not_summable(self):\n        if not isinstance(self.parameters, (float, int, np.floating)):\n            raise TypeError(f\"Invalid type: number required. Got {type(self.parameters)}\")\n\n        if self.parameters &lt;= 0:\n            raise ValueError(f\"Invalid value: positive number required. Got {self.parameters}\")\n\n        self.a = float(self.parameters)\n        self._rule = self._calc_not_summable\n\n    def _init_square_summable(self):\n        if not isinstance(self.parameters, (tuple, list, np.ndarray)):\n            raise TypeError(f\"Invalid type: list/tuple required. Got {type(self.parameters)}\")\n\n        if len(self.parameters) != 2:\n            raise ValueError(f\"Invalid length: 2 parameters [a, b] required. Got {len(self.parameters)}\")\n\n        self.a, self.b = self.parameters[0], self.parameters[1]\n\n        if self.a &lt;= 0 or self.b &lt; 0:\n            raise ValueError(f\"Invalid parameters: a &gt; 0 and b &gt;= 0 required. Got a={self.a}, b={self.b}\")\n\n        self._rule = self._calc_square_summable_not_summable\n\n    def _init_geometric(self):\n        if not isinstance(self.parameters, (tuple, list, np.ndarray)):\n            raise TypeError(f\"Invalid type: list/tuple required. Got {type(self.parameters)}\")\n\n        if len(self.parameters) != 2:\n            raise ValueError(f\"Invalid length: 2 parameters [a, r] required. Got {len(self.parameters)}\")\n\n        self.a, self.r = self.parameters[0], self.parameters[1]\n\n        if self.a &lt;= 0 or not (0.0 &lt; self.r &lt; 1.0):\n            raise ValueError(f\"Invalid parameters: a &gt; 0 and 0 &lt; r &lt; 1 required. Got a={self.a}, r={self.r}\")\n\n        self._rule = self._calc_geometric_series\n\n    def _init_user_defined(self):\n        if not callable(self.parameters):\n            raise TypeError(\"Invalid type: callable function required.\")\n\n        self._rule = self.parameters\n\n    # ==== Calculation Methods ====\n    def _calc_constant(self, k: int) -&gt; float:\n        \"\"\"\n        h_k = a \n        \"\"\"\n        return self.a\n\n    def _calc_not_summable(self, k: int) -&gt; float:\n        \"\"\"\n        h_k = a / sqrt{k}\n        \"\"\"\n        return self.a / math.sqrt(k)\n\n    def _calc_square_summable_not_summable(self, k: int) -&gt; float:\n        \"\"\"\n        h_k = a / (b + k)\n        \"\"\"\n        return self.a / (self.b + k)\n\n    def _calc_geometric_series(self, k: int) -&gt; float:\n        \"\"\"\n        h_k = a * r**k\n        \"\"\"\n        return self.a * (self.r ** k)\n</code></pre>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__","title":"<code>__init__(name, parameters)</code>","text":"<p>The step size rules for optimization methods:</p> <p>:math:<code>x_{k+1} = x_k - h_k s_k</code>,</p> <p>where :math:<code>s_k</code> is the search direction and :math:<code>h_k &gt; 0</code> is the step size at iteration <code>k &gt;= 1</code>.</p>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--parameters","title":"Parameters","text":"<p>name : str     Options: 'constant', 'not_summable', 'square_summable_not_summable', 'geometric_series', 'user_defined' parameters : float | int | tuple | list | np.ndarray | Callable     The parameters required for the selected step size rule:</p> <pre><code>* 'constant': float or int\n\n    A number `a &gt; 0` for the rule :math:`h_k = a` for each `k`.\n\n* 'not_summable': float or int\n\n    A number `a &gt; 0` for the rule :math:`h_k = a / sqrt{k}` for each `k`.\n\n* 'square_summable_not_summable': list or tuple\n\n    A pair of numbers `[a, b]`, where `a &gt; 0` and `b &gt;= 0`, for the rule :math:`h_k = a / (b + k)` for each `k`.\n\n* 'geometric_series': list or tuple\n\n    A pair of numbers `[a, r]`, where `a &gt; 0` and `0 &lt; r &lt; 1`, for the rule :math:`h_k = a * r^k` for each `k`.\n\n* 'user_defined': Callable\n\n    A function that takes the current iteration `k` as input and returns the step size (float).\n</code></pre>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--examples","title":"Examples","text":"<p>from specular.optimization.step_size import StepSize</p> Source code in <code>specular\\optimization\\step_size.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    parameters: float | np.floating | int | Tuple | list | np.ndarray | Callable\n):\n    \"\"\"\n    The step size rules for optimization methods:\n\n    :math:`x_{k+1} = x_k - h_k s_k`,\n\n    where :math:`s_k` is the search direction and :math:`h_k &gt; 0` is the step size at iteration `k &gt;= 1`.\n\n    Parameters\n    ----------\n    name : str\n        Options: 'constant', 'not_summable', 'square_summable_not_summable', 'geometric_series', 'user_defined'\n    parameters : float | int | tuple | list | np.ndarray | Callable\n        The parameters required for the selected step size rule:\n\n        * 'constant': float or int\n\n            A number `a &gt; 0` for the rule :math:`h_k = a` for each `k`.\n\n        * 'not_summable': float or int\n\n            A number `a &gt; 0` for the rule :math:`h_k = a / sqrt{k}` for each `k`.\n\n        * 'square_summable_not_summable': list or tuple\n\n            A pair of numbers `[a, b]`, where `a &gt; 0` and `b &gt;= 0`, for the rule :math:`h_k = a / (b + k)` for each `k`.\n\n        * 'geometric_series': list or tuple\n\n            A pair of numbers `[a, r]`, where `a &gt; 0` and `0 &lt; r &lt; 1`, for the rule :math:`h_k = a * r^k` for each `k`.\n\n        * 'user_defined': Callable\n\n            A function that takes the current iteration `k` as input and returns the step size (float).\n\n    Examples\n    --------\n    &gt;&gt;&gt; from specular.optimization.step_size import StepSize\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # 'constant': h_k = a\n    &gt;&gt;&gt; step = StepSize(name='constant', parameters=0.5)\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # 'not_summable' rule: h_k = a / sqrt(k)\n    &gt;&gt;&gt; # a = 2.0\n    &gt;&gt;&gt; step = StepSize(name='not_summable', parameters=2.0)\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # 'square_summable_not_summable' rule: h_k = a / (b + k\n    &gt;&gt;&gt; # a = 10, b = 2\n    &gt;&gt;&gt; step = StepSize(name='square_summable_not_summable', parameters=[10.0, 2.0])\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # 'geometric_series' rule: h_k = a * r^k\n    &gt;&gt;&gt; # a = 1.0, r = 0.5\n    &gt;&gt;&gt; step = StepSize(name='geometric_series', parameters=[1.0, 0.5])\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # 'user_defined' callable.\n    &gt;&gt;&gt; # Custom rule: h_k = 1 / k^2\n    &gt;&gt;&gt; custom_rule = lambda k: 1.0 / (k**2)\n    &gt;&gt;&gt; step = StepSize(name='user_defined', parameters=custom_rule)\n    \"\"\"\n    self.step_size = name\n    self.parameters = parameters\n\n    init_methods = {\n        'constant': self._init_constant,\n        'not_summable': self._init_not_summable,\n        'square_summable_not_summable': self._init_square_summable,\n        'geometric_series': self._init_geometric,\n        'user_defined': self._init_user_defined\n    }\n\n    if name not in init_methods:\n         raise ValueError(f\"Invalid step size '{name}'. Options: {self.__options__}\")\n\n    init_methods[name]()\n</code></pre>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--constant-h_k-a","title":"'constant': h_k = a","text":"<p>step = StepSize(name='constant', parameters=0.5)</p>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--not_summable-rule-h_k-a-sqrtk","title":"'not_summable' rule: h_k = a / sqrt(k)","text":""},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--a-20","title":"a = 2.0","text":"<p>step = StepSize(name='not_summable', parameters=2.0)</p>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--square_summable_not_summable-rule-h_k-a-b-k","title":"'square_summable_not_summable' rule: h_k = a / (b + k","text":""},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--a-10-b-2","title":"a = 10, b = 2","text":"<p>step = StepSize(name='square_summable_not_summable', parameters=[10.0, 2.0])</p>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--geometric_series-rule-h_k-a-rk","title":"'geometric_series' rule: h_k = a * r^k","text":""},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--a-10-r-05","title":"a = 1.0, r = 0.5","text":"<p>step = StepSize(name='geometric_series', parameters=[1.0, 0.5])</p>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--user_defined-callable","title":"'user_defined' callable.","text":""},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--custom-rule-h_k-1-k2","title":"Custom rule: h_k = 1 / k^2","text":"<p>custom_rule = lambda k: 1.0 / (k**2) step = StepSize(name='user_defined', parameters=custom_rule)</p>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__call__","title":"<code>__call__(k)</code>","text":"<p>Returns the step size at iteration k.</p> Source code in <code>specular\\optimization\\step_size.py</code> <pre><code>def __call__(self, k: int) -&gt; float:\n    \"\"\"\n    Returns the step size at iteration k.\n    \"\"\"\n    return self._rule(k)\n</code></pre>"},{"location":"api/optimization/#quick-example","title":"Quick Example","text":"<pre><code>from specular.optimization.step_size import StepSize\n\n# Use a square-summable rule: h_k = 10 / (2 + k)\nstep = StepSize(name='square_summable_not_summable', parameters=[10.0, 2.0])\nh_1 = step(1)\nprint(h_1)\n# Output: 3.3333333333333335\n</code></pre>"},{"location":"api/optimization/#232-the-specular-gradient-method","title":"2.3.2 The specular gradient method","text":""},{"location":"api/optimization/#the-one-dimensional-case","title":"The one dimensional case","text":"<pre><code>import specular \n\n# Objective function: f(x) = |x|\ndef f(x):\n    return abs(x)\n\nstep_size = specular.StepSize('constant', 0.1) \n\n# Specular gradient method\nres = specular.gradient_method(f=f, x_0=1.0, step_size=step_size, form='specular gradient', max_iter=20)\n</code></pre>"},{"location":"api/optimization/#higher-dimensional-cases","title":"Higher dimensional cases","text":"<pre><code>import specular \n\n# Objective function: f(x) = sum(x^2)\ndef f(x):\n    return float(np.sum(np.array(x)**2))\n\n# Component functions for Stochastic test\n# f(x) = x1^2 + x2^2\n# f1(x) = x1^2, f2(x) = x2^2\ndef f_comp_1(x):\n    return x[0]**2\n\ndef f_comp_2(x):\n    return x[1]**2\n\nf_components = [f_comp_1, f_comp_2]\n\nx_0 = [1.0, 1.0]\nstep_size = specular.StepSize('square_summable_not_summable', [0.5, 1.0]) \n\n# Specular gradient method\nres1 = specular.gradient_method(f=f, x_0=x_0, step_size=step_size, form='specular gradient', max_iter=50)\n\n# Stochastic specular gradient method\nres2 = specular.gradient_method(f=f, x_0=x_0, step_size=step_size, form='stochastic', f_j=f_components, max_iter=100)\n\n# hybrid specular gradient method\nres3 = specular.gradient_method(f=f_quad_vector, x_0=x_0, step_size=step_size, form='hybrid', f_j=f_components, switch_iter=5, max_iter=20)\n</code></pre>"},{"location":"api/optimization/#233-optimizationresult","title":"2.3.3 <code>OptimizationResult</code>","text":"<p>The class <code>OptimizationResult</code> collects the optimization results. To get history of optimization, call <code>history()</code>.</p> <pre><code>import specular \n\n# Objective function: f(x) = sum(x^2)\ndef f(x):\n    return float(np.sum(np.array(x)**2))\n\nx_0 = [1.0, 1.0]\nstep_size = specular.StepSize('square_summable_not_summable', [0.5, 1.0]) \n\n# Specular gradient method\nres_x, res_f, res_time = specular.gradient_method(f=f, x_0=x_0, step_size=step_size, form='specular gradient', max_iter=50).history()\n</code></pre>"},{"location":"examples/","title":"3. Examples","text":"<p>This directory includes applications of specular differentiation. Each subdirectory is based on a paper.</p>"},{"location":"examples/#31-ordinary-differential-equation","title":"3.1. Ordinary differential equation","text":"<ul> <li>Directory: <code>examples/ode/</code></li> </ul>"},{"location":"examples/#2026-jung","title":"2026-Jung","text":"<ul> <li>Directory: <code>examples/ode/2026-Jung/</code></li> <li>Keywords: generalized differentiation, Mean Value Theorem, Fermat's Theorem, explicit Euler scheme, implicit Euler scheme, Crank-Nicolson scheme</li> <li>Reference: TBA</li> </ul>"},{"location":"examples/#32-optimization","title":"3.2. Optimization","text":"<ul> <li>Directory: <code>examples/optimization/</code></li> <li>Requirements: </li> <li><code>scipy</code> &gt;= 1.10.0</li> <li><code>torch</code> &gt;= 2.0.0</li> </ul>"},{"location":"examples/#2024-jung-oh","title":"2024-Jung-Oh","text":"<ul> <li>Directory: <code>examples/optimization/2024-Jung-Oh/</code></li> <li>Keywords: nonsmooth convex optimization, subgradient methods, non-differentiable convex functions, generalization of derivatives, convergence rate</li> <li>Reference: K. Jung and J. Oh. Nonsmooth convex optimization using the specular gradient method with root-linear convergence. arXiv preprint arXiv:2210.06933, 2024</li> </ul>"},{"location":"examples/#2026-jung_1","title":"2026-Jung","text":"<ul> <li>Directory: <code>examples/optimization/2026-Jung/</code></li> <li>Keywords: TBA</li> <li>Reference: TBA</li> </ul>"}]}