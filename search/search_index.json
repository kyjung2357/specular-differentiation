{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Specular Differentiation","text":"<p>The Python package <code>specular</code> implements specular differentiation which generalizes classical differentiation. This implementation strictly follows the definitions, notations, and results in [1] and [2].</p> <p>A specular derivative (the red line) can be understood as the average of the inclination angles of the right and left derivatives.  In contrast, a symmetric derivative (the purple line) is the average of the right and left derivatives. Their difference is illustrated as in the following figure.</p> <p></p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Applications</li> <li>Documentation</li> <li>LaTeX macro</li> <li>Citing specular-differentiation</li> <li>References</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#requirements","title":"Requirements","text":"<p><code>specular-differentiation</code> requires:</p> <ul> <li>Python &gt;= 3.11</li> <li><code>ipython</code> &gt;= 8.12.3</li> <li><code>matplotlib</code> &gt;= 3.10.8</li> <li><code>numpy</code> &gt;= 2.4.0</li> <li><code>pandas</code> &gt;= 2.3.3</li> <li><code>tqdm</code> &gt;= 4.67.1</li> </ul>"},{"location":"#user-installation","title":"User installation","text":"<p>Standard Installation (NumPy backend)</p> <pre><code>pip install specular-differentiation\n</code></pre> <p>Advanced Installation (JAX backend)</p> <pre><code>pip install \"specular-differentiation[jax]\"\n</code></pre> <p>See the documentation for advanced installation (JAX backend, Pytest).</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>The following simple example calculates the specular derivative of the ReLU function \\(f(x) = max(0, x)\\) at the origin.</p> <pre><code>import specular\n\nReLU = lambda x: max(x, 0)\nspecular.derivative(ReLU, x=0)\n# Output: 0.41421356237309515\n</code></pre>"},{"location":"#applications","title":"Applications","text":"<p>Specular differentiation is defined in normed vector spaces, allowing for applications in higher-dimensional Euclidean spaces.  The <code>specular</code> package includes the following applications.</p>"},{"location":"#ordinary-differential-equation","title":"Ordinary differential equation","text":"<ul> <li>Directory: <code>examples/ode/</code></li> <li>References: [1], [3], [4]</li> </ul> <p>In [1], seven schemes are proposed for solving ODEs numerically:</p> <ul> <li>the specular Euler scheme of Type 1~6</li> <li>the specular trigonometric scheme</li> </ul> <p>The following example shows that the specular Euler schemes of Type 5 and 6 yield more accurate numerical solutions than classical schemes: the explicit and implicit Euler schemes and the Crank-Nicolson scheme.</p> <p></p>"},{"location":"#optimization","title":"Optimization","text":"<ul> <li>Directory: <code>examples/optimization/</code></li> <li>References: [2], [5]</li> </ul> <p>In [2], three methods are proposed for optimizing nonsmooth convex objective functions:</p> <ul> <li>the specular gradient (SPEG) method</li> <li>the stochastic specular gradient (S-SPEG) method</li> <li>the hybrid specular gradient (H-SPEG) method</li> </ul> <p>The following example compares the three proposed methods with the classical methods: gradient descent (GD), Adaptive Moment Estimation (Adam), and Broyden-Fletcher-Goldfarb-Shanno (BFGS).</p> <p></p>"},{"location":"#documentation","title":"Documentation","text":""},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#api-reference","title":"API Reference","text":""},{"location":"#examples","title":"Examples","text":""},{"location":"#latex-macro","title":"LaTeX macro","text":"<p>To use the specular differentiation symbol in your LaTeX document, add the following code to your preamble (before <code>\\begin{document}</code>):</p> <pre><code>% Required packages\n\\usepackage{graphicx}\n\\usepackage{bm}\n\n% Definition of Specular Differentiation symbol\n\\newcommand\\sd[1][.5]{\\mathbin{\\vcenter{\\hbox{\\scalebox{#1}{\\,$\\bm{\\wedge}$}}}}}\n</code></pre>"},{"location":"#usage-examples","title":"Usage examples","text":"<p>Use the symbol in your document (after <code>\\begin{document}</code>):</p> <pre><code>% A specular derivative in the one-dimensional Euclidean space\n$f^{\\sd}(x)$\n\n% A specular directional derivative in normed vector spaces\n$\\partial^{\\sd}_v f(x)$\n</code></pre>"},{"location":"#citing-specular-differentiation","title":"Citing specular-differentiation","text":"<p>To cite this repository:</p> <pre><code>@software{Jung_specular-differentiation_2026,\n  author = {Jung, Kiyuob},\n  doi = {10.5281/zenodo.18246734},\n  license = {MIT},\n  month = jan,\n  title = {{specular-differentiation}},\n  url = {https://github.com/kyjung2357/specular-differentiation},\n  version = {1.0.0},\n  year = {2026},\n}\n</code></pre>"},{"location":"#references","title":"References","text":"<p>[1] K. Jung. Nonlinear numerical schemes using specular differentiation for initial value problems of first-order ordinary differential equations. arXiv preprint arXiv:2601.09900, 2026.</p> <p>[2] K. Jung. Specular differentiation in normed vector spaces and its applications to nonsmooth convex optimization. arXiv preprint arXiv:????.?????, TBA. </p> <p>[3] K. Jung and J. Oh. The specular derivative. arXiv preprint arXiv:2210.06062, 2022.</p> <p>[4] K. Jung and J. Oh. The wave equation with specular derivatives. arXiv preprint arXiv:2210.06933, 2022.</p> <p>[5] K. Jung and J. Oh. Nonsmooth convex optimization using the specular gradient method with root-linear convergence. arXiv preprint arXiv:2210.06933, 2024.</p>"},{"location":"started/","title":"1. Getting Started","text":""},{"location":"started/#11-user-installation","title":"1.1. User installation","text":"<p>Standard Installation (NumPy backend)</p> <p>The package is available on PyPI:</p> <pre><code>pip install specular-differentiation\n</code></pre> <p>Check the version:</p> <pre><code>import specular\n\nprint(\"version: \", specular.__version__)\n# Output: version:  1.0.0\n</code></pre> <p>Advanced Installation (JAX backend)</p> <p>By default, the package uses the NumPy backend (CPU).  To enable hardware acceleration, you can install the package with the JAX backend (GPU/TPU).  This adds the following dependencies:</p> <ul> <li>JAX (<code>jax</code>, <code>jaxlib</code> &gt;= 0.4):</li> </ul> <pre><code>pip install \"specular-differentiation[jax]\"\n</code></pre> <p>Note</p> <p>This feature is experimental for now. See 2.4 JAX backend.</p> <p>Developer installation</p> <p>To install all dependencies including tests, docs, and examples. This adds the following dependencies:</p> <ul> <li>JAX (<code>jax</code>, <code>jaxlib</code> &gt;= 0.4):</li> <li>SciPy (<code>scipy</code> &gt;= 1.10.0)</li> <li>PyTorch (<code>torch</code> &gt;= 2.0.0)</li> <li>Pytest (<code>pytest</code> &gt;= 7.0)</li> </ul> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"started/#12-quick-start","title":"1.2. Quick start","text":"<p>The following simple example calculates the specular derivative of the ReLU function \\(f(x) = max(0, x)\\) at the origin.</p> <pre><code>import specular\n\nReLU = lambda x: max(x, 0)\nspecular.derivative(ReLU, x=0)\n# Output: 0.41421356237309515\n</code></pre>"},{"location":"started/#13-jax-backend-usage","title":"1.3. JAX Backend Usage","text":"<p>To leverage JAX for hardware acceleration instead of the standard NumPy backend, import <code>specular.jax</code>:</p> <pre><code>import specular.jax as sjax\n\nReLU = lambda x: jax.numpy.maximum(x, 0)\nsjax.derivative(ReLU, 0.0)\n# Output: Array(0.41421354, dtype=float32)\n</code></pre> <p>To enable 64-bit precision (double precision), update the JAX configuration as follows:</p> <pre><code>import jax\njax.config.update(\"jax_enable_x64\", True)\n\nimport specular.jax as sjax\n\nReLU = lambda x: jax.numpy.maximum(x, 0)\nsjax.derivative(ReLU, 0.0)\n# Output: Array(0.41421356, dtype=float64)\n</code></pre>"},{"location":"api/","title":"2. API Reference","text":"<p>The specular package consists of the following modules and subpackages.</p>"},{"location":"api/#21-calculation","title":"2.1. Calculation","text":"<ul> <li><code>specular.calculation</code>: five primary functions to calculate specular differentiation, depending on the dimension of input.</li> </ul>"},{"location":"api/#22-ode","title":"2.2. ODE","text":"<ul> <li> <p><code>specular.ode.solver</code>: the explicit Euler, implicit Euler, Crank-Nicolson, specular trigonometric, specular Euler schemes.</p> </li> <li> <p><code>specular.ode.result</code>: the <code>ODEResult</code> class to store the results.</p> </li> </ul>"},{"location":"api/#23-optimization","title":"2.3. Optimization","text":"<ul> <li> <p><code>specular.optimization.step_size</code>: the <code>StepSize</code> class to define step size \\(h_k\\).</p> </li> <li> <p><code>specular.optimization.solver</code>: the specular gradient method.</p> </li> <li> <p><code>specular.optimization.classical_solver</code>: the gradient descent method, Adam, and BFGS. Lazy importing.</p> </li> <li> <p><code>specular.optimization.result</code>: the <code>OptimizationResult</code> class to store the results.</p> </li> </ul>"},{"location":"api/#24-jax-backend","title":"2.4. JAX Backend","text":"<ul> <li> <p><code>specular.jax.calculation</code>: JAX implementations of the calculations in <code>specular.calculation</code>. Lazy importing.</p> </li> <li> <p><code>specular.jax.optimization.solver</code>: JAX implementations of the numerical schemes in <code>specular.optimization.solver</code>. Lazy importing.</p> </li> </ul>"},{"location":"api/calculation/","title":"2.1. Calculation","text":"<p>Directory: <code>specular/calculation/</code></p> <p>The <code>specular.calculation</code> module provides five primary functions to calculate specular differentiation, depending on the dimension of input.</p> Function Space Description Input Type Output Type <code>derivative</code> \\(\\mathbb{R} \\to \\mathbb{R}^m\\) specular derivative <code>float</code> <code>float</code>, <code>np.ndarray</code> <code>directional_derivative</code> \\(\\mathbb{R}^n \\to \\mathbb{R}\\) specular directional derivative in direction \\(v \\in \\mathbb{R}^n\\) <code>np.ndarry</code> <code>float</code> <code>partial_derivative</code> \\(\\mathbb{R}^n \\to \\mathbb{R}\\) specular partial derivative w.r.t. \\(v = x_i\\) <code>np.ndarray</code> <code>float</code> <code>gradient</code> \\(\\mathbb{R}^n \\to \\mathbb{R}\\) specular gradient vector <code>np.ndarray</code> <code>np.ndarray</code> <code>jacobian</code> \\(\\mathbb{R}^n \\to \\mathbb{R}^m\\) specular jacobian matrix <code>np.ndarray</code> <code>np.ndarray</code>"},{"location":"api/calculation/#211-one-dimensional-euclidean-space-n1","title":"2.1.1. One-dimensional Euclidean Space (\\(n=1\\))","text":"<p>In \\(\u211d\\), the specular derivative can be calculated using the function <code>derivative</code>.</p> <pre><code>import specular\n\ndef f(x):\n    return max(x, 0.0)\n\nspecular.derivative(f, x=0.0)\n# Output: 0.41421356237309515\n</code></pre>"},{"location":"api/calculation/#212-the-n-dimensional-euclidean-space-n1","title":"2.1.2. the \\(n\\)-dimensional Euclidean space (\\(n&gt;1\\))","text":"<p>In \\(\u211d^n\\), the specular directional derivative of a function \\(f: \u211d^n \\to \u211d\\) at a point \\(x \\in \u211d^n\\) in the direction \\(v \\in \u211d^n\\) can be calculated using the function <code>directional_derivative</code>.</p> <pre><code>import specular\nimport math \n\nf = lambda x: math.sqrt(x[0]**2 + x[1]**2 + x[2]**2)\nspecular.directional_derivative(f, x=[0.0, 0.1, -0.1], v=[1.0, -1.0, 2.0])\n# Output: -2.1213203434708223\n</code></pre> <p>Let \\(e_1, e_2, \\ldots, e_n\\) be the standard basis of \\(\u211d^n\\). For each \\(i \\in \u2115\\) with \\(1 \\leq i \\leq n\\), the specular partial derivative with respect to a variable \\(x_i\\) can be calculated using the function <code>partial_derivative</code>, which yields the same result as <code>directional_derivative</code> with direction \\(v=e_i\\).</p> <pre><code>import specular\nimport math\n\ndef f(x):\n    return math.sqrt(x[0]**2 + x[1]**2 + x[2]**2)\n\nspecular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=2)\n# Output: 0.8859268982863702\nspecular.directional_derivative(f, x=[0.1, 2.3, -1.2], v=[0.0, 1.0, 0.0])\n# Output: 0.8859268982863702\n</code></pre> <p>Also, the specular gradient can be calculated using <code>gradient</code>.</p> <pre><code>import specular\nimport numpy as np\n\ndef f(x):\n    return np.linalg.norm(x)\n\nspecular.gradient(f, x=[0.1, 2.3, -1.2])\n# Output: [ 0.03851856  0.8859269  -0.46222273]\nspecular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=1)\n# Output: 0.03851856078540371\nspecular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=2)\n# Output: 0.8859268982863702\nspecular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=3)\n# Output: -0.4622227292028128\n</code></pre>"},{"location":"api/calculation/#213-api-reference","title":"2.1.3. API Reference","text":""},{"location":"api/calculation/#specular.calculation","title":"<code>specular.calculation</code>","text":"<p>This module provides implementations of specular directional derivatives, specular partial derivatives, specular derivatives, specular gradients, and specular Jacobians.</p> <p>The calculations are based on the function \\(\\mathcal{A}:\\mathbb{R}^2 \\to \\mathbb{R}\\) defined by </p> \\[ \\mathcal{A}(\\alpha, \\beta) = \\begin{cases}     \\frac{\\alpha \\beta - 1 + \\sqrt{(1 + \\alpha^2)(1 + \\beta^2)}}{\\alpha + \\beta} &amp; \\text{if } \\alpha + \\beta \\neq 0, \\\\     0 &amp; \\text{otherwise.} \\end{cases} \\] <p>The parameters \\(\\alpha\\) and \\(\\beta\\) are intended to represent right and left derivatives. In the code, computations are based on the finite difference approximation of one-sided (directional) derivatives:</p> \\[ \\alpha \\approx \\frac{f(x + hv) - f(x)}{h} \\qquad \\text{and} \\qquad \\beta \\approx \\frac{f(x) - f(x - hv)}{h}, \\] <p>where a function \\(f : \\mathbb{R}^n \\to \\mathbb{R}\\), a real number \\(h &gt; 0\\), and vectors \\(x, v \\in \\mathbb{R}^n\\).</p>"},{"location":"api/calculation/#specular.calculation.A","title":"<code>A(alpha, beta, zero_tol=1e-08)</code>","text":"<p>Compute the function \\(\\mathcal{A}\\) from one-sided directional derivatives.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float | number | int</code> <p>One-sided directional derivative.</p> required <code>beta</code> <code>float | number | int</code> <p>One-sided directional derivative.</p> required <code>zero_tol</code> <code>float</code> <p>A small threshold used to determine if the denominator <code>alpha + beta</code> is close to zero for numerical stability.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>float</code> <p>The function \\(\\mathcal{A}\\).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import specular\n&gt;&gt;&gt; specular.calculation.A(1.0, 2.0)\n1.3874258867227933\n</code></pre> Source code in <code>specular\\calculation.py</code> <pre><code>def A(\n    alpha: float | np.number | int,\n    beta: float | np.number | int,\n    zero_tol: float = 1e-8\n) -&gt; float:\n    \"\"\"\n    Compute the function $\\\\mathcal{A}$ from one-sided directional derivatives.\n\n    Parameters:\n        alpha (float | np.number | int):\n            One-sided directional derivative.\n        beta (float | np.number | int):\n            One-sided directional derivative.\n        zero_tol (float, optional):\n            A small threshold used to determine if the denominator ``alpha + beta`` is close to zero for numerical stability.\n\n    Returns:\n        The function $\\\\mathcal{A}$.\n\n    Examples:\n        &gt;&gt;&gt; import specular\n        &gt;&gt;&gt; specular.calculation.A(1.0, 2.0)\n        1.3874258867227933\n    \"\"\"\n    denominator = alpha + beta\n\n    if abs(denominator) &lt;= zero_tol:\n        return 0.0\n\n    numerator = alpha * beta - 1.0 + math.sqrt((1.0 + alpha**2) * (1.0 + beta**2))\n\n    return numerator / denominator\n</code></pre>"},{"location":"api/calculation/#specular.calculation.derivative","title":"<code>derivative(f, x, h=1e-06, zero_tol=1e-08)</code>","text":"<p>Approximates the specular derivative of a function \\(f:\\mathbb{R} \\to \\mathbb{R}^m\\) at a scalar point \\(x\\).</p> <p>If <code>f</code> returns a scalar, the result is a float.</p> <p>If <code>f</code> returns a vector, the result is a vector (component-wise derivative).</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>callable</code> <p>A function of a single real variable, returning a scalar or a vector.</p> required <code>x</code> <code>float | number | int</code> <p>The point at which the derivative is evaluated.</p> required <code>h</code> <code>float</code> <p>Mesh size used in the finite difference approximation. Must be positive.</p> <code>1e-06</code> <code>zero_tol</code> <code>float</code> <p>A small threshold used to determine if the denominator <code>alpha + beta</code> is close to zero for numerical stability.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>float | ndarray</code> <p>The approximated specular derivative of <code>f</code> at <code>x</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the type of <code>x</code> is not a scalar.</p> <code>ValueError</code> <p>If <code>h</code> is not positive.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import specular\n&gt;&gt;&gt; f = lambda x: max(x, 0.0)\n&gt;&gt;&gt; specular.derivative(f, x=0.0)\n0.41421356237309515\n&gt;&gt;&gt; f = lambda x: abs(x)\n&gt;&gt;&gt; specular.derivative(f, x=0.0)\n0.0\n</code></pre> Source code in <code>specular\\calculation.py</code> <pre><code>def derivative(\n    f: Callable[[int | float | np.number], int | float | np.number | list | np.ndarray],\n    x: float | np.number | int,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; float | np.ndarray:\n    \"\"\"\n    Approximates the specular derivative of a function $f:\\\\mathbb{R} \\\\to \\\\mathbb{R}^m$ at a scalar point $x$.\n\n    If ``f`` returns a scalar, the result is a float.\n\n    If ``f`` returns a vector, the result is a vector (component-wise derivative).\n\n    Parameters:\n        f (callable):\n            A function of a single real variable, returning a scalar or a vector.\n        x (float | np.number | int):\n            The point at which the derivative is evaluated.\n        h (float, optional):\n            Mesh size used in the finite difference approximation. Must be positive.\n        zero_tol (float, optional):\n            A small threshold used to determine if the denominator ``alpha + beta`` is close to zero for numerical stability.\n\n    Returns:\n        The approximated specular derivative of ``f`` at ``x``.\n\n    Raises:\n        TypeError:\n            If the type of ``x`` is not a scalar.\n        ValueError:\n            If ``h`` is not positive.\n\n    Examples:\n        &gt;&gt;&gt; import specular\n        &gt;&gt;&gt; f = lambda x: max(x, 0.0)\n        &gt;&gt;&gt; specular.derivative(f, x=0.0)\n        0.41421356237309515\n        &gt;&gt;&gt; f = lambda x: abs(x)\n        &gt;&gt;&gt; specular.derivative(f, x=0.0)\n        0.0\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    try:\n        x = float(x)\n\n    except TypeError:\n        raise TypeError(\n            f\"Input 'x' must be a scalar. \"\n            f\"Got {type(x).__name__}. \"\n            \"Use `specular.directional_derivative`, `specular.gradient`, or `specular.jacobian` for vectors inputs.\"\n        )\n\n    f_val = f(x)\n\n    # f is real-valued\n    if np.ndim(f_val) == 0:\n        alpha = (f(x + h) - f_val) / h # type: ignore\n        beta = (f_val - f(x - h)) / h # type: ignore\n\n        return A(alpha, beta, zero_tol=zero_tol) # type: ignore\n\n    # f is vector-valued\n    else:\n        f_right = np.asarray(f(x + h), dtype=float)\n        f_val = np.asarray(f_val, dtype=float)\n        f_left = np.asarray(f(x - h), dtype=float)\n\n        return _A_vector(f_right, f_val, f_left, h, zero_tol)\n</code></pre>"},{"location":"api/calculation/#specular.calculation.directional_derivative","title":"<code>directional_derivative(f, x, v, h=1e-06, zero_tol=1e-08)</code>","text":"<p>Approximates the specular directional derivative of a function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) at a point \\(x\\) in the direction \\(v\\).</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>callable</code> <p>A function of a real vector variable, returning a scalar.</p> required <code>x</code> <code>list | ndarray</code> <p>The point at which the derivative is evaluated.</p> required <code>v</code> <code>list | ndarray</code> <p>The direction in which the derivative is taken.</p> required <code>h</code> <code>float</code> <p>Mesh size used in the finite difference approximation. Must be positive.</p> <code>1e-06</code> <code>zero_tol</code> <code>float</code> <p>A small threshold used to determine if the denominator <code>alpha + beta</code> is close to zero for numerical stability.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>float</code> <p>The approximated specular directional derivative of <code>f</code> at <code>x</code> in the direction <code>v</code> as a scalar.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>x</code> or <code>v</code> are not of valid array-like types.</p> <code>ValueError</code> <p>If <code>x</code> and <code>v</code> have different shape. If <code>h</code> is not positive.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import specular\n&gt;&gt;&gt; import math\n&gt;&gt;&gt; f = lambda x: math.sqrt(x[0]**2 + x[1]**2 + x[2]**2)\n&gt;&gt;&gt; specular.directional_derivative(f, x=[0.0, 0.1, -0.1], v=[1.0, -1.0, 2.0])\n-2.1213203434708223\n</code></pre> Source code in <code>specular\\calculation.py</code> <pre><code>def directional_derivative(\n    f: Callable[[list | np.ndarray], int | float | np.number],\n    x: list | np.ndarray,\n    v: list | np.ndarray,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; float:\n    \"\"\"\n    Approximates the specular directional derivative of a function $f:\\\\mathbb{R}^n \\\\to \\\\mathbb{R}$ at a point $x$ in the direction $v$.\n\n    Parameters:\n        f (callable):\n            A function of a real vector variable, returning a scalar.\n        x (list | np.ndarray):\n            The point at which the derivative is evaluated.\n        v (list | np.ndarray):\n            The direction in which the derivative is taken.\n        h (float, optional):\n            Mesh size used in the finite difference approximation. Must be positive.\n        zero_tol (float, optional):\n            A small threshold used to determine if the denominator ``alpha + beta`` is close to zero for numerical stability.\n\n    Returns:\n        The approximated specular directional derivative of ``f`` at ``x`` in the direction ``v`` as a scalar.\n\n    Raises:\n        TypeError:\n            If ``x`` or ``v`` are not of valid array-like types.\n        ValueError:\n            If ``x`` and ``v`` have different shape.\n            If ``h`` is not positive.\n\n    Examples:\n        &gt;&gt;&gt; import specular\n        &gt;&gt;&gt; import math\n        &gt;&gt;&gt; f = lambda x: math.sqrt(x[0]**2 + x[1]**2 + x[2]**2)\n        &gt;&gt;&gt; specular.directional_derivative(f, x=[0.0, 0.1, -0.1], v=[1.0, -1.0, 2.0])\n        -2.1213203434708223\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    x = np.asarray(x, dtype=float)\n    v = np.asarray(v, dtype=float)\n\n    if x.ndim == 0:\n        raise TypeError(\n            f\"Input 'x' must be a vector. \"\n            f\"Got {type(x).__name__}. \"\n            \"Use `specular.derivative` for scalar inputs.\"\n        )\n\n    if v.ndim == 0:\n        raise TypeError(\n            \"Input 'v' must be a vector. \"\n            f\"Got {type(v).__name__}.\"\n        )\n\n    if x.shape != v.shape:\n        raise ValueError(f\"Shape mismatch: x {x.shape} vs v {v.shape}\")\n\n    f_val = f(x) \n\n    if np.ndim(f_val) != 0:\n        raise ValueError(\n            \"Function f must return a scalar value. \"\n            f\"Got shape {np.shape(f_val)}.\"\n        )\n\n    alpha = (f(x + h * v) - f_val)/h\n    beta = (f_val - f(x - h * v))/h\n\n    return A(alpha, beta, zero_tol=zero_tol)\n</code></pre>"},{"location":"api/calculation/#specular.calculation.gradient","title":"<code>gradient(f, x, h=1e-06, zero_tol=1e-08)</code>","text":"<p>Approximates the specular gradient of a real-valued function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) at point \\(x\\) for \\(n &gt; 1\\).</p> <p>The specular gradient is defined as the vector of all partial specular derivatives along the standard basis directions.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>callable</code> <p>A function of a real vector variable, returning a scalar.</p> required <code>x</code> <code>list | ndarray</code> <p>The point at which the specular gradient is evaluated.</p> required <code>h</code> <code>float</code> <p>Mesh size used in the finite difference approximation. Must be positive.</p> <code>1e-06</code> <code>zero_tol</code> <code>float</code> <p>A small threshold used to determine if the denominator <code>alpha + beta</code> is close to zero for numerical stability.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The approximated specular gradient of <code>f</code> at <code>x</code> as a vector.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>x</code> is not of a valid array-like type.</p> <code>ValueError</code> <p>If <code>f</code> does not return a scalar value. If <code>h</code> is not positive.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import specular\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; f = lambda x: np.linalg.norm(x)\n&gt;&gt;&gt; specular.gradient(f, x=[1.4, -3.47, 4.57, 9.9])\narray([ 0.12144298, -0.3010051 ,  0.39642458,  0.85877534])\n</code></pre> Source code in <code>specular\\calculation.py</code> <pre><code>def gradient(\n    f: Callable[[list | np.ndarray], int |float | np.number],\n    x: list | np.ndarray,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; np.ndarray:\n    \"\"\"\n    Approximates the specular gradient of a real-valued function $f:\\\\mathbb{R}^n \\\\to \\\\mathbb{R}$ at point $x$ for $n &gt; 1$.\n\n    The specular gradient is defined as the vector of all partial specular derivatives along the standard basis directions.\n\n    Parameters:\n        f (callable):\n            A function of a real vector variable, returning a scalar.\n        x (list | np.ndarray):\n            The point at which the specular gradient is evaluated.\n        h (float, optional):\n            Mesh size used in the finite difference approximation. Must be positive.\n        zero_tol (float, optional):\n            A small threshold used to determine if the denominator ``alpha + beta`` is close to zero for numerical stability.\n\n    Returns:\n        The approximated specular gradient of ``f`` at ``x`` as a vector.\n\n    Raises:\n        TypeError:\n            If ``x`` is not of a valid array-like type.\n        ValueError:\n            If ``f`` does not return a scalar value.\n            If ``h`` is not positive.\n\n    Examples:\n        &gt;&gt;&gt; import specular\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; f = lambda x: np.linalg.norm(x)\n        &gt;&gt;&gt; specular.gradient(f, x=[1.4, -3.47, 4.57, 9.9])\n        array([ 0.12144298, -0.3010051 ,  0.39642458,  0.85877534])\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    x = np.asarray(x, dtype=float).copy()\n\n    if x.ndim != 1:\n        raise TypeError(\n            f\"Input 'x' must be a vector. \"\n            f\"Got {type(x).__name__} with shape {x.shape}. \"\n            \"Use `specular.derivative` for scalar inputs.\"\n        )\n\n    n = x.size\n\n    f_val_scalar = f(x)\n\n    if np.ndim(f_val_scalar) != 0:\n        raise ValueError(\n            \"Function 'f' must return a scalar value. \"\n            f\"Got shape {np.shape(f_val_scalar)}.\"\n        )\n\n    f_right = np.empty(n, dtype=float)\n    f_left = np.empty(n, dtype=float)\n\n    for i in range(n):\n        origin_val = x[i]\n\n        x[i] = origin_val + h\n        f_right[i] = f(x)\n\n        x[i] = origin_val - h\n        f_left[i] = f(x)\n\n        x[i] = origin_val\n\n    f_val_arr = np.full_like(f_right, f_val_scalar)\n\n    return _A_vector(f_right, f_val_arr, f_left, h, zero_tol)\n</code></pre>"},{"location":"api/calculation/#specular.calculation.jacobian","title":"<code>jacobian(f, x, h=1e-06, zero_tol=1e-08)</code>","text":"<p>Approximates the specular Jacobian matrix of a vector-valued function \\(f:\\mathbb{R}^n \\to \\mathbb{R}^m\\).</p> <p>Returns a matrix of shape (\\(m\\), \\(n\\)) where \\(J[j, i]\\) is the partial derivative of a component function \\(f_j\\) with respect to \\(x_i\\) (\\(1 \\leq i \\leq n\\), \\(1 \\leq j \\leq m\\).</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>callable</code> <p>A function of a real vector variable, returning a vector.</p> required <code>x</code> <code>list | ndarray</code> <p>The point at which the specular gradient is evaluated.</p> required <code>h</code> <code>float</code> <p>Mesh size used in the finite difference approximation. Must be positive.</p> <code>1e-06</code> <code>zero_tol</code> <code>float</code> <p>A small threshold used to determine if the denominator <code>alpha + beta</code> is close to zero for numerical stability.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The approximated specular Jacobian of <code>f</code> at <code>x</code> as a matrix.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>x</code> is not of a valid array-like type.</p> <code>ValueError</code> <p>If <code>h</code> is not positive.</p> Source code in <code>specular\\calculation.py</code> <pre><code>def jacobian(\n    f: Callable[[list | np.ndarray], int | float | np.number | list | np.ndarray],\n    x: list | np.ndarray,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; np.ndarray:\n    \"\"\"\n    Approximates the specular Jacobian matrix of a vector-valued function $f:\\\\mathbb{R}^n \\\\to \\\\mathbb{R}^m$.\n\n    Returns a matrix of shape ($m$, $n$) where $J[j, i]$ is the partial derivative of a component function $f_j$ with respect to $x_i$ ($1 \\\\leq i \\\\leq n$, $1 \\\\leq j \\\\leq m$.\n\n    Parameters:\n        f (callable):\n            A function of a real vector variable, returning a vector.\n        x (list | np.ndarray):\n            The point at which the specular gradient is evaluated.\n        h (float, optional):\n            Mesh size used in the finite difference approximation. Must be positive.\n        zero_tol (float, optional):\n            A small threshold used to determine if the denominator ``alpha + beta`` is close to zero for numerical stability.\n\n    Returns:\n        The approximated specular Jacobian of ``f`` at ``x`` as a matrix.\n\n    Raises:\n        TypeError:\n            If ``x`` is not of a valid array-like type.\n        ValueError:\n            If ``h`` is not positive.\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    x = np.asarray(x, dtype=float)\n\n    if x.ndim != 1:\n        raise TypeError(\n            f\"Input 'x' must be a vector. \"\n            f\"Got {type(x).__name__} with shape {x.shape}. \"\n            \"Use `specular.derivative` for scalar inputs.\"\n        )\n\n    n = x.size\n\n    f_val = np.asarray(f(x), dtype=float)\n\n    if f_val.ndim == 0:\n        f_val = f_val.reshape(1)\n\n    m = f_val.size\n\n    identity = np.eye(n)\n\n    x_right = x + h * identity\n    x_left = x - h * identity\n\n    f_right = np.array([f(row) for row in x_right], dtype=float)\n    f_left = np.array([f(row) for row in x_left], dtype=float)\n\n    f_right = f_right.reshape(n, m)\n    f_left = f_left.reshape(n, m)\n\n    f_val = np.tile(f_val, (n, 1))\n\n    J_transposed = _A_vector(f_right, f_val, f_left, h, zero_tol)\n\n    return J_transposed.T\n</code></pre>"},{"location":"api/calculation/#specular.calculation.partial_derivative","title":"<code>partial_derivative(f, x, i, h=1e-06, zero_tol=1e-08)</code>","text":"<p>Approximates the i-th specular partial derivative of a real-valued function \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) at point \\(x\\) for \\(n &gt; 1\\).</p> <p>This is computed using <code>specular.directional_derivative</code> with the direction of the \\(i\\)-th standard basis vector of \\(\\mathbb{R}^n\\).</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>callable</code> <p>A function of a real vector variable, returning a scalar.</p> required <code>x</code> <code>list | ndarray</code> <p>The point at which the derivative is evaluated.</p> required <code>i</code> <code>int | integer</code> <p>The index of the specular partial derivative with respect to \\(x_i\\) (<code>1 &lt;= i &lt;= n</code>).</p> required <code>h</code> <code>float</code> <p>Mesh size used in the finite difference approximation. Must be positive.</p> <code>1e-06</code> <code>zero_tol</code> <code>float</code> <p>A small threshold used to determine if the denominator <code>alpha + beta</code> is close to zero for numerical stability.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>float</code> <p>The approximated <code>i</code>-th partial specular derivative of <code>f</code> at <code>x</code> as a scalar.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>i</code> is not an integer.</p> <code>ValueError</code> <p>If <code>i</code> is out of the valid range (<code>1 &lt;= i &lt;= n</code>).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import specular\n&gt;&gt;&gt; import math \n&gt;&gt;&gt; f = lambda x: math.sqrt(x[0]**2 + x[1]**2 + x[2]**2)\n&gt;&gt;&gt; specular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=2)\n0.8859268982863702\n</code></pre> Source code in <code>specular\\calculation.py</code> <pre><code>def partial_derivative(\n    f: Callable[[list | np.ndarray], int |float | np.number],\n    x: list | np.ndarray,\n    i: int | np.integer,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; float:\n    \"\"\"\n    Approximates the i-th specular partial derivative of a real-valued function $f:\\\\mathbb{R}^n \\\\to \\\\mathbb{R}$ at point $x$ for $n &gt; 1$.\n\n    This is computed using ``specular.directional_derivative`` with the direction of the $i$-th standard basis vector of $\\\\mathbb{R}^n$.\n\n    Parameters:\n        f (callable):\n            A function of a real vector variable, returning a scalar.\n        x (list | np.ndarray):\n            The point at which the derivative is evaluated.\n        i (int | np.integer):\n            The index of the specular partial derivative with respect to $x_i$ (``1 &lt;= i &lt;= n``).\n        h (float, optional):\n            Mesh size used in the finite difference approximation. Must be positive.\n        zero_tol (float, optional):\n            A small threshold used to determine if the denominator ``alpha + beta`` is close to zero for numerical stability.\n\n    Returns:\n        The approximated ``i``-th partial specular derivative of ``f`` at ``x`` as a scalar.\n\n    Raises:\n        TypeError:\n            If ``i`` is not an integer.\n        ValueError:\n            If ``i`` is out of the valid range (``1 &lt;= i &lt;= n``).\n\n    Examples:\n        &gt;&gt;&gt; import specular\n        &gt;&gt;&gt; import math \n        &gt;&gt;&gt; f = lambda x: math.sqrt(x[0]**2 + x[1]**2 + x[2]**2)\n        &gt;&gt;&gt; specular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=2)\n        0.8859268982863702\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n\n    if not isinstance(i, (int, np.integer)):\n        raise TypeError(f\"Index 'i' must be an integer. Got {type(i).__name__}\")\n\n    n = x.size\n    if i &lt; 1 or i &gt; n:\n        raise ValueError(f\"Index 'i' must be between 1 and {n} (dimension of x). Got {i}\")\n\n    e_i = np.zeros_like(x)\n    e_i[i - 1] = 1.0\n\n    return directional_derivative(f, x, e_i, h, zero_tol)\n</code></pre>"},{"location":"api/jax/","title":"2.4. JAX backend","text":"<p>See the official homepage of JAX.</p>"},{"location":"api/jax/#241-for-now","title":"2.4.1. For now","text":"<ul> <li>This feature is currently experimental and undergoing verification.</li> <li>Benchmarks indicate significant speedups compared to the NumPy backend.</li> <li>Full GPU support and optimization are planned but not yet finalized.</li> <li>Requirement: Your objective function must use <code>jax.numpy</code> instead of standard <code>numpy</code> to avoid errors.</li> </ul>"},{"location":"api/jax/#242-why-jax","title":"2.4.2. Why JAX?","text":"<ul> <li>The JAX is chosen as the primary acceleration backend due to its high compatibility and similar syntax to NumPy. </li> <li>The core calculation logic is planned to be ported to other backends like PyTorch or TensorFlow to provide native GPU/TPU support and broader ecosystem integration.</li> </ul>"},{"location":"api/jax/#243-example","title":"2.4.3. Example","text":"<p>For a detailed comparison of the algorithms, please refer to the following scripts:</p> <ul> <li> <p><code>examples/jax/main.py</code>: A basic implementation using the JAX backend.</p> </li> <li> <p><code>examples/optimization/2026-Jung/main.py</code>: The full experimental setup used in the paper.</p> </li> </ul> <p>Note</p> <p>Preliminary tests show that the computation time is shorter than BFGS; however, the exact theoretical reasons for this performance gain are still being investigated and have not yet been fully proven.</p>"},{"location":"api/jax/#244-api-reference","title":"2.4.4. API Reference","text":""},{"location":"api/jax/#specular.jax.calculation","title":"<code>specular.jax.calculation</code>","text":"<p>This module provides JAX-based implementations of the function \\(\\mathcal{A}\\), specular directional derivatives, specular partial derivatives, specular derivatives, specular gradients, and specular Jacobians.</p> <p>It utilizes <code>jax.numpy</code> for GPU/TPU acceleration and <code>jax.vmap</code> for auto-vectorization.</p>"},{"location":"api/jax/#specular.jax.calculation.derivative","title":"<code>derivative(f, x, h=1e-06, zero_tol=1e-08)</code>","text":"<p>JAX version of <code>specular.derivative</code>.</p> Source code in <code>specular\\jax\\calculation.py</code> <pre><code>def derivative(\n    f: Callable[[ArrayLike], ArrayLike],\n    x: float | int | ArrayLike,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; ArrayLike:\n    \"\"\"\n    JAX version of ``specular.derivative``.\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    x = jnp.asarray(x, dtype=float)\n\n    if x.ndim != 0:\n         raise TypeError(f\"Input 'x' must be a scalar. Got shape {x.shape}.\")\n\n    f_right = jnp.asarray(f(x + h))\n    f_val = jnp.asarray(f(x))\n    f_left = jnp.asarray(f(x - h))\n\n    return _A_vector(f_right, f_val, f_left, h, zero_tol)\n</code></pre>"},{"location":"api/jax/#specular.jax.calculation.directional_derivative","title":"<code>directional_derivative(f, x, v, h=1e-06, zero_tol=1e-08)</code>","text":"<p>JAX version of <code>specular.directional_derivative</code>.</p> Source code in <code>specular\\jax\\calculation.py</code> <pre><code>def directional_derivative(\n    f: Callable[[ArrayLike], ArrayLike],\n    x: ArrayLike,\n    v: ArrayLike,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; float | Array:\n    \"\"\"\n    JAX version of ``specular.directional_derivative``.\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    x = jnp.asarray(x, dtype=float)\n    v = jnp.asarray(v, dtype=float)\n\n    if x.ndim == 0 or v.ndim == 0:\n        raise TypeError(\"Input 'x' and 'v' must be vectors.\")\n\n    if x.shape != v.shape:\n        raise ValueError(f\"Shape mismatch: x {x.shape} vs v {v.shape}\")\n\n    f_val = jnp.asarray(f(x))\n\n    if f_val.ndim != 0:\n        raise ValueError(f\"Function f must return a scalar. Got shape {f_val.shape}\")\n\n    f_right = jnp.asarray(f(x + h * v))\n    f_left = jnp.asarray(f(x - h * v))\n\n    return _A_vector(f_right, f_val, f_left, h, zero_tol)\n</code></pre>"},{"location":"api/jax/#specular.jax.calculation.gradient","title":"<code>gradient(f, x, h=1e-06, zero_tol=1e-08)</code>","text":"<p>JAX version of <code>specular.gradient</code>.</p> Source code in <code>specular\\jax\\calculation.py</code> <pre><code>def gradient(\n    f: Callable[[ArrayLike], ArrayLike],\n    x: ArrayLike,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; Array:\n    \"\"\"\n    JAX version of ``specular.gradient``.\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    x = jnp.asarray(x, dtype=float)\n\n    if x.ndim != 1:\n        raise TypeError(f\"Input 'x' must be a vector. Got shape {x.shape}.\")\n\n    n = x.size\n    identity = jnp.eye(n)\n\n    f_val = jnp.asarray(f(x))\n\n    if f_val.ndim != 0:\n        raise ValueError(f\"Function f must return a scalar. Got shape {f_val.shape}.\")\n\n    x_right_batch = x + h * identity\n    x_left_batch = x - h * identity\n\n    f_right = jnp.asarray(jax.vmap(f)(x_right_batch))\n    f_left = jnp.asarray(jax.vmap(f)(x_left_batch))\n\n    return _A_vector(f_right, f_val, f_left, h, zero_tol)\n</code></pre>"},{"location":"api/jax/#specular.jax.calculation.jacobian","title":"<code>jacobian(f, x, h=1e-06, zero_tol=1e-08)</code>","text":"<p>JAX version of <code>specular.jacobian</code>.</p> Source code in <code>specular\\jax\\calculation.py</code> <pre><code>def jacobian(\n    f: Callable[[ArrayLike], ArrayLike],\n    x: ArrayLike,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; Array:\n    \"\"\"\n    JAX version of ``specular.jacobian``.\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    x = jnp.asarray(x, dtype=float)\n\n    if x.ndim != 1:\n        raise TypeError(f\"Input 'x' must be a vector. Got shape {x.shape}.\")\n\n    n = x.size\n\n    f_val = jnp.asarray(f(x), dtype=float)\n\n    if f_val.ndim == 0:\n        f_val = f_val.reshape(1)\n\n    m = f_val.size\n    identity = jnp.eye(n)\n\n    x_right_batch = x + h * identity\n    x_left_batch = x - h * identity\n\n    f_right = jax.vmap(f)(x_right_batch)\n    f_left = jax.vmap(f)(x_left_batch)\n\n    f_right = jnp.asarray(f_right, dtype=float).reshape(n, m)\n    f_left = jnp.asarray(f_left, dtype=float).reshape(n, m)\n\n    J_transposed = _A_vector(f_right, f_val, f_left, h, zero_tol)\n\n    return J_transposed.T\n</code></pre>"},{"location":"api/jax/#specular.jax.calculation.partial_derivative","title":"<code>partial_derivative(f, x, i, h=1e-06, zero_tol=1e-08)</code>","text":"<p>JAX version of <code>specular.partial_derivative</code>.</p> Source code in <code>specular\\jax\\calculation.py</code> <pre><code>def partial_derivative(\n    f: Callable[[ArrayLike], ArrayLike],\n    x: ArrayLike,\n    i: int,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; float | Array:\n    \"\"\"\n    JAX version of ``specular.partial_derivative``.\n    \"\"\"\n    x = jnp.asarray(x, dtype=float)\n    n = x.size\n\n    if i &lt; 1 or i &gt; n:\n        raise ValueError(f\"Index 'i' must be between 1 and {n}.\")\n\n    e_i = jnp.zeros_like(x)\n    e_i = e_i.at[i - 1].set(1.0) \n\n    return directional_derivative(f, x, e_i, h, zero_tol)\n</code></pre>"},{"location":"api/jax/#specular.jax.optimization.solver","title":"<code>specular.jax.optimization.solver</code>","text":""},{"location":"api/jax/#specular.jax.optimization.solver.gradient_method","title":"<code>gradient_method(f, x_0, step_size, h=1e-06, form='specular gradient', tol=1e-06, zero_tol=1e-08, max_iter=1000, f_j=None, m=1, seed=0, switch_iter=2, record_history=True)</code>","text":"<p>JAX implementation of <code>specular.gradient_method</code>.</p> Source code in <code>specular\\jax\\optimization\\solver.py</code> <pre><code>def gradient_method(\n    f: Callable[[Any], Any],\n    x_0: Any,\n    step_size: StepSize,\n    h: float = 1e-6,\n    form: str = 'specular gradient',\n    tol: float = 1e-6,\n    zero_tol: float = 1e-8,\n    max_iter: int = 1000,\n    f_j: Callable[[Any, int], Any] | None = None,\n    m: int = 1,\n    seed: int = 0,\n    switch_iter: int | None = 2,\n    record_history: bool = True\n) -&gt; OptimizationResult:\n    \"\"\"\n    JAX implementation of ``specular.gradient_method``.\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    x_0_jax = jnp.array(x_0, dtype=float)\n\n    step_func = _create_step_fn(step_size)\n\n    all_history = {}\n\n    start_time = time.time()\n\n    if form == 'specular gradient':\n        res_x, x_history, res_f, f_history, res_k = _vector_jax(\n            f, x_0_jax, step_func, h, tol, zero_tol, max_iter, record_history, k_start=1\n        )\n\n    elif form == 'stochastic':\n        if f_j is None:\n            raise ValueError(\"Component functions 'f_j' must be provided for the stochastic form.\")\n\n        form = 'stochastic specular gradient'\n        res_x, x_history, res_f, f_history, res_k = _vector_stochastic_jax(\n            f, x_0_jax, step_func, h, tol, zero_tol, f_j, m, max_iter, record_history, seed, k_start=1 # type: ignore\n        )\n\n    elif form == 'hybrid':\n        if f_j is None:\n            raise ValueError(\"Component functions 'f_j' must be provided for the stochastic form.\")\n\n        form = 'hybrid specular gradient'\n        switch_iter = switch_iter if switch_iter is not None else max_iter\n        remaining_iter = max_iter - switch_iter\n\n        # Phase 1: Deterministic\n        x_1, hist_x1, _, hist_f1, k1 = _vector_jax(\n            f, x_0_jax, step_func, h, tol, zero_tol, switch_iter, record_history, k_start=1\n        )\n\n        # Phase 2: Stochastic\n        res_x, hist_x2, res_f, hist_f2, k2 = _vector_stochastic_jax(\n            f, x_1, step_func, h, tol, zero_tol, f_j, m, remaining_iter, record_history, seed, k_start=k1 + 1\n        )\n\n        if record_history and hist_x1 is not None:\n             x_history = (hist_x1, hist_x2)\n             f_history = (hist_f1, hist_f2)\n        else:\n             x_history, f_history = None, None\n\n        res_k = k1 + k2\n\n    else:\n        raise TypeError(f\"Unknown form for JAX backend: '{form}'. Supported forms: {SUPPORTED_METHODS}\")\n\n    res_x.block_until_ready()\n    runtime = time.time() - start_time\n\n    final_x_hist = None\n    final_f_hist = None\n\n    if record_history:\n        if isinstance(x_history, tuple):\n            final_x_hist = np.concatenate([np.array(x_history[0]), np.array(x_history[1])], axis=0)\n            final_f_hist = np.concatenate([np.array(f_history[0]), np.array(f_history[1])], axis=0) # type: ignore\n\n        elif x_history is not None:\n            final_x_hist = np.array(x_history)\n            final_f_hist = np.array(f_history)\n\n        if final_x_hist is not None:\n             final_x_hist = np.insert(final_x_hist, 0, np.array(x_0_jax), axis=0)\n             final_f_hist = np.insert(final_f_hist, 0, float(f(x_0_jax)), axis=0) # type: ignore\n\n        all_history[\"variables\"] = final_x_hist\n        all_history[\"values\"] = final_f_hist\n\n    return OptimizationResult(\n        method=f\"JAX {form}\",\n        solution=np.array(res_x),\n        func_val=float(res_f),\n        iteration=int(res_k),\n        runtime=runtime,\n        all_history=all_history\n    )\n</code></pre>"},{"location":"api/ode/","title":"2.2. Ordinary differential equations","text":"<p>Let the source function \\(F:[t_0, T] \\times \u211d \\to \u211d\\) be given, and the initial data \\(u_0:\u211d \\to \u211d\\) be given.  Consider the initial value problem:</p> \\[ u'(t) = F(t, u(t)) \\] <p>with the initial condition \\(u(t_0) = u_0(t_0)\\).</p> <p>To solve the problem numerically, the subpackage <code>specular.ode.solver</code> provides the following numerical schemes:</p> <ul> <li>the specular Euler scheme (Type 1 ~ 6)</li> <li>the specular trigonometric scheme</li> <li>the explicit Euler scheme</li> <li>the implicit Euler scheme</li> <li>the Crank-Nicolson scheme</li> </ul>"},{"location":"api/ode/#221-specular-euler-scheme","title":"2.2.1. Specular Euler scheme","text":"<p>All functions return an instance of the <code>ODEResult</code> class that encapsulates the numerical results.</p> <pre><code>import specular\n\ndef F(t, u):\n    return -2*u \n\nspecular.Euler_scheme(of_Type='1', F=F, t_0=0.0, u_0=1.0, T=2.5, h=0.1)\n# Output: Running the specular Euler scheme of Type 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00&lt;?, ?it/s]\n# Output: &lt;specular.ode.result.ODEResult at 0x1765982d8d0&gt;\n</code></pre> <p>To access the numerical results, call <code>.history()</code>. It returns a tuple containing the time grid and the numerical solution.</p> <pre><code>import specular\n\ndef F(t, u):\n    return -2*u \n\nspecular.Euler_scheme(of_Type=1, F=F, t_0=0.0, u_0=1.0, T=2.5, h=0.1).history()\n# Output: Running the specular Euler scheme of Type 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00&lt;?, ?it/s]\n# Output: (array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2,\n#        1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5]),\n# Output:  array([1.        , 0.8       , 0.62169432, 0.48101574, 0.37172557,\n#        0.2870388 , 0.22149069, 0.17081087, 0.13166787, 0.1014624 ,\n#        0.07816953, 0.06021577, 0.04638162, 0.0357239 , 0.02751427,\n#        0.02119088, 0.01632056, 0.0125695 , 0.00968054, 0.00745555,\n#        0.00574195, 0.00442221, 0.00340579, 0.00262299, 0.00202011,\n#        0.0015558 ]))\n</code></pre> <p>To visualize the numerical results, call <code>.visualization()</code>.</p> <pre><code>import specular\nimport numpy as np\n\ndef F(t, u):\n   return -2*u \n\ndef exact_sol(t):\n    return np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nspecular.Euler_scheme(of_Type='1', F=F, t_0=0.0, u_0=u_0, T=2.5, h=0.1).visualization(exact_sol=exact_sol, save_path=\"specular-Euler-scheme-of-Type-1\")\n# Output: Running the specular Euler scheme of Type 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00&lt;?, ?it/s]\n# Output: Figure saved: figures\\specular-Euler-scheme-of-Type-1\n</code></pre> <p></p> <p>To obtain the table of the numerical results, call <code>.table()</code>. </p> <pre><code>import specular\nimport numpy as np\n\ndef F(t, u):\n    return -2*u\n\ndef exact_sol(t):\n    return np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nspecular.Euler_scheme(of_Type=4, F=F, t_0=0.0, u_0=u_0, T=2.5, h=0.1).table(exact_sol=exact_sol, save_path=\"specular-Euler-scheme-of-type-4\")\n# Output: Running the specular Euler scheme of Type 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00&lt;?, ?it/s]\n# Output: Table saved: tables\\specular-Euler-scheme-of-type-4.csv\n</code></pre> <p><code>.visualization()</code> and <code>.table()</code> are are chainable.</p> <pre><code>import specular\nimport numpy as np\n\ndef F(t, u):\n    return -2*u\n\ndef exact_sol(t):\nreturn np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nspecular.Euler_scheme(of_Type=4, F=F, t_0=0.0, u_0=u_0, T=2.5, h=0.1).visualization(exact_sol=exact_sol).table(exact_sol=exact_sol)\n# Output: Running the specular Euler scheme of Type 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00&lt;?, ?it/s]\n</code></pre> <p>To compute the total error of the numerical results, call <code>.total_error()</code>. The exact solution is required. The norm can be <code>max</code>, <code>l1</code>, or <code>l2</code>.</p> <pre><code>def F(t, u):\n    return -2*u \n\ndef exact_sol(t):\n    return np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nspecular.Euler_scheme(of_Type=5, F=F, t_0=0.0, u_0=u_0, T=10.0, h=0.1).total_error(exact_sol=exact_sol, norm='max')\n# Output: Running the specular Euler scheme of Type 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 300882.64it/s]\n# Output: 0.0011409613137273178\n</code></pre>"},{"location":"api/ode/#222-specular-trigonometric-scheme","title":"2.2.2. Specular trigonometric scheme","text":"<pre><code>import specular\n\ndef F(t, u):\n    return -2*u \n\ndef exact_sol(t):\n    return np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nu_1 = exact_sol(t_0 + h)\n\nspecular.trigonometric_scheme(F=F, t_0=0.0, u_0=u_0, u_1=u_1, T=2.5, h=0.1).visualization(exact_sol=exact_sol, save_path=\"specular-trigonometric\")\n# Output: Running specular trigonometric scheme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00&lt;?, ?it/s]\n# Output: Figure saved: figures\\specular-trigonometric\n</code></pre>"},{"location":"api/ode/#223-classical-schemes","title":"2.2.3. Classical schemes","text":"<p>The three classical schemes are available: the explicit Euler, the implicit Euler, and the Crank-Nicolson schemes.</p> <pre><code>import specular\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef F(t, u):\n    return -(t*u)/(1-t**2)\ndef exact_sol(t):\n    return np.sqrt(1 - t**2)\ndef u_0(t_0):\n    return exact_sol(t_0)\nt_0 = 0.0\nT = 0.9\nh = 0.05\n\nresult_EE = specular.ode.solver.classical_scheme(F=F, t_0=t_0, u_0=u_0, T=T, h=h, scheme=\"explicit Euler\").history()\nresult_IE = specular.ode.solver.classical_scheme(F=F, t_0=t_0, u_0=u_0, T=T, h=h, scheme=\"implicit Euler\").history()\nresult_CN = specular.ode.solver.classical_scheme(F=F, t_0=t_0, u_0=u_0, T=T, h=h, scheme=\"Crank-Nicolson\").history()\nexact_values = np.array([exact_sol(t) for t in result_EE[0]])\n\nplt.figure(figsize=(5.5, 2.5))\n\nplt.plot(result_EE[0], exact_values, color='black', label='Exact solution')\nplt.plot(result_EE[0], result_EE[1],  marker='x', linestyle='None', markerfacecolor='none', markeredgecolor='red', label='Explicit Euler') \nplt.plot(result_IE[0], result_IE[1],  marker='x', linestyle='None', markerfacecolor='none', markeredgecolor='blue', label='Implicit Euler') \nplt.plot(result_CN[0], result_CN[1],  marker='x', linestyle='None', markerfacecolor='none', markeredgecolor='purple', label='Crank-Nicolson')\n\nplt.xlabel(r\"Time\", fontsize=10)\nplt.ylabel(r\"Solution\", fontsize=10)\nplt.grid(True)\nplt.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), borderaxespad=0., fontsize=10)\nplt.savefig('figures/classical-schemes.png', dpi=1000, bbox_inches='tight')\nplt.show()\n# Output: Running the explicit Euler scheme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00&lt;?, ?it/s]\n# Output: Running the implicit Euler scheme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00&lt;?, ?it/s]\n# Output: Running Crank-Nicolson scheme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00&lt;00:00, 17988.44it/s]\n</code></pre> <p></p>"},{"location":"api/ode/#224-api-reference","title":"2.2.4. API Reference","text":""},{"location":"api/ode/#specular.ode.solver","title":"<code>specular.ode.solver</code>","text":"<p>Let the source function \\(F:[t_0, T] \\times \\mathbb{R} \\to \\mathbb{R}\\) be given, and the initial data \\(u_0:\\mathbb{R} \\to \\mathbb{R}\\) be given.  Consider the initial value problem:</p> \\[ u'(t) = F(t, u(t))  \\qquad \\text{(IVP)} \\] <p>with the initial condition \\(u(t_0) = u_0(t_0)\\). To solve (IVP) numerically, this module provides implementations of the specular Euler schemes, the Crank-Nicolson scheme, and the specular trigonometric scheme.</p>"},{"location":"api/ode/#specular.ode.solver.Euler_scheme","title":"<code>Euler_scheme(of_Type, F, t_0, u_0, T, h=1e-06, u_1=False, tol=1e-06, zero_tol=1e-08, max_iter=100)</code>","text":"<p>Solves an initial value problem (IVP) using the specular Euler scheme of Type 1, 2, 3, 4, 5, and 6.</p> <p>Parameters:</p> Name Type Description Default <code>of_Type</code> <code>int | str</code> <p>The type of the specular Euler scheme. Options: <code>1</code>, <code>'1'</code>, <code>2</code>, <code>'2'</code>, <code>3</code>, <code>'3'</code>, <code>4</code>, <code>'4'</code>, <code>5</code>, <code>'5'</code>, <code>6</code>, <code>'6'</code>.</p> required <code>F</code> <code>callable</code> <p>The given source function <code>F</code> in (IVP). The calling signature should be <code>F(t, u)</code> where <code>t</code> and <code>u</code> are scalars.</p> required <code>t_0</code> <code>float</code> <p>The starting time of the simulation.</p> required <code>u_0</code> <code>callable</code> <p>The given initial condition <code>u_0</code> in (IVP).</p> required <code>T</code> <code>float</code> <p>The end time of the simulation.</p> required <code>h</code> <code>float</code> <p>Mesh size used in the finite difference approximation. Must be positive.</p> <code>1e-06</code> <code>u_1</code> <code>callable | float | bool</code> <p>The numerical solution at the time <code>t_1 = t_0 + h</code> for Types 1, 2, and 3. If a float or callable is provided, it is used as the exact value. If False, the explicit Euler scheme is applied.</p> <code>False</code> <code>tol</code> <code>float | optional</code> <p>Tolerance for fixed-point iteration Used for Types 3, 4, 5, and 6.</p> <code>1e-06</code> <code>zero_tol</code> <code>float | floating</code> <p>A small threshold used to determine if the denominator (alpha + beta) is close to zero for numerical stability.</p> <code>1e-08</code> <code>max_iter</code> <code>int | optional</code> <p>Max iterations for fixed-point solver.</p> <code>100</code> <p>Returns:</p> Type Description <code>ODEResult</code> <p>An object containing <code>(t, u)</code> data and the scheme name.</p> Source code in <code>specular\\ode\\solver.py</code> <pre><code>def Euler_scheme(\n    of_Type: int | str,\n    F: Callable[[float, float], float],\n    t_0: float, \n    u_0: Callable[[float], float] | float,\n    T: float, \n    h: float = 1e-6,\n    u_1: Callable[[float], float] | float | bool = False,\n    tol: float = 1e-6, \n    zero_tol: float = 1e-8,\n    max_iter: int = 100\n) -&gt; ODEResult:\n    \"\"\"\n    Solves an initial value problem (IVP) using the specular Euler scheme of Type 1, 2, 3, 4, 5, and 6.\n\n    Parameters:\n        of_Type (int | str):\n            The type of the specular Euler scheme.\n            Options: ``1``, ``'1'``, ``2``, ``'2'``, ``3``, ``'3'``, ``4``, ``'4'``, ``5``, ``'5'``, ``6``, ``'6'``.\n        F (callable):\n            The given source function ``F`` in (IVP).\n            The calling signature should be ``F(t, u)`` where ``t`` and ``u`` are scalars.\n        t_0 (float):\n            The starting time of the simulation.\n        u_0 (callable):\n            The given initial condition ``u_0`` in (IVP).\n        T (float):\n            The end time of the simulation.\n        h (float, optional):\n            Mesh size used in the finite difference approximation. Must be positive.\n        u_1 (callable | float | bool):\n            The numerical solution at the time ``t_1 = t_0 + h`` for Types 1, 2, and 3.\n            If a float or callable is provided, it is used as the exact value.\n            If False, the explicit Euler scheme is applied.\n        tol (float | optional):\n            Tolerance for fixed-point iteration\n            Used for Types 3, 4, 5, and 6.\n        zero_tol (float | np.floating):\n            A small threshold used to determine if the denominator (alpha + beta) is close to zero for numerical stability.\n        max_iter (int | optional):\n            Max iterations for fixed-point solver.\n\n    Returns:\n        An object containing ``(t, u)`` data and the scheme name.\n    \"\"\"\n    Type = str(of_Type)\n\n    scheme = 'specular Euler scheme of Type ' + Type\n    steps = int((T - t_0) / h)\n\n    all_history = {}\n    t_history = []\n    u_history = []\n\n    if Type in ['1', '2', '3']:\n        t_prev = t_0\n        u_prev = u_0(t_0) if callable(u_0) else u_0\n\n        t_history.append(t_prev)\n        u_history.append(u_prev)\n\n        t_curr = t_prev + h\n\n        if u_1 == False:\n            # explicit Euler to get u_1\n            u_curr = u_prev + h * F(t_prev, u_prev)\n        else:\n            u_curr = u_1\n\n        t_history.append(t_curr)\n        u_history.append(u_curr)\n\n        if Type == '1':\n            _of_Type_1(F, h, zero_tol, steps, t_prev, t_curr, u_prev, u_curr, t_history, u_history)\n\n        elif Type == '2':\n            _of_Type_2(F, h, zero_tol, steps, t_prev, t_curr, u_prev, u_curr, t_history, u_history)\n\n        elif Type == '3':\n            _of_Type_3(F, h, tol, zero_tol, max_iter, steps, t_prev, t_curr, u_prev, u_curr, t_history, u_history)\n\n    elif Type in ['4', '5', '6']:\n        t_curr = t_0\n        u_curr = u_0(t_0) if callable(u_0) else u_0  \n\n        t_history.append(t_curr)\n        u_history.append(u_curr)\n\n        if Type == '4':\n            _of_Type_4(F, h, tol, zero_tol, max_iter, steps, t_curr, u_curr, t_history, u_history)\n\n        elif Type == '5':\n            _of_Type_5(F, h, tol, zero_tol, max_iter, steps, t_curr, u_curr, t_history, u_history)\n\n        elif Type == '6':\n            _of_Type_6(F, h, tol, zero_tol, max_iter, steps, t_curr, u_curr, t_history, u_history)\n\n    else:\n        raise ValueError(f\"Unknown type. Got {of_Type}. Supported types: '1', '2', '3', '4', '5', and '6'\")\n\n    all_history[\"variables\"] = np.array(t_history)\n    all_history[\"values\"] = np.array(u_history)\n\n    return ODEResult(\n        scheme=scheme,\n        h=h,\n        all_history=all_history\n    )\n</code></pre>"},{"location":"api/ode/#specular.ode.solver.classical_scheme","title":"<code>classical_scheme(F, t_0, u_0, T, h=1e-06, form='explicit Euler', tol=1e-06, max_iter=100)</code>","text":"<p>Solves an initial value problem (IVP) using classical numerical schemes. Supported forms: explicit Euler, implicit Euler, and Crank-Nicolson.</p> <p>Parameters:</p> Name Type Description Default <code>F</code> <code>callable</code> <p>The given source function <code>F</code> in (IVP). The calling signature should be <code>F(t, u)</code> where <code>t</code> and <code>u</code> are scalars.</p> required <code>t_0</code> <code>float</code> <p>The starting time of the simulation.</p> required <code>u_0</code> <code>callable</code> <p>The given initial condition <code>u_0</code> in (IVP).</p> required <code>T</code> <code>float</code> <p>The end time of the simulation.</p> required <code>h</code> <code>float</code> <p>Mesh size used in the finite difference approximation. Must be positive.</p> <code>1e-06</code> <code>form</code> <code>str | optional</code> <p>The form of the numerical scheme.  Options: <code>'explicit_Euler'</code>, <code>'implicit_Euler'</code>, <code>'Crank-Nicolson'</code>.</p> <code>'explicit Euler'</code> <code>tol</code> <code>float | optional</code> <p>Tolerance for fixed-point iteration. Used for implicit Euler and Crank-Nicolson schemes.</p> <code>1e-06</code> <code>max_iter</code> <code>int | optional</code> <p>Max iterations for fixed-point solver.</p> <code>100</code> <p>Returns:</p> Type Description <code>ODEResult</code> <p>An object containing <code>(t, u)</code> data and the scheme name.</p> Source code in <code>specular\\ode\\solver.py</code> <pre><code>def classical_scheme(\n    F: Callable[[float, float], float], \n    t_0: float, \n    u_0: Callable[[float], float] | float,\n    T: float, \n    h: float = 1e-6,\n    form: str = 'explicit Euler',\n    tol: float = 1e-6, \n    max_iter: int = 100\n) -&gt; ODEResult:\n    \"\"\"\n    Solves an initial value problem (IVP) using classical numerical schemes.\n    Supported forms: explicit Euler, implicit Euler, and Crank-Nicolson.\n\n    Parameters:\n        F (callable):\n            The given source function ``F`` in (IVP).\n            The calling signature should be ``F(t, u)`` where ``t`` and ``u`` are scalars.\n        t_0 (float):\n            The starting time of the simulation.\n        u_0 (callable):\n            The given initial condition ``u_0`` in (IVP).\n        T (float):\n            The end time of the simulation.\n        h (float, optional):\n            Mesh size used in the finite difference approximation. Must be positive.\n        form (str | optional):\n            The form of the numerical scheme. \n            Options: ``'explicit_Euler'``, ``'implicit_Euler'``, ``'Crank-Nicolson'``.\n        tol (float | optional):\n            Tolerance for fixed-point iteration.\n            Used for implicit Euler and Crank-Nicolson schemes.\n        max_iter (int | optional):\n            Max iterations for fixed-point solver.\n\n    Returns:\n        An object containing ``(t, u)`` data and the scheme name.\n    \"\"\"\n    t_curr = t_0\n    u_curr = u_0(t_0) if callable(u_0) else u_0 \n\n    all_history = {}\n    t_history = [t_curr]\n    u_history = [u_curr]\n\n    steps = int((T - t_0) / h)\n\n    if form == \"explicit Euler\":\n        for _ in tqdm(range(steps), desc=\"Running the explicit Euler scheme\"):\n            t_curr, u_curr = t_curr + h, u_curr + h*F(t_curr, u_curr) # type: ignore\n\n            t_history.append(t_curr) \n            u_history.append(u_curr) \n\n    elif form == \"implicit Euler\":\n        for k in tqdm(range(steps), desc=\"Running the implicit Euler scheme\"):\n            t_next = t_curr + h\n\n            # Initial guess: explicit Euler \n            u_temp = u_curr + h*F(t_curr, u_curr) # type: ignore\n            u_guess = u_temp\n\n            # Fixed-point iteration\n            for _ in range(max_iter):\n                u_guess = u_curr + h*F(t_next, u_temp)\n                if np.linalg.norm(u_guess - u_temp) &lt; tol:\n                    break\n                u_temp = u_guess\n            else:\n                print(f\"Warning: step {k+1} did not converge.\")\n\n            t_curr, u_curr = t_next, u_guess  \n            t_history.append(t_curr)\n            u_history.append(u_curr)\n\n    elif form == \"Crank-Nicolson\":\n        for k in tqdm(range(steps), desc=\"Running Crank-Nicolson scheme\"):\n            t_next = t_curr + h\n\n            F_curr = F(t_curr, u_curr)\n\n            # Initial guess: explicit Euler\n            u_temp = u_curr + h * F_curr\n            u_guess = u_temp\n\n            # Fixed-point iteration\n            for _ in range(max_iter):\n                f_guess = F(t_next, u_temp)\n                u_guess = u_curr + 0.5 * h * (F_curr + f_guess)\n\n                if np.linalg.norm(u_guess - u_temp) &lt; tol:\n                    break\n\n                u_temp = u_guess\n            else:\n                print(f\"Warning: step {k+1} did not converge.\")\n\n            t_curr, u_curr = t_next, u_guess\n            t_history.append(t_curr)\n            u_history.append(u_curr)\n\n    else:\n        raise ValueError(f\"Unknown form '{form}'. Supported forms: {SUPPORTED_SCHEMES}\")\n\n    all_history[\"variables\"] = np.array(t_history)\n    all_history[\"values\"] = np.array(u_history)\n\n    return ODEResult(\n        scheme= form + \" scheme\",\n        h=h,\n        all_history=all_history\n    )\n</code></pre>"},{"location":"api/ode/#specular.ode.solver.trigonometric_scheme","title":"<code>trigonometric_scheme(F, t_0, u_0, u_1, T, h=1e-06)</code>","text":"<p>Solves an initial value problem (IVP) using the specular trigonometric scheme.</p> <p>Parameters:</p> Name Type Description Default <code>F</code> <code>callable</code> <p>The given source function <code>F</code> in (IVP). The calling signature should be <code>F(t, u)</code> where <code>t</code> and <code>u</code> are scalars.</p> required <code>t_0</code> <code>float</code> <p>The starting time of the simulation.</p> required <code>u_0</code> <code>callable</code> <p>The given initial condition <code>u_0</code> in (IVP).</p> required <code>u_1</code> <code>callable | float | bool</code> <p>The numerical solution at the time <code>t_1 = t_0 + h</code> for Types 1, 2, and 3. If a float or callable is provided, it is used as the exact value. If False, the explicit Euler scheme is applied.</p> required <code>T</code> <code>float</code> <p>The end time of the simulation.</p> required <code>h</code> <code>float</code> <p>Mesh size used in the finite difference approximation. Must be positive.</p> <code>1e-06</code> <p>Returns:</p> Type Description <code>ODEResult</code> <p>An object containing <code>(t, u)</code> data and the scheme name.</p> Source code in <code>specular\\ode\\solver.py</code> <pre><code>def trigonometric_scheme(\n    F: Callable[[float], float],\n    t_0: float, \n    u_0: Callable[[float], float] | float,\n    u_1: Callable[[float], float] | float,\n    T: float, \n    h: float = 1e-6\n) -&gt; ODEResult:\n    \"\"\"\n    Solves an initial value problem (IVP) using the specular trigonometric scheme.\n\n    Parameters:\n        F (callable):\n            The given source function ``F`` in (IVP).\n            The calling signature should be ``F(t, u)`` where ``t`` and ``u`` are scalars.\n        t_0 (float):\n            The starting time of the simulation.\n        u_0 (callable):\n            The given initial condition ``u_0`` in (IVP).\n        u_1 (callable | float | bool):\n            The numerical solution at the time ``t_1 = t_0 + h`` for Types 1, 2, and 3.\n            If a float or callable is provided, it is used as the exact value.\n            If False, the explicit Euler scheme is applied.\n        T (float):\n            The end time of the simulation.\n        h (float, optional):\n            Mesh size used in the finite difference approximation. Must be positive.\n\n    Returns:\n        An object containing ``(t, u)`` data and the scheme name.\n    \"\"\"\n    t_prev = t_0\n    u_prev = u_0(t_0) if callable(u_0) else u_0\n\n    t_curr = t_0 + h\n    u_curr = u_1(t_curr) if callable(u_1) else u_1\n\n    all_history = {}\n    t_history = [t_prev, t_curr]\n    u_history = [u_prev, u_curr]\n\n    steps = int((T - t_0) / h)\n\n    for m in tqdm(range(steps - 1), desc=\"Running specular trigonometric scheme\"):\n        t_next = t_curr + h\n        u_next = u_curr + h*math.tan(2*math.atan(F(t_curr, u_curr)) - math.atan((u_curr - u_prev) / h)) # type: ignore\n\n        t_history.append(t_next)\n        u_history.append(u_next)\n\n        t_prev, u_prev = t_curr, u_curr\n        t_curr, u_curr = t_next, u_next\n\n    all_history[\"variables\"] = np.array(t_history)\n    all_history[\"values\"] = np.array(u_history)\n\n    return ODEResult(\n        scheme=\"specular trigonometric scheme\",\n        h=h,\n        all_history=all_history\n    )\n</code></pre>"},{"location":"api/ode/#specular.ode.result","title":"<code>specular.ode.result</code>","text":""},{"location":"api/ode/#specular.ode.result.ODEResult","title":"<code>ODEResult</code>","text":"Source code in <code>specular\\ode\\result.py</code> <pre><code>class ODEResult:\n    def __init__(\n        self,\n        scheme: str,\n        h: float,\n        all_history: dict\n    ):\n        self.scheme = scheme\n        self.h = h\n        self.time_grid = all_history[\"variables\"]\n        self.numerical_sol = all_history[\"values\"]\n\n    def history(\n        self\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Returns the time grid and the numerical solution as a tuple.\n\n        Returns:\n            (time_grid, numerical_sol)\n        \"\"\"\n        return self.time_grid, self.numerical_sol\n\n    def visualization(\n        self, \n        figure_size: tuple = (5.5, 2.5), \n        exact_sol: Optional[Callable[[float], float]] = None,\n        save_path: Optional[str] = None\n    ):\n        plt.figure(figsize=figure_size)\n\n        if exact_sol is not None:\n            exact_values = np.array([exact_sol(t) for t in self.time_grid])\n            plt.plot(self.time_grid, exact_values, color='black', label='Exact solution')\n\n        number_of_circles = max(1, len(self.time_grid) // 30)\n\n        capitalized_name = self.scheme[0].upper() + self.scheme[1:]\n\n        plt.plot(self.time_grid, self.numerical_sol, linestyle='--', marker='o', color='red', markersize=5, markevery=number_of_circles, markerfacecolor='none', markeredgewidth=1.0, label=capitalized_name)\n\n        plt.xlabel(r\"Time\", fontsize=10)\n        plt.ylabel(r\"Solution\", fontsize=10)\n        plt.grid(True)\n        plt.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), borderaxespad=0., fontsize=10)\n\n        if save_path:\n            if not os.path.exists('figures'):\n                os.makedirs('figures')\n\n            save_path = save_path.replace(\" \", \"-\")\n            full_path = os.path.join(\"figures\", save_path)\n\n            if not save_path.endswith(\".png\"):\n                save_path += \".png\"\n\n            plt.savefig(full_path, dpi=1000, bbox_inches='tight')\n\n            print(f\"Figure saved: {full_path}\")\n\n        plt.show()\n\n        return self\n\n    def table(self,\n        exact_sol: Optional[Callable[[float], float]] = None,\n        save_path: Optional[str] = None\n    ):\n\n        result = pd.DataFrame(self.numerical_sol, index=self.time_grid, columns=[\"Numerical solution\"])\n        result.index.name = \"Time\"\n\n        if exact_sol:\n            result[\"Exact solution\"] = [exact_sol(t) for t in self.time_grid]\n            result[\"Error\"] = abs(result[\"Numerical solution\"] - result[\"Exact solution\"])\n\n        if save_path:\n            if not os.path.exists('tables'):\n                os.makedirs('tables')\n\n            save_path = save_path.replace(\" \", \"-\")\n            full_path = os.path.join(\"tables\", save_path)\n\n            if full_path.endswith(\".txt\"):\n                with open(full_path, \"w\") as f:\n                    f.write(result.to_string())\n            else:\n                if not full_path.endswith(\".csv\"):\n                    full_path += \".csv\"\n\n                result.to_csv(full_path)\n\n            print(f\"Table saved: {full_path}\")\n\n        return result\n\n    def total_error(self,\n        exact_sol: Callable[[float], float] | list | np.ndarray,\n        norm: str = 'max'\n    ) -&gt; float:\n        \"\"\"\n        Calculates the error between the numerical solution and the exact solution.\n\n        Parameters:\n            exact_sol (callable | list | np.ndarray):\n                A function that returns the exact solution at a given time ``t``, or a list/array containing the exact values corresponding to ``time_grid``.\n            norm (str | optional):\n                The type of norm to use ``'max'``, ``'l2'``, or ``'l1'``.\n\n        Returns:\n            The computed error value.\n\n        Raises:\n            TypeError:\n                If ``exact_sol`` is neither a callable nor a list/array.\n            ValueError:\n                If ``exact_sol`` (list) shape does not match ``numerical_sol``.\n        \"\"\"\n        if callable(exact_sol):\n            exact_values = np.array([exact_sol(t) for t in self.time_grid])\n\n        elif isinstance(exact_sol, (list, np.ndarray)):\n            exact_values = np.asarray(exact_sol, dtype=float) \n\n        else:\n            raise TypeError(\"exact_sol must be a callable or a list/array.\")\n\n        if exact_values.shape != self.numerical_sol.shape:\n             raise ValueError(f\"Shape mismatch: exact_sol {exact_values.shape} vs numerical_sol {self.numerical_sol.shape}\")\n\n        error_vector = np.abs(exact_values - self.numerical_sol)\n\n        if norm == 'max':\n            return float(np.max(error_vector))\n\n        elif norm == 'l2':\n            return float(np.sqrt(np.sum(error_vector**2) * self.h))\n\n        elif norm == 'l1':\n            return float(np.sum(error_vector) * self.h)\n\n        else:\n            raise ValueError(f\"Unknown norm type. Got '{norm}'. Supported types: 'max', 'l2', 'l1'.\")\n</code></pre>"},{"location":"api/ode/#specular.ode.result.ODEResult.history","title":"<code>history()</code>","text":"<p>Returns the time grid and the numerical solution as a tuple.</p> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>(time_grid, numerical_sol)</p> Source code in <code>specular\\ode\\result.py</code> <pre><code>def history(\n    self\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Returns the time grid and the numerical solution as a tuple.\n\n    Returns:\n        (time_grid, numerical_sol)\n    \"\"\"\n    return self.time_grid, self.numerical_sol\n</code></pre>"},{"location":"api/ode/#specular.ode.result.ODEResult.total_error","title":"<code>total_error(exact_sol, norm='max')</code>","text":"<p>Calculates the error between the numerical solution and the exact solution.</p> <p>Parameters:</p> Name Type Description Default <code>exact_sol</code> <code>callable | list | ndarray</code> <p>A function that returns the exact solution at a given time <code>t</code>, or a list/array containing the exact values corresponding to <code>time_grid</code>.</p> required <code>norm</code> <code>str | optional</code> <p>The type of norm to use <code>'max'</code>, <code>'l2'</code>, or <code>'l1'</code>.</p> <code>'max'</code> <p>Returns:</p> Type Description <code>float</code> <p>The computed error value.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>exact_sol</code> is neither a callable nor a list/array.</p> <code>ValueError</code> <p>If <code>exact_sol</code> (list) shape does not match <code>numerical_sol</code>.</p> Source code in <code>specular\\ode\\result.py</code> <pre><code>def total_error(self,\n    exact_sol: Callable[[float], float] | list | np.ndarray,\n    norm: str = 'max'\n) -&gt; float:\n    \"\"\"\n    Calculates the error between the numerical solution and the exact solution.\n\n    Parameters:\n        exact_sol (callable | list | np.ndarray):\n            A function that returns the exact solution at a given time ``t``, or a list/array containing the exact values corresponding to ``time_grid``.\n        norm (str | optional):\n            The type of norm to use ``'max'``, ``'l2'``, or ``'l1'``.\n\n    Returns:\n        The computed error value.\n\n    Raises:\n        TypeError:\n            If ``exact_sol`` is neither a callable nor a list/array.\n        ValueError:\n            If ``exact_sol`` (list) shape does not match ``numerical_sol``.\n    \"\"\"\n    if callable(exact_sol):\n        exact_values = np.array([exact_sol(t) for t in self.time_grid])\n\n    elif isinstance(exact_sol, (list, np.ndarray)):\n        exact_values = np.asarray(exact_sol, dtype=float) \n\n    else:\n        raise TypeError(\"exact_sol must be a callable or a list/array.\")\n\n    if exact_values.shape != self.numerical_sol.shape:\n         raise ValueError(f\"Shape mismatch: exact_sol {exact_values.shape} vs numerical_sol {self.numerical_sol.shape}\")\n\n    error_vector = np.abs(exact_values - self.numerical_sol)\n\n    if norm == 'max':\n        return float(np.max(error_vector))\n\n    elif norm == 'l2':\n        return float(np.sqrt(np.sum(error_vector**2) * self.h))\n\n    elif norm == 'l1':\n        return float(np.sum(error_vector) * self.h)\n\n    else:\n        raise ValueError(f\"Unknown norm type. Got '{norm}'. Supported types: 'max', 'l2', 'l1'.\")\n</code></pre>"},{"location":"api/optimization/","title":"2.3. Optimization","text":"<p>Consider the optimization problem:</p> \\[ \\min_{x \\in \\mathbb{R}^n} f(x), \\] <p>where \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) is convex.</p> <p>To solve the problem numerically, the subpackage <code>specular.optimization.solver</code> provides the following methods:</p> <ul> <li>the specular gradient (SPEG) method</li> <li>the stochastic specular gradient (S-SPEG) method</li> <li>the hybrid specular gradient (H-SPEG) method</li> </ul> <p>Given an initial point \\(x_0 \\in \\mathbb{R}^n\\), method takes the form: </p> \\[ x_{k+1} = x_k - h_k s_k, \\] <p>where \\(h_k &gt; 0\\) is the step size and \\(s_k\\) is the specular gradient for each \\(k \\in \\mathbb{N}\\).</p> Name Rule (Formula) Type Input Description <code>constant</code> \\(h_k = a\\) <code>float</code> <code>a</code> Fixed step size for all \\(k\\). <code>not_summable</code> \\(h_k = a / \\sqrt{k}\\) <code>float</code> <code>a</code> \\(\\lim_{k \\to \\infty }h_k = 0\\), but \\(\\sum h_k = \\infty\\). <code>square_summable_not_summable</code> \\(h_k = a / (b + k)\\) <code>list</code> <code>[a, b]</code> \\(\\sum h_k^2 &lt; \\infty\\) and \\(\\sum h_k = \\infty\\). <code>geometric_series</code> \\(h_k = a \\cdot r^k\\) <code>list</code> <code>[a, r]</code> Exponentially decaying step size. <code>user_defined</code> Custom <code>Callable</code> <code>f(k)</code> User-provided function of iteration \\(k\\)."},{"location":"api/optimization/#quick-example","title":"Quick Example","text":"<pre><code>from specular.optimization.step_size import StepSize\n\n# Use a square-summable rule: h_k = 10 / (2 + k)\nstep = StepSize(name='square_summable_not_summable', parameters=[10.0, 2.0])\nh_1 = step(1)\nprint(h_1)\n# Output: 3.3333333333333335\n</code></pre>"},{"location":"api/optimization/#232-the-specular-gradient-method","title":"2.3.2. The specular gradient method","text":""},{"location":"api/optimization/#the-one-dimensional-case","title":"The one dimensional case","text":"<pre><code>import specular \n\n# Objective function: f(x) = |x|\ndef f(x):\n    return abs(x)\n\nstep_size = specular.StepSize('constant', 0.1) \n\n# Specular gradient method\nres = specular.gradient_method(f=f, x_0=1.0, step_size=step_size, form='specular gradient', max_iter=20)\n</code></pre>"},{"location":"api/optimization/#higher-dimensional-cases","title":"Higher dimensional cases","text":"<pre><code>import specular \n\n# Objective function: f(x) = sum(x^2)\ndef f(x):\n    return float(np.sum(np.array(x)**2))\n\n# Component functions for Stochastic test\n# f(x) = x1^2 + x2^2\n# f1(x) = x1^2, f2(x) = x2^2\ndef f_comp_1(x):\n    return x[0]**2\n\ndef f_comp_2(x):\n    return x[1]**2\n\nf_components = [f_comp_1, f_comp_2]\n\nx_0 = [1.0, 1.0]\nstep_size = specular.StepSize('square_summable_not_summable', [0.5, 1.0]) \n\n# Specular gradient method\nres1 = specular.gradient_method(f=f, x_0=x_0, step_size=step_size, form='specular gradient', max_iter=50)\n\n# Stochastic specular gradient method\nres2 = specular.gradient_method(f=f, x_0=x_0, step_size=step_size, form='stochastic', f_j=f_components, max_iter=100)\n\n# hybrid specular gradient method\nres3 = specular.gradient_method(f=f_quad_vector, x_0=x_0, step_size=step_size, form='hybrid', f_j=f_components, switch_iter=5, max_iter=20)\n</code></pre>"},{"location":"api/optimization/#233-optimizationresult","title":"2.3.3. <code>OptimizationResult</code>","text":"<p>The class <code>OptimizationResult</code> collects the optimization results. To get history of optimization, call <code>history()</code>.</p> <pre><code>import specular \n\n# Objective function: f(x) = sum(x^2)\ndef f(x):\n    return float(np.sum(np.array(x)**2))\n\nx_0 = [1.0, 1.0]\nstep_size = specular.StepSize('square_summable_not_summable', [0.5, 1.0]) \n\n# Specular gradient method\nres_x, res_f, res_time = specular.gradient_method(f=f, x_0=x_0, step_size=step_size, form='specular gradient', max_iter=50).history()\n</code></pre>"},{"location":"api/optimization/#234-api-reference","title":"2.3.4. API Reference","text":""},{"location":"api/optimization/#specular.optimization.step_size.StepSize","title":"<code>specular.optimization.step_size.StepSize</code>","text":"<p>Step size rules for optimization methods.</p> Source code in <code>specular\\optimization\\step_size.py</code> <pre><code>class StepSize:\n    \"\"\"\n    Step size rules for optimization methods.\n    \"\"\"\n    __options__ = [\n        'constant',\n        'not_summable',\n        'square_summable_not_summable',\n        'geometric_series',\n        'user_defined'\n    ]\n\n    def __init__(\n        self,\n        name: str,\n        parameters: float | np.floating | int | Tuple | list | np.ndarray | Callable\n    ):\n        \"\"\"\n        The step size rules for optimization methods $x_{k+1} = x_k - h_k s_k$, where $s_k$ is the search direction and $h_k &gt; 0$ is the step size at iteration $k &gt;= 1$.\n\n        Parameters:\n            name (str):\n                Options: 'constant', 'not_summable', 'square_summable_not_summable', 'geometric_series', 'user_defined'\n            parameters (float | int | tuple | list | np.ndarray | Callable):\n                The parameters required for the selected step size rule:\n\n                * 'constant': float or int\n\n                    A number `a &gt; 0` for the rule :math:`h_k = a` for each `k`.\n\n                * 'not_summable': float or int\n\n                    A number `a &gt; 0` for the rule :math:`h_k = a / sqrt{k}` for each `k`.\n\n                * 'square_summable_not_summable': list or tuple\n\n                    A pair of numbers `[a, b]`, where `a &gt; 0` and `b &gt;= 0`, for the rule :math:`h_k = a / (b + k)` for each `k`.\n\n                * 'geometric_series': list or tuple\n\n                    A pair of numbers `[a, r]`, where `a &gt; 0` and `0 &lt; r &lt; 1`, for the rule :math:`h_k = a * r^k` for each `k`.\n\n                * 'user_defined': Callable\n\n                    A function that takes the current iteration `k` as input and returns the step size (float).\n\n        Examples:\n            &gt;&gt;&gt; from specular.optimization.step_size import StepSize\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # 'constant': h_k = a\n            &gt;&gt;&gt; step = StepSize(name='constant', parameters=0.5)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # 'not_summable' rule: h_k = a / sqrt(k)\n            &gt;&gt;&gt; # a = 2.0\n            &gt;&gt;&gt; step = StepSize(name='not_summable', parameters=2.0)\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # 'square_summable_not_summable' rule: h_k = a / (b + k\n            &gt;&gt;&gt; # a = 10, b = 2\n            &gt;&gt;&gt; step = StepSize(name='square_summable_not_summable', parameters=[10.0, 2.0])\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # 'geometric_series' rule: h_k = a * r^k\n            &gt;&gt;&gt; # a = 1.0, r = 0.5\n            &gt;&gt;&gt; step = StepSize(name='geometric_series', parameters=[1.0, 0.5])\n            &gt;&gt;&gt; \n            &gt;&gt;&gt; # 'user_defined' callable.\n            &gt;&gt;&gt; # Custom rule: h_k = 1 / k^2\n            &gt;&gt;&gt; custom_rule = lambda k: 1.0 / (k**2)\n            &gt;&gt;&gt; step = StepSize(name='user_defined', parameters=custom_rule)\n        \"\"\"\n        self.step_size = name\n        self.parameters = parameters\n\n        init_methods = {\n            'constant': self._init_constant,\n            'not_summable': self._init_not_summable,\n            'square_summable_not_summable': self._init_square_summable,\n            'geometric_series': self._init_geometric,\n            'user_defined': self._init_user_defined\n        }\n\n        if name not in init_methods:\n             raise ValueError(f\"Invalid step size '{name}'. Options: {self.__options__}\")\n\n        init_methods[name]()\n\n    def __call__(self, k: int) -&gt; float:\n        \"\"\"\n        Returns the step size at iteration k.\n        \"\"\"\n        return self._rule(k)\n\n    # ==== Initialization Methods ====\n    def _init_constant(self):\n        if not isinstance(self.parameters, (float, int, np.floating)):\n            raise TypeError(f\"Invalid type: number required. Got {type(self.parameters)}\")\n\n        if self.parameters &lt;= 0:\n            raise ValueError(f\"Invalid value: positive number required. Got {self.parameters}\")\n\n        self.a = float(self.parameters)\n        self._rule = self._calc_constant\n\n    def _init_not_summable(self):\n        if not isinstance(self.parameters, (float, int, np.floating)):\n            raise TypeError(f\"Invalid type: number required. Got {type(self.parameters)}\")\n\n        if self.parameters &lt;= 0:\n            raise ValueError(f\"Invalid value: positive number required. Got {self.parameters}\")\n\n        self.a = float(self.parameters)\n        self._rule = self._calc_not_summable\n\n    def _init_square_summable(self):\n        if not isinstance(self.parameters, (tuple, list, np.ndarray)):\n            raise TypeError(f\"Invalid type: list/tuple required. Got {type(self.parameters)}\")\n\n        if len(self.parameters) != 2:\n            raise ValueError(f\"Invalid length: 2 parameters [a, b] required. Got {len(self.parameters)}\")\n\n        self.a, self.b = self.parameters[0], self.parameters[1]\n\n        if self.a &lt;= 0 or self.b &lt; 0:\n            raise ValueError(f\"Invalid parameters: a &gt; 0 and b &gt;= 0 required. Got a={self.a}, b={self.b}\")\n\n        self._rule = self._calc_square_summable_not_summable\n\n    def _init_geometric(self):\n        if not isinstance(self.parameters, (tuple, list, np.ndarray)):\n            raise TypeError(f\"Invalid type: list/tuple required. Got {type(self.parameters)}\")\n\n        if len(self.parameters) != 2:\n            raise ValueError(f\"Invalid length: 2 parameters [a, r] required. Got {len(self.parameters)}\")\n\n        self.a, self.r = self.parameters[0], self.parameters[1]\n\n        if self.a &lt;= 0 or not (0.0 &lt; self.r &lt; 1.0):\n            raise ValueError(f\"Invalid parameters: a &gt; 0 and 0 &lt; r &lt; 1 required. Got a={self.a}, r={self.r}\")\n\n        self._rule = self._calc_geometric_series\n\n    def _init_user_defined(self):\n        if not callable(self.parameters):\n            raise TypeError(\"Invalid type: callable function required.\")\n\n        self._rule = self.parameters\n\n    # ==== Calculation Methods ====\n    def _calc_constant(self, k: int) -&gt; float:\n        \"\"\"\n        h_k = a \n        \"\"\"\n        return self.a\n\n    def _calc_not_summable(self, k: int) -&gt; float:\n        \"\"\"\n        h_k = a / sqrt{k}\n        \"\"\"\n        return self.a / math.sqrt(k)\n\n    def _calc_square_summable_not_summable(self, k: int) -&gt; float:\n        \"\"\"\n        h_k = a / (b + k)\n        \"\"\"\n        return self.a / (self.b + k)\n\n    def _calc_geometric_series(self, k: int) -&gt; float:\n        \"\"\"\n        h_k = a * r**k\n        \"\"\"\n        return self.a * (self.r ** k)\n</code></pre>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__call__","title":"<code>__call__(k)</code>","text":"<p>Returns the step size at iteration k.</p> Source code in <code>specular\\optimization\\step_size.py</code> <pre><code>def __call__(self, k: int) -&gt; float:\n    \"\"\"\n    Returns the step size at iteration k.\n    \"\"\"\n    return self._rule(k)\n</code></pre>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__","title":"<code>__init__(name, parameters)</code>","text":"<p>The step size rules for optimization methods \\(x_{k+1} = x_k - h_k s_k\\), where \\(s_k\\) is the search direction and \\(h_k &gt; 0\\) is the step size at iteration \\(k &gt;= 1\\).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Options: 'constant', 'not_summable', 'square_summable_not_summable', 'geometric_series', 'user_defined'</p> required <code>parameters</code> <code>float | int | tuple | list | ndarray | Callable</code> <p>The parameters required for the selected step size rule:</p> <ul> <li> <p>'constant': float or int</p> <p>A number <code>a &gt; 0</code> for the rule :math:<code>h_k = a</code> for each <code>k</code>.</p> </li> <li> <p>'not_summable': float or int</p> <p>A number <code>a &gt; 0</code> for the rule :math:<code>h_k = a / sqrt{k}</code> for each <code>k</code>.</p> </li> <li> <p>'square_summable_not_summable': list or tuple</p> <p>A pair of numbers <code>[a, b]</code>, where <code>a &gt; 0</code> and <code>b &gt;= 0</code>, for the rule :math:<code>h_k = a / (b + k)</code> for each <code>k</code>.</p> </li> <li> <p>'geometric_series': list or tuple</p> <p>A pair of numbers <code>[a, r]</code>, where <code>a &gt; 0</code> and <code>0 &lt; r &lt; 1</code>, for the rule :math:<code>h_k = a * r^k</code> for each <code>k</code>.</p> </li> <li> <p>'user_defined': Callable</p> <p>A function that takes the current iteration <code>k</code> as input and returns the step size (float).</p> </li> </ul> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from specular.optimization.step_size import StepSize\n&gt;&gt;&gt; \n&gt;&gt;&gt; # 'constant': h_k = a\n&gt;&gt;&gt; step = StepSize(name='constant', parameters=0.5)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # 'not_summable' rule: h_k = a / sqrt(k)\n&gt;&gt;&gt; # a = 2.0\n&gt;&gt;&gt; step = StepSize(name='not_summable', parameters=2.0)\n&gt;&gt;&gt; \n&gt;&gt;&gt; # 'square_summable_not_summable' rule: h_k = a / (b + k\n&gt;&gt;&gt; # a = 10, b = 2\n&gt;&gt;&gt; step = StepSize(name='square_summable_not_summable', parameters=[10.0, 2.0])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # 'geometric_series' rule: h_k = a * r^k\n&gt;&gt;&gt; # a = 1.0, r = 0.5\n&gt;&gt;&gt; step = StepSize(name='geometric_series', parameters=[1.0, 0.5])\n&gt;&gt;&gt; \n&gt;&gt;&gt; # 'user_defined' callable.\n&gt;&gt;&gt; # Custom rule: h_k = 1 / k^2\n&gt;&gt;&gt; custom_rule = lambda k: 1.0 / (k**2)\n&gt;&gt;&gt; step = StepSize(name='user_defined', parameters=custom_rule)\n</code></pre> Source code in <code>specular\\optimization\\step_size.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    parameters: float | np.floating | int | Tuple | list | np.ndarray | Callable\n):\n    \"\"\"\n    The step size rules for optimization methods $x_{k+1} = x_k - h_k s_k$, where $s_k$ is the search direction and $h_k &gt; 0$ is the step size at iteration $k &gt;= 1$.\n\n    Parameters:\n        name (str):\n            Options: 'constant', 'not_summable', 'square_summable_not_summable', 'geometric_series', 'user_defined'\n        parameters (float | int | tuple | list | np.ndarray | Callable):\n            The parameters required for the selected step size rule:\n\n            * 'constant': float or int\n\n                A number `a &gt; 0` for the rule :math:`h_k = a` for each `k`.\n\n            * 'not_summable': float or int\n\n                A number `a &gt; 0` for the rule :math:`h_k = a / sqrt{k}` for each `k`.\n\n            * 'square_summable_not_summable': list or tuple\n\n                A pair of numbers `[a, b]`, where `a &gt; 0` and `b &gt;= 0`, for the rule :math:`h_k = a / (b + k)` for each `k`.\n\n            * 'geometric_series': list or tuple\n\n                A pair of numbers `[a, r]`, where `a &gt; 0` and `0 &lt; r &lt; 1`, for the rule :math:`h_k = a * r^k` for each `k`.\n\n            * 'user_defined': Callable\n\n                A function that takes the current iteration `k` as input and returns the step size (float).\n\n    Examples:\n        &gt;&gt;&gt; from specular.optimization.step_size import StepSize\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'constant': h_k = a\n        &gt;&gt;&gt; step = StepSize(name='constant', parameters=0.5)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'not_summable' rule: h_k = a / sqrt(k)\n        &gt;&gt;&gt; # a = 2.0\n        &gt;&gt;&gt; step = StepSize(name='not_summable', parameters=2.0)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'square_summable_not_summable' rule: h_k = a / (b + k\n        &gt;&gt;&gt; # a = 10, b = 2\n        &gt;&gt;&gt; step = StepSize(name='square_summable_not_summable', parameters=[10.0, 2.0])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'geometric_series' rule: h_k = a * r^k\n        &gt;&gt;&gt; # a = 1.0, r = 0.5\n        &gt;&gt;&gt; step = StepSize(name='geometric_series', parameters=[1.0, 0.5])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'user_defined' callable.\n        &gt;&gt;&gt; # Custom rule: h_k = 1 / k^2\n        &gt;&gt;&gt; custom_rule = lambda k: 1.0 / (k**2)\n        &gt;&gt;&gt; step = StepSize(name='user_defined', parameters=custom_rule)\n    \"\"\"\n    self.step_size = name\n    self.parameters = parameters\n\n    init_methods = {\n        'constant': self._init_constant,\n        'not_summable': self._init_not_summable,\n        'square_summable_not_summable': self._init_square_summable,\n        'geometric_series': self._init_geometric,\n        'user_defined': self._init_user_defined\n    }\n\n    if name not in init_methods:\n         raise ValueError(f\"Invalid step size '{name}'. Options: {self.__options__}\")\n\n    init_methods[name]()\n</code></pre>"},{"location":"api/optimization/#specular.optimization.solver","title":"<code>specular.optimization.solver</code>","text":""},{"location":"api/optimization/#specular.optimization.solver.gradient_method","title":"<code>gradient_method(f, x_0, step_size, h=1e-06, form='specular gradient', tol=1e-06, zero_tol=1e-08, max_iter=1000, f_j=None, m=1, switch_iter=2, record_history=True, print_bar=True)</code>","text":"<p>The specular gradient method for minimizing a nonsmooth convex function.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>callable</code> <p>The objective function to minimize.</p> required <code>x_0</code> <code>int | float | list | ndarray</code> <p>The starting point for the optimization.</p> required <code>step_size</code> <code>StepSize</code> <p>The step size <code>h_k</code>.</p> required <code>h</code> <code>float</code> <p>Mesh size used in the finite difference approximation. Must be positive.</p> <code>1e-06</code> <code>form</code> <code>str</code> <p>The form of the specular gradient method. Supported forms: <code>'specular gradient'</code>, <code>'implicit'</code>, <code>'stochastic'</code>, <code>'hybrid'</code>.</p> <code>'specular gradient'</code> <code>tol</code> <code>float</code> <p>Tolerance for iterations.</p> <code>1e-06</code> <code>zero_tol</code> <code>float</code> <p>A small threshold used to determine if the denominator <code>alpha + beta</code> is close to zero for numerical stability.</p> <code>1e-08</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations.</p> <code>1000</code> <code>f_j</code> <code>sequence of callable | callable | None</code> <p>The component function of <code>f</code>. Used for the stochastic and hybrid forms to compute a random component of the objective function.</p> <ul> <li> <p>If a sequence of callables is provided, each callable should accept a single argument (the variable <code>x</code>).</p> </li> <li> <p>If a single callable is provided, it should accept two arguments: the variable <code>x</code> and an index <code>j</code>, and return the <code>j</code>-th component function value at <code>x</code>.</p> </li> </ul> <code>None</code> <code>m</code> <code>int</code> <p>The number of component functions. Used for the stochastic and hybrid forms.</p> <code>1</code> <code>switch_iter</code> <code>int | None</code> <p>The iteration to switch from a method to another for the hybrid form. Used for the hybrid form only.</p> <code>2</code> <code>record_history</code> <code>bool</code> <p>Whether to record the history of variables and function values.</p> <code>True</code> <code>print_bar</code> <code>bool</code> <p>Whether to print the progress bar.</p> <code>True</code> <p>Returns:</p> Type Description <code>OptimizationResult</code> <p>The result of the optimization containing the solution, function value, number of iterations, runtime, and history.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>h</code> is not positive.</p> <code>TypeError</code> <p>If an unknown <code>form</code> is provided.</p> Source code in <code>specular\\optimization\\solver.py</code> <pre><code>def gradient_method(\n    f: Callable[[int | float | np.number | list | np.ndarray], int | float | np.number],\n    x_0: int | float | list | np.ndarray,\n    step_size: StepSize,\n    h: float = 1e-6,\n    form: str = 'specular gradient',\n    tol: float = 1e-6,\n    zero_tol: float = 1e-8,\n    max_iter: int = 1000,\n    f_j: Sequence[ComponentFunc] | Callable | None = None,\n    m: int = 1,\n    switch_iter: int | None = 2,\n    record_history: bool = True,\n    print_bar: bool = True\n) -&gt; OptimizationResult:\n    \"\"\"\n    The specular gradient method for minimizing a nonsmooth convex function.\n\n    Parameters:\n        f (callable):\n            The objective function to minimize.\n        x_0 (int | float | list | np.ndarray):\n            The starting point for the optimization.\n        step_size (StepSize):\n            The step size `h_k`.\n        h (float, optional):\n            Mesh size used in the finite difference approximation. Must be positive.\n        form (str, optional):\n            The form of the specular gradient method.\n            Supported forms: ``'specular gradient'``, ``'implicit'``, ``'stochastic'``, ``'hybrid'``.\n        tol (float, optional):\n            Tolerance for iterations.\n        zero_tol (float, optional):\n            A small threshold used to determine if the denominator ``alpha + beta`` is close to zero for numerical stability.\n        max_iter (int, optional):\n            Maximum number of iterations.\n        f_j (sequence of callable | callable | None, optional):\n            The component function of ``f``.\n            Used for the stochastic and hybrid forms to compute a random component of the objective function.\n\n            * If a sequence of callables is provided, each callable should accept a single argument (the variable `x`).\n\n            * If a single callable is provided, it should accept two arguments: the variable `x` and an index `j`, and return the `j`-th component function value at `x`.\n        m (int, optional):\n            The number of component functions.\n            Used for the stochastic and hybrid forms.\n        switch_iter (int | None, optional):\n            The iteration to switch from a method to another for the hybrid form.\n            Used for the hybrid form only.\n        record_history (bool, optional):\n            Whether to record the history of variables and function values.\n        print_bar (bool, optional):\n            Whether to print the progress bar.\n\n    Returns:\n        The result of the optimization containing the solution, function value, number of iterations, runtime, and history.\n\n    Raises:\n        ValueError:\n            If ``h`` is not positive.\n        TypeError:\n            If an unknown ``form`` is provided.\n    \"\"\"\n\n    if h is None or h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    x = np.array(x_0, dtype=float).copy()\n    n = x.size\n\n    all_history = {}\n    x_history = []\n    f_history = []\n\n    start_time = time.time()\n\n    # the n-dimensional case\n    if n &gt; 1:\n        if form == 'specular gradient':\n            res_x, res_f, res_k = _vector(f, f_history, x, x_history, step_size, h, tol, zero_tol, max_iter, record_history, print_bar)\n\n        elif form == 'stochastic':\n            if f_j is None:\n                raise ValueError(\"Component functions 'f_j' must be provided for the stochastic form.\")\n\n            form = 'stochastic specular gradient'\n            res_x, res_f, res_k = _vector_stochastic(f, f_history, x, x_history, step_size, h, tol, zero_tol, f_j, m, max_iter, record_history, print_bar) # type: ignore\n\n        elif form == 'hybrid':\n            if f_j is None:\n                raise ValueError(\"Component functions 'f_j' must be provided for the stochastic form.\")\n\n            # Phase 1: deterministic\n            form = 'hybrid specular gradient'\n            switch_iter = switch_iter if switch_iter is not None else max_iter\n            remaining_iter = max_iter - switch_iter\n\n            # Phase 2: stochastic\n            res_x, res_f, res_k = _vector(f, f_history, x, x_history, step_size, h, tol, zero_tol, switch_iter, record_history, print_bar)\n            res_x, res_f, res_k = _vector_stochastic(f, f_history, res_x, x_history, step_size, h, tol, zero_tol, f_j, m, remaining_iter, record_history, print_bar) # type: ignore\n\n        else:\n            raise TypeError(f\"Unknown form '{form}'. Supported forms: {SUPPORTED_METHODS}\")\n\n    # the one-dimensional case\n    elif n == 1:\n        x = x.item()\n\n        if form == 'specular gradient':\n            res_x, res_f, res_k = _scalar(f, f_history, x, x_history, step_size, h, tol, zero_tol, max_iter, record_history, print_bar)\n\n        elif form == 'implicit':\n            form = 'implicit specular gradient'\n            res_x, res_f, res_k = _scalar_implicit(f, f_history, x, x_history, step_size, h, tol, max_iter, record_history, print_bar)\n\n        else:\n            raise TypeError(f\"Unknown form '{form}'. Supported forms: {SUPPORTED_METHODS}\")\n\n    else:\n        raise TypeError(f\"Unknown form '{form}'. Supported forms: {SUPPORTED_METHODS}\")\n\n    runtime = time.time() - start_time\n\n    if record_history:\n        all_history[\"variables\"] = x_history\n        all_history[\"values\"] = f_history\n\n    return OptimizationResult(\n        method=form,\n        solution=res_x,\n        func_val=res_f,\n        iteration=res_k,\n        runtime=runtime,\n        all_history=all_history\n    ) \n</code></pre>"},{"location":"api/optimization/#specular.optimization.result","title":"<code>specular.optimization.result</code>","text":""},{"location":"api/optimization/#specular.optimization.result.OptimizationResult","title":"<code>OptimizationResult</code>","text":"Source code in <code>specular\\optimization\\result.py</code> <pre><code>class OptimizationResult:\n    def __init__(\n            self, \n            method: str,\n            solution: np.ndarray, \n            func_val: int | float | np.number, \n            iteration: int, \n            runtime: float,\n            all_history: dict\n    ):\n        self.method = method\n        self.solution = solution\n        self.func_val = func_val\n        self.iteration = iteration\n        self.runtime = runtime\n        self.all_history = all_history\n\n    def __repr__(self):\n        return (\n            f\"[{self.method}]\\n\"\n            f\"    solution: {self.solution}\\n\"\n            f\"  func value: {self.func_val}\\n\"\n            f\"   iteration: {self.iteration}\"\n        )\n\n    def last_record(\n        self\n    ) -&gt; Tuple[float, float, float]:\n        \"\"\"\n        Returns the final solution x, the value of f at x, and the runtime as a tuple.\n\n        Returns:\n            (x, f(x), runtime)\n        \"\"\"\n        return self.solution, self.func_val, self.runtime # type: ignore\n\n    def history(\n        self\n    ) -&gt; Tuple[np.ndarray, np.ndarray, float]:\n        \"\"\"\n        Returns the time grid and the numerical solution as a tuple.\n\n        Returns:\n            (x_history, f_history, runtime)\n        \"\"\"\n        return self.all_history[\"variables\"], self.all_history[\"values\"], self.runtime\n</code></pre>"},{"location":"api/optimization/#specular.optimization.result.OptimizationResult.history","title":"<code>history()</code>","text":"<p>Returns the time grid and the numerical solution as a tuple.</p> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, float]</code> <p>(x_history, f_history, runtime)</p> Source code in <code>specular\\optimization\\result.py</code> <pre><code>def history(\n    self\n) -&gt; Tuple[np.ndarray, np.ndarray, float]:\n    \"\"\"\n    Returns the time grid and the numerical solution as a tuple.\n\n    Returns:\n        (x_history, f_history, runtime)\n    \"\"\"\n    return self.all_history[\"variables\"], self.all_history[\"values\"], self.runtime\n</code></pre>"},{"location":"api/optimization/#specular.optimization.result.OptimizationResult.last_record","title":"<code>last_record()</code>","text":"<p>Returns the final solution x, the value of f at x, and the runtime as a tuple.</p> <p>Returns:</p> Type Description <code>Tuple[float, float, float]</code> <p>(x, f(x), runtime)</p> Source code in <code>specular\\optimization\\result.py</code> <pre><code>def last_record(\n    self\n) -&gt; Tuple[float, float, float]:\n    \"\"\"\n    Returns the final solution x, the value of f at x, and the runtime as a tuple.\n\n    Returns:\n        (x, f(x), runtime)\n    \"\"\"\n    return self.solution, self.func_val, self.runtime # type: ignore\n</code></pre>"},{"location":"examples/","title":"3. Examples","text":"<p>This directory includes applications of specular differentiation. Each subdirectory is based on a paper.</p>"},{"location":"examples/#31-ordinary-differential-equation","title":"3.1. Ordinary differential equation","text":"<ul> <li>Directory: <code>examples/ode/</code></li> </ul>"},{"location":"examples/#2026-jung","title":"2026-Jung","text":"<ul> <li>Directory: <code>examples/ode/2026-Jung/</code></li> <li>Keywords: generalized differentiation, Mean Value Theorem, Fermat's Theorem, explicit Euler scheme, implicit Euler scheme, Crank-Nicolson scheme</li> <li>Reference: TBA</li> </ul>"},{"location":"examples/#32-optimization","title":"3.2. Optimization","text":"<ul> <li>Directory: <code>examples/optimization/</code></li> <li>Requirements: </li> <li><code>scipy</code> &gt;= 1.10.0</li> <li><code>torch</code> &gt;= 2.0.0</li> </ul>"},{"location":"examples/#2024-jung-oh","title":"2024-Jung-Oh","text":"<ul> <li>Directory: <code>examples/optimization/2024-Jung-Oh/</code></li> <li>Keywords: nonsmooth convex optimization, subgradient methods, non-differentiable convex functions, generalization of derivatives, convergence rate</li> <li>Reference: K. Jung and J. Oh. Nonsmooth convex optimization using the specular gradient method with root-linear convergence. arXiv preprint arXiv:2210.06933, 2024</li> </ul>"},{"location":"examples/#2026-jung_1","title":"2026-Jung","text":"<ul> <li>Directory: <code>examples/optimization/2026-Jung/</code></li> <li>Keywords: TBA</li> <li>Reference: TBA</li> </ul>"}]}