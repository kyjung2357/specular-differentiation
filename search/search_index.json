{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Specular Differentiation","text":"<p>The Python package <code>specular</code> implements specular differentiation which generalizes classical differentiation. This implementation strictly follows the definitions, notations, and results in [1] and [2].</p> <p>A specular derivative (the red line) can be understood as the average of the inclination angles of the right and left derivatives.  In contrast, a symmetric derivative (the purple line) is the average of the right and left derivatives. Their difference is illustrated as in the following figure.</p> <p></p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Applications</li> <li>Documentation</li> <li>LaTeX macro</li> <li>Citing specular-differentiation</li> <li>References</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#requirements","title":"Requirements","text":"<p><code>specular-differentiation</code> requires:</p> <ul> <li>Python &gt;= 3.11</li> <li><code>ipython</code> &gt;= 8.12.3</li> <li><code>matplotlib</code> &gt;= 3.10.8</li> <li><code>numpy</code> &gt;= 2.4.0</li> <li><code>pandas</code> &gt;= 2.3.3</li> <li><code>tqdm</code> &gt;= 4.67.1</li> </ul>"},{"location":"#user-installation","title":"User installation","text":"<p>Standard Installation (NumPy backend)</p> <pre><code>pip install specular-differentiation\n</code></pre> <p>Advanced Installation (JAX backend)</p> <pre><code>pip install \"specular-differentiation[jax]\"\n</code></pre> <p>See the documentation for advanced installation (JAX backend, Pytest).</p>"},{"location":"#quick-start","title":"Quick start","text":"<p>The following simple example calculates the specular derivative of the ReLU function \\(f(x) = max(0, x)\\) at the origin.</p> <pre><code>import specular\n\nReLU = lambda x: max(x, 0)\nspecular.derivative(ReLU, x=0)\n# Output: 0.41421356237309515\n</code></pre>"},{"location":"#applications","title":"Applications","text":"<p>Specular differentiation is defined in normed vector spaces, allowing for applications in higher-dimensional Euclidean spaces.  The <code>specular</code> package includes the following applications.</p>"},{"location":"#ordinary-differential-equation","title":"Ordinary differential equation","text":"<ul> <li>Directory: <code>examples/ode/</code></li> <li>References: [1], [3], [4]</li> </ul> <p>In [1], seven schemes are proposed for solving ODEs numerically:</p> <ul> <li>the specular Euler scheme of Type 1~6</li> <li>the specular trigonometric scheme</li> </ul> <p>The following example shows that the specular Euler schemes of Type 5 and 6 yield more accurate numerical solutions than classical schemes: the explicit and implicit Euler schemes and the Crank-Nicolson scheme.</p> <p></p>"},{"location":"#optimization","title":"Optimization","text":"<ul> <li>Directory: <code>examples/optimization/</code></li> <li>References: [2], [5]</li> </ul> <p>In [2], three methods are proposed for optimizing nonsmooth convex objective functions:</p> <ul> <li>the specular gradient (SPEG) method</li> <li>the stochastic specular gradient (S-SPEG) method</li> <li>the hybrid specular gradient (H-SPEG) method</li> </ul> <p>The following example compares the three proposed methods with the classical methods: gradient descent (GD), Adaptive Moment Estimation (Adam), and Broyden-Fletcher-Goldfarb-Shanno (BFGS).</p> <p></p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>1. Getting Started</li> <li>2. API Reference</li> <li>3. Examples</li> </ul>"},{"location":"#latex-macro","title":"LaTeX macro","text":"<p>To use the specular differentiation symbol in your LaTeX document, add the following code to your preamble (before <code>\\begin{document}</code>):</p> <pre><code>% Required packages\n\\usepackage{graphicx}\n\\usepackage{bm}\n\n% Definition of Specular Differentiation symbol\n\\newcommand\\sd[1][.5]{\\mathbin{\\vcenter{\\hbox{\\scalebox{#1}{\\,$\\bm{\\wedge}$}}}}}\n</code></pre>"},{"location":"#usage-examples","title":"Usage examples","text":"<p>Use the symbol in your document (after <code>\\begin{document}</code>):</p> <pre><code>% A specular derivative in the one-dimensional Euclidean space\n$f^{\\sd}(x)$\n\n% A specular directional derivative in normed vector spaces\n$\\partial^{\\sd}_v f(x)$\n</code></pre>"},{"location":"#citing-specular-differentiation","title":"Citing specular-differentiation","text":"<p>To cite this repository:</p> <pre><code>@software{specular2026,\n  author  = {Kiyuob Jung},\n  title   = {specular-differentiation},\n  url     = {https://github.com/your-id/specular-differentiation},\n  version = {0.13.0},\n  year    = {2026},\n  doi     = {Pending},\n}\n</code></pre>"},{"location":"#references","title":"References","text":"<p>[1] K. Jung. Nonlinear numerical schemes using specular differentiation for initial value problems of first-order ordinary differential equations. arXiv preprint arXiv:????.?????, TBA.</p> <p>[2] K. Jung. Specular differentiation in normed vector spaces and its applications to nonsmooth convex optimization. arXiv preprint arXiv:????.?????, TBA. </p> <p>[3] K. Jung and J. Oh. The specular derivative. arXiv preprint arXiv:2210.06062, 2022.</p> <p>[4] K. Jung and J. Oh. The wave equation with specular derivatives. arXiv preprint arXiv:2210.06933, 2022.</p> <p>[5] K. Jung and J. Oh. Nonsmooth convex optimization using the specular gradient method with root-linear convergence. arXiv preprint arXiv:2210.06933, 2024.</p>"},{"location":"started/","title":"1. Getting Started","text":""},{"location":"started/#11-user-installation","title":"1.1. User installation","text":"<p>Standard Installation (NumPy backend)</p> <p>The package is available on PyPI:</p> <pre><code>pip install specular-differentiation\n</code></pre> <p>Check the version:</p> <pre><code>import specular\n\nprint(\"version: \", specular.__version__)\n# Output: version:  1.0.0\n</code></pre> <p>Advanced Installation (JAX backend)</p> <p>By default, the package uses the NumPy backend (CPU).  To enable hardware acceleration, you can install the package with the JAX backend (GPU/TPU).  This adds the following dependencies:</p> <ul> <li>JAX (<code>jax</code>, <code>jaxlib</code> &gt;= 0.4):</li> </ul> <pre><code>pip install \"specular-differentiation[jax]\"\n</code></pre> <p>Note</p> <p>This feature is experimental for now. See 2.4 JAX backend.</p> <p>Developer installation</p> <p>To install all dependencies including tests, docs, and examples. This adds the following dependencies:</p> <ul> <li>JAX (<code>jax</code>, <code>jaxlib</code> &gt;= 0.4):</li> <li>SciPy (<code>scipy</code> &gt;= 1.10.0)</li> <li>PyTorch (<code>torch</code> &gt;= 2.0.0)</li> <li>Pytest (<code>pytest</code> &gt;= 7.0)</li> </ul> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"started/#12-quick-start","title":"1.2. Quick start","text":"<p>The following simple example calculates the specular derivative of the ReLU function \\(f(x) = max(0, x)\\) at the origin.</p> <pre><code>import specular\n\nReLU = lambda x: max(x, 0)\nspecular.derivative(ReLU, x=0)\n# Output: 0.41421356237309515\n</code></pre>"},{"location":"started/#13-jax-backend-usage","title":"1.3. JAX Backend Usage","text":"<p>To leverage JAX for hardware acceleration instead of the standard NumPy backend, import <code>specular.jax</code>:</p> <pre><code>import specular.jax as sjax\n\nReLU = lambda x: jax.numpy.maximum(x, 0)\nsjax.derivative(ReLU, 0.0)\n# Output: Array(0.41421354, dtype=float32)\n</code></pre> <p>To enable 64-bit precision (double precision), update the JAX configuration as follows:</p> <pre><code>import jax\njax.config.update(\"jax_enable_x64\", True)\n\nimport specular.jax as sjax\n\nReLU = lambda x: jax.numpy.maximum(x, 0)\nsjax.derivative(ReLU, 0.0)\n# Output: Array(0.41421356, dtype=float64)\n</code></pre>"},{"location":"api/","title":"2. API Reference","text":"<p>The specular package consists of the following modules and subpackages.</p>"},{"location":"api/#21-calculation","title":"2.1. Calculation","text":"<ul> <li> <p>The <code>specular.calculation</code> module provides five primary functions to calculate specular differentiation, depending on the dimension of input.</p> Function Space Description Input Type Output Type <code>derivative</code> \\(\\mathbb{R} \\to \\mathbb{R}^m\\) specular derivative <code>float</code> <code>float</code>, <code>np.ndarray</code> <code>directional_derivative</code> \\(\\mathbb{R}^n \\to \\mathbb{R}\\) specular directional derivative in direction \\(v \\in \\mathbb{R}^n\\) <code>np.ndarry</code> <code>float</code> <code>partial_derivative</code> \\(\\mathbb{R}^n \\to \\mathbb{R}\\) specular partial derivative w.r.t. \\(v = x_i\\) <code>np.ndarray</code> <code>float</code> <code>gradient</code> \\(\\mathbb{R}^n \\to \\mathbb{R}\\) specular gradient vector <code>np.ndarray</code> <code>np.ndarray</code> <code>jacobian</code> \\(\\mathbb{R}^n \\to \\mathbb{R}^m\\) specular jacobian matrix <code>np.ndarray</code> <code>np.ndarray</code> </li> </ul>"},{"location":"api/#22-ode","title":"2.2 ODE","text":"<ul> <li>Let the source function \\(F:[t_0, T] \\times \u211d \\to \u211d\\) be given, and the initial data \\(u_0:\u211d \\to \u211d\\) be given.  Consider the initial value problem:</li> </ul> \\[ u'(t) = F(t, u(t)) \\] <p>with the initial condition \\(u(t_0) = u_0(t_0)\\).</p> <ul> <li> <p>To solve the problem numerically, the subpackage <code>specular.ode.solver</code> provides the following numerical schemes:</p> </li> <li> <p>the specular Euler scheme (Type 1 ~ 6)</p> </li> <li>the specular trigonometric scheme</li> <li>the explicit Euler scheme</li> <li>the implicit Euler scheme</li> <li> <p>the Crank-Nicolson scheme</p> </li> <li> <p>The <code>specular.ode.result</code> module provides the <code>ODEResult</code> class to store the results.</p> </li> </ul>"},{"location":"api/#23-optimization","title":"2.3 Optimization","text":"<ul> <li>Consider the optimization problem:</li> </ul> \\[ \\min_{x \\in \\mathbb{R}^n} f(x), \\] <p>where \\(f:\\mathbb{R}^n \\to \\mathbb{R}\\) is convex.</p> <ul> <li> <p>To solve the problem numerically, the subpackage <code>specular.optimization.solver</code> provides the following methods:</p> </li> <li> <p>the specular gradient (SPEG) method</p> </li> <li>the stochastic specular gradient (S-SPEG) method</li> <li> <p>the hybrid specular gradient (H-SPEG) method</p> </li> <li> <p>Given an initial point \\(x_0 \\in \\mathbb{R}^n\\), method takes the form: </p> </li> </ul> \\[ x_{k+1} = x_k - h_k s_k, \\] <p>where \\(h_k &gt; 0\\) is the step size and \\(s_k\\) is the specular gradient for each \\(k \\in \\mathbb{N}\\).</p> <ul> <li> <p>The <code>specular.optimization.step_size</code> module provides the <code>StepSize</code> class to define step size \\(h_k\\).</p> Name Rule (Formula) Type Input Description <code>constant</code> \\(h_k = a\\) <code>float</code> <code>a</code> Fixed step size for all \\(k\\). <code>not_summable</code> \\(h_k = a / \\sqrt{k}\\) <code>float</code> <code>a</code> \\(\\lim_{k \\to \\infty }h_k = 0\\), but \\(\\sum h_k = \\infty\\). <code>square_summable_not_summable</code> \\(h_k = a / (b + k)\\) <code>list</code> <code>[a, b]</code> \\(\\sum h_k^2 &lt; \\infty\\) and \\(\\sum h_k = \\infty\\). <code>geometric_series</code> \\(h_k = a \\cdot r^k\\) <code>list</code> <code>[a, r]</code> Exponentially decaying step size. <code>user_defined</code> Custom <code>Callable</code> <code>f(k)</code> User-provided function of iteration \\(k\\). </li> <li> <p>The <code>specular.optimization.result</code> module provides the <code>OptimizationResult</code> class to store the results.</p> </li> </ul>"},{"location":"api/#24-jax-backend","title":"2.4 JAX Backend","text":"<ul> <li> <p>The <code>specular.jax.calculation</code> module provides JAX implementations of the calculations in <code>specular.calculation</code>.</p> </li> <li> <p>The <code>specular.jax.optimization.solver</code> module provides JAX implementations of the numerical schemes in <code>specular.optimization.solver</code>.</p> </li> </ul>"},{"location":"api/calculation/","title":"2.1. Calculation","text":""},{"location":"api/calculation/#specular.calculation","title":"<code>specular.calculation</code>","text":"<p>======================================== Calculations of specular differentiation ========================================</p> <p>This module provides implementations of the function :math:<code>\\mathcal{A}</code>, specular directional derivatives, specular partial derivatives, specular derivatives, specular gradients, and specular Jacobians.</p> <p>Computations are based on the finite difference approximation of one-sided (directional) derivatives.</p>"},{"location":"api/calculation/#specular.calculation.A","title":"<code>A(alpha, beta, zero_tol=1e-08)</code>","text":"<p>Compute the function :math:<code>\\mathcal{A}</code> from one-sided directional derivatives.</p> <p>Given real numbers <code>alpha</code> and <code>beta</code>, the function :math:<code>\\mathcal{A}:\\mathbb{R}^2 \\to \\mathbb{R}</code> is defined by </p> <pre><code>:math:`\\mathcal{A}(\\alpha, \\beta) = \\frac{\\alpha \\beta - 1 + \\sqrt((1 + \\alpha^2)(1 + \\beta^2))}{\\alpha + \\beta}` if ``alpha + beta != 0``; otherwise, it returns ``0``.\n</code></pre>"},{"location":"api/calculation/#specular.calculation.A--parameters","title":"Parameters","text":"<p>alpha : float | np.number | int | list | np.ndarray     One-sided directional derivative. beta : float | np.number | int | list | np.ndarray     One-sided directional derivative. zero_tol : float, optional     A small threshold used to determine if the denominator <code>alpha + beta</code> is close to zero for numerical stability.     Default: <code>1e-8</code>.</p>"},{"location":"api/calculation/#specular.calculation.A--returns","title":"Returns","text":"<p>float     The function :math:<code>\\mathcal{A}</code>.</p>"},{"location":"api/calculation/#specular.calculation.A--raises","title":"Raises","text":"<p>ValueError     If <code>alpha</code> and <code>beta</code> have different shape.</p>"},{"location":"api/calculation/#specular.calculation.A--examples","title":"Examples","text":"<p>import specular specular.calculation.A(1.0, 2.0) 1.3874258867227933 specular.calculation.A([1.0, 2.4], [2.0, 4.1]) array([1.38742589, 3.04807583])</p> Source code in <code>specular\\calculation.py</code> <pre><code>def A(\n    alpha: float | np.number | int | list | np.ndarray,\n    beta: float | np.number | int | list | np.ndarray,\n    zero_tol: float = 1e-8\n) -&gt; float | np.ndarray:\n    \"\"\"\n    Compute the function :math:`\\\\mathcal{A}` from one-sided directional derivatives.\n\n    Given real numbers ``alpha`` and ``beta``, the function :math:`\\\\mathcal{A}:\\\\mathbb{R}^2 \\\\to \\\\mathbb{R}` is defined by \n\n        :math:`\\\\mathcal{A}(\\\\alpha, \\\\beta) = \\\\frac{\\\\alpha \\\\beta - 1 + \\\\sqrt((1 + \\\\alpha^2)(1 + \\\\beta^2))}{\\\\alpha + \\\\beta}` if ``alpha + beta != 0``; otherwise, it returns ``0``.\n\n    Parameters\n    ----------\n    alpha : float | np.number | int | list | np.ndarray\n        One-sided directional derivative.\n    beta : float | np.number | int | list | np.ndarray\n        One-sided directional derivative.\n    zero_tol : float, optional\n        A small threshold used to determine if the denominator ``alpha + beta`` is close to zero for numerical stability.\n        Default: ``1e-8``.\n\n    Returns\n    -------\n    float\n        The function :math:`\\\\mathcal{A}`.\n\n    Raises\n    ------\n    ValueError\n        If ``alpha`` and ``beta`` have different shape.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import specular\n    &gt;&gt;&gt; specular.calculation.A(1.0, 2.0)\n    1.3874258867227933\n    &gt;&gt;&gt; specular.calculation.A([1.0, 2.4], [2.0, 4.1])\n    array([1.38742589, 3.04807583])\n    \"\"\"\n    if np.isscalar(alpha) and np.isscalar(beta):\n        return _A_scalar(alpha, beta, zero_tol=zero_tol) # type: ignore\n\n    return _A_vector(alpha, beta, zero_tol=zero_tol) # type: ignore\n</code></pre>"},{"location":"api/calculation/#specular.calculation.derivative","title":"<code>derivative(f, x, h=1e-06, zero_tol=1e-08)</code>","text":"<p>Approximates the specular derivative of a function :math:<code>f:\\mathbb{R} \\to \\mathbb{R}^m</code> at a scalar point <code>x</code>.</p> <p>If <code>f</code> returns a scalar, the result is a float. If <code>f</code> returns a vector, the result is a vector (component-wise derivative).</p>"},{"location":"api/calculation/#specular.calculation.derivative--parameters","title":"Parameters","text":"<p>f : callable     A function of a single real variable, returning a scalar or a vector. x : float | np.number | int      The point at which the derivative is evaluated. h : float, optional     Mesh size used in the finite difference approximation. Must be positive.     Default: <code>1e-6</code>. zero_tol : float, optional      A small threshold used to determine if the denominator <code>alpha + beta</code> is close to zero for numerical stability.     Default: <code>1e-8</code>.</p>"},{"location":"api/calculation/#specular.calculation.derivative--returns","title":"Returns","text":"<p>float | np.ndarray     The approximated specular derivative of <code>f</code> at <code>x</code>.</p>"},{"location":"api/calculation/#specular.calculation.derivative--raises","title":"Raises","text":"<p>TypeError     If the type of <code>x</code> is not a scalar. ValueError     If <code>h</code> is not positive.</p>"},{"location":"api/calculation/#specular.calculation.derivative--examples","title":"Examples","text":"<p>import specular f = lambda x: max(x, 0.0) specular.derivative(f, x=0.0) 0.41421356237309515 f = lambda x: abs(x) specular.derivative(f, x=0.0) 0.0</p> Source code in <code>specular\\calculation.py</code> <pre><code>def derivative(\n    f: Callable[[int | float | np.number], int | float | np.number | list | np.ndarray],\n    x: float | np.number | int,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; float | np.ndarray:\n    \"\"\"\n    Approximates the specular derivative of a function :math:`f:\\\\mathbb{R} \\\\to \\\\mathbb{R}^m` at a scalar point ``x``.\n\n    If ``f`` returns a scalar, the result is a float.\n    If ``f`` returns a vector, the result is a vector (component-wise derivative).\n\n\n    Parameters\n    ----------\n    f : callable\n        A function of a single real variable, returning a scalar or a vector.\n    x : float | np.number | int \n        The point at which the derivative is evaluated.\n    h : float, optional\n        Mesh size used in the finite difference approximation. Must be positive.\n        Default: ``1e-6``.\n    zero_tol : float, optional \n        A small threshold used to determine if the denominator ``alpha + beta`` is close to zero for numerical stability.\n        Default: ``1e-8``.\n\n    Returns\n    -------\n    float | np.ndarray\n        The approximated specular derivative of ``f`` at ``x``.\n\n    Raises\n    ------\n    TypeError\n        If the type of ``x`` is not a scalar.\n    ValueError\n        If ``h`` is not positive.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import specular\n    &gt;&gt;&gt; f = lambda x: max(x, 0.0)\n    &gt;&gt;&gt; specular.derivative(f, x=0.0)\n    0.41421356237309515\n    &gt;&gt;&gt; f = lambda x: abs(x)\n    &gt;&gt;&gt; specular.derivative(f, x=0.0)\n    0.0\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    try:\n        x = float(x)\n\n    except TypeError:\n        raise TypeError(\n            f\"Input 'x' must be a scalar. \"\n            f\"Got {type(x).__name__}. \"\n            \"Use `specular.directional_derivative`, `specular.gradient`, or `specular.jacobian` for vectors inputs.\"\n        )\n\n    f_val = f(x)\n\n    # f is real-valued\n    if np.ndim(f_val) == 0:\n        alpha = (f(x + h) - f_val) / h # type: ignore\n        beta = (f_val - f(x - h)) / h # type: ignore\n\n        return _A_scalar(alpha, beta, zero_tol=zero_tol) # type: ignore\n\n    # f is vector-valued\n    else:\n        f_val = np.asarray(f_val, dtype=float)\n        f_right = np.asarray(f(x + h), dtype=float)\n        f_left = np.asarray(f(x - h), dtype=float)\n\n        alpha = (f_right - f_val) / h\n        beta = (f_val - f_left) / h\n\n        return _A_vector(alpha, beta, zero_tol=zero_tol)\n</code></pre>"},{"location":"api/calculation/#specular.calculation.directional_derivative","title":"<code>directional_derivative(f, x, v, h=1e-06, zero_tol=1e-08)</code>","text":"<p>Approximates the specular directional derivative of a function :math:<code>f:\\mathbb{R}^n \\to \\mathbb{R}</code> at a point <code>x</code> in the direction <code>v</code>. The one-sided derivatives <code>alpha</code> and <code>beta</code> are approximated. The function :math:<code>\\mathcal{A}</code> returns the specular directional derivative for each component of the vector <code>x</code>.</p>"},{"location":"api/calculation/#specular.calculation.directional_derivative--parameters","title":"Parameters","text":"<p>f : callable     A real-valued function defined on an open subset of :math:<code>\\mathbb{R}^n</code>. x : list | np.ndarray     The point at which the derivative is evaluated. v : list | np.ndarray     The direction in which the derivative is taken. h : float, optional     Mesh size used in the finite difference approximation. Must be positive.     Default: <code>1e-6</code>. zero_tol : float, optional     A small threshold used to determine if the denominator <code>alpha + beta</code> is close to zero for numerical stability.     Default: <code>1e-8</code>.</p>"},{"location":"api/calculation/#specular.calculation.directional_derivative--returns","title":"Returns","text":"<p>float     The approximated specular directional derivative of <code>f</code> at <code>x</code> in the direction <code>v</code> as a scalar.</p>"},{"location":"api/calculation/#specular.calculation.directional_derivative--raises","title":"Raises","text":"<p>TypeError     If <code>x</code> or <code>v</code> are not of valid array-like types. ValueError     If <code>x</code> and <code>v</code> have different shape.     If <code>h</code> is not positive.</p>"},{"location":"api/calculation/#specular.calculation.directional_derivative--examples","title":"Examples","text":"<p>import specular import math f = lambda x: math.sqrt(x[0]2 + x[1]2 + x[2]**2) specular.directional_derivative(f, x=[0.0, 0.1, -0.1], v=[1.0, -1.0, 2.0]) -2.1213203434708223</p> Source code in <code>specular\\calculation.py</code> <pre><code>def directional_derivative(\n    f: Callable[[list | np.ndarray], int | float | np.number],\n    x: list | np.ndarray,\n    v: list | np.ndarray,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; float:\n    \"\"\"\n    Approximates the specular directional derivative of a function :math:`f:\\\\mathbb{R}^n \\\\to \\\\mathbb{R}` at a point ``x`` in the direction ``v``.\n    The one-sided derivatives ``alpha`` and ``beta`` are approximated.\n    The function :math:`\\\\mathcal{A}` returns the specular directional derivative for each component of the vector ``x``.\n\n    Parameters\n    ----------\n    f : callable\n        A real-valued function defined on an open subset of :math:`\\\\mathbb{R}^n`.\n    x : list | np.ndarray\n        The point at which the derivative is evaluated.\n    v : list | np.ndarray\n        The direction in which the derivative is taken.\n    h : float, optional\n        Mesh size used in the finite difference approximation. Must be positive.\n        Default: ``1e-6``.\n    zero_tol : float, optional\n        A small threshold used to determine if the denominator ``alpha + beta`` is close to zero for numerical stability.\n        Default: ``1e-8``.\n\n    Returns\n    -------\n    float\n        The approximated specular directional derivative of ``f`` at ``x`` in the direction ``v`` as a scalar.\n\n    Raises\n    ------\n    TypeError\n        If ``x`` or ``v`` are not of valid array-like types.\n    ValueError\n        If ``x`` and ``v`` have different shape.\n        If ``h`` is not positive.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import specular\n    &gt;&gt;&gt; import math\n    &gt;&gt;&gt; f = lambda x: math.sqrt(x[0]**2 + x[1]**2 + x[2]**2)\n    &gt;&gt;&gt; specular.directional_derivative(f, x=[0.0, 0.1, -0.1], v=[1.0, -1.0, 2.0])\n    -2.1213203434708223\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    x = np.asarray(x, dtype=float)\n    v = np.asarray(v, dtype=float)\n\n    if x.ndim == 0:\n        raise TypeError(\n            f\"Input 'x' must be a vector. \"\n            f\"Got {type(x).__name__}. \"\n            \"Use `specular.derivative` for scalar inputs.\"\n        )\n\n    if v.ndim == 0:\n        raise TypeError(\n            \"Input 'v' must be a vector. \"\n            f\"Got {type(v).__name__}.\"\n        )\n\n    if x.shape != v.shape:\n        raise ValueError(f\"Shape mismatch: x {x.shape} vs v {v.shape}\")\n\n    f_val = f(x) \n\n    if np.ndim(f_val) != 0:\n        raise ValueError(\n            \"Function f must return a scalar value. \"\n            f\"Got shape {np.shape(f_val)}.\"\n        )\n\n    alpha = (f(x + h * v) - f_val)/h\n    beta = (f_val - f(x - h * v))/h\n\n    return _A_scalar(alpha, beta, zero_tol=zero_tol)\n</code></pre>"},{"location":"api/calculation/#specular.calculation.gradient","title":"<code>gradient(f, x, h=1e-06, zero_tol=1e-08)</code>","text":"<p>Approximates the specular gradient of a real-valued function :math:<code>f:\\mathbb{R}^n \\to \\mathbb{R}</code> at point <code>x</code> for <code>n &gt; 1</code>.</p> <p>The specular gradient is defined as the vector of all partial specular derivatives along the standard basis directions.</p>"},{"location":"api/calculation/#specular.calculation.gradient--parameters","title":"Parameters","text":"<p>f : callable     A real-valued function defined on :math:<code>\\mathbb{R}^n</code>. x : list | np.ndarray     The point at which the specular gradient is evaluated. h : float, optional     Mesh size used in the finite difference approximation. Must be positive.     Default: <code>1e-6</code>. zero_tol : float, optional     A small threshold used to determine if the denominator <code>alpha + beta</code> is close to zero for numerical stability.     Default: <code>1e-8</code>.</p>"},{"location":"api/calculation/#specular.calculation.gradient--returns","title":"Returns","text":"<p>np.ndarray     The approximated specular gradient of <code>f</code> at <code>x</code> as a vector.</p>"},{"location":"api/calculation/#specular.calculation.gradient--raises","title":"Raises","text":"<p>TypeError     If <code>x</code> is not of a valid array-like type. ValueError     If <code>f</code> does not return a scalar value.     If <code>h</code> is not positive.</p>"},{"location":"api/calculation/#specular.calculation.gradient--examples","title":"Examples","text":"<p>import specular import numpy as np f = lambda x: np.linalg.norm(x) specular.gradient(f, x=[1.4, -3.47, 4.57, 9.9]) array([ 0.12144298, -0.3010051 ,  0.39642458,  0.85877534])</p> Source code in <code>specular\\calculation.py</code> <pre><code>def gradient(\n    f: Callable[[list | np.ndarray], int |float | np.number],\n    x: list | np.ndarray,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; np.ndarray:\n    \"\"\"\n    Approximates the specular gradient of a real-valued function :math:`f:\\\\mathbb{R}^n \\\\to \\\\mathbb{R}` at point ``x`` for ``n &gt; 1``.\n\n    The specular gradient is defined as the vector of all partial specular derivatives along the standard basis directions.\n\n    Parameters\n    ----------\n    f : callable\n        A real-valued function defined on :math:`\\\\mathbb{R}^n`.\n    x : list | np.ndarray\n        The point at which the specular gradient is evaluated.\n    h : float, optional\n        Mesh size used in the finite difference approximation. Must be positive.\n        Default: ``1e-6``.\n    zero_tol : float, optional\n        A small threshold used to determine if the denominator ``alpha + beta`` is close to zero for numerical stability.\n        Default: ``1e-8``.\n\n    Returns\n    -------\n    np.ndarray\n        The approximated specular gradient of ``f`` at ``x`` as a vector.\n\n    Raises\n    ------\n    TypeError\n        If ``x`` is not of a valid array-like type.\n    ValueError\n        If ``f`` does not return a scalar value.\n        If ``h`` is not positive.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import specular\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; f = lambda x: np.linalg.norm(x)\n    &gt;&gt;&gt; specular.gradient(f, x=[1.4, -3.47, 4.57, 9.9])\n    array([ 0.12144298, -0.3010051 ,  0.39642458,  0.85877534])\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    x = np.asarray(x, dtype=float)\n\n    if x.ndim != 1:\n        raise TypeError(\n            f\"Input 'x' must be a vector. \"\n            f\"Got {type(x).__name__} with shape {x.shape}. \"\n            \"Use `specular.derivative` for scalar inputs.\"\n        )\n\n    n = x.size\n    identity = np.eye(n)\n\n    f_val = f(x)\n\n    if np.ndim(f_val) != 0:\n        raise ValueError(\n            \"Function f must return a scalar value. \"\n            f\"Got shape {np.shape(f_val)}.\"\n        )\n\n    x_right = x + h*identity\n    x_left = x - h*identity\n\n    f_right = np.array([f(row) for row in x_right], dtype=float)\n    f_left = np.array([f(row) for row in x_left], dtype=float)\n\n    alpha = (f_right - f_val) / h \n    beta = (f_val - f_left) / h \n\n    return _A_vector(alpha, beta, zero_tol=zero_tol) # type: ignore\n</code></pre>"},{"location":"api/calculation/#specular.calculation.jacobian","title":"<code>jacobian(f, x, h=1e-06, zero_tol=1e-08)</code>","text":"<p>Approximates the specular Jacobian matrix of a vector-valued function :math:<code>f:\\mathbb{R}^n \\to \\mathbb{R}^m</code>.</p> <p>Returns a matrix of shape (m, n) where J[j, i] is the partial derivative of f_j with respect to x_i (1 &lt;= i &lt;= n, 1 &lt;= j &lt;= m).</p>"},{"location":"api/calculation/#specular.calculation.jacobian--raises","title":"Raises","text":"<p>TypeError     If <code>x</code> is not of a valid array-like type. ValueError     If <code>h</code> is not positive.</p> Source code in <code>specular\\calculation.py</code> <pre><code>def jacobian(\n    f: Callable[[list | np.ndarray], int | float | np.number | list | np.ndarray],\n    x: list | np.ndarray,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; np.ndarray:\n    \"\"\"\n    Approximates the specular Jacobian matrix of a vector-valued function :math:`f:\\\\mathbb{R}^n \\\\to \\\\mathbb{R}^m`.\n\n    Returns a matrix of shape (m, n) where J[j, i] is the partial derivative of f_j with respect to x_i (1 &lt;= i &lt;= n, 1 &lt;= j &lt;= m).\n\n    Raises\n    ------\n    TypeError\n        If ``x`` is not of a valid array-like type.\n    ValueError\n        If ``h`` is not positive.\n    \"\"\"\n    if h &lt;= 0:\n        raise ValueError(f\"Mesh size 'h' must be positive. Got {h}\")\n\n    x = np.asarray(x, dtype=float)\n\n    if x.ndim != 1:\n        raise TypeError(\n            f\"Input 'x' must be a vector. \"\n            f\"Got {type(x).__name__} with shape {x.shape}. \"\n            \"Use `specular.derivative` for scalar inputs.\"\n        )\n\n    n = x.size\n\n    f_val = np.asarray(f(x), dtype=float)\n\n    if f_val.ndim == 0:\n        f_val = f_val.reshape(1)\n\n    m = f_val.size\n\n    identity = np.eye(n)\n\n    x_right = x + h * identity\n    x_left = x - h * identity\n\n    f_right = np.array([f(row) for row in x_right], dtype=float)\n    f_left = np.array([f(row) for row in x_left], dtype=float)\n\n    f_right = f_right.reshape(n, m)\n    f_left = f_left.reshape(n, m)\n\n    alpha = (f_right - f_val) / h\n    beta = (f_val - f_left) / h\n\n    J_transposed = _A_vector(alpha, beta, zero_tol=zero_tol)\n\n    return J_transposed.T\n</code></pre>"},{"location":"api/calculation/#specular.calculation.partial_derivative","title":"<code>partial_derivative(f, x, i, h=1e-06, zero_tol=1e-08)</code>","text":"<p>Approximates the i-th specular partial derivative of a real-valued function :math:<code>f:\\mathbb{R}^n \\to \\mathbb{R}</code> at point <code>x</code> for <code>n &gt; 1</code>.</p> <p>This is computed using :func:<code>specular_directional_derivative</code> with the direction of the <code>i</code>-th standard basis vector of :math:<code>\\mathbb{R}^n</code>.</p>"},{"location":"api/calculation/#specular.calculation.partial_derivative--parameters","title":"Parameters","text":"<p>f : callable     A real-valued function defined on :math:<code>\\mathbb{R}^n</code>.     <code>n</code> is the dimension of <code>x</code>. x : list | np.ndarray     The point at which the derivative is evaluated. i : int | np.integer     The index of the specular partial derivative with respect to :math:<code>x_i</code> (<code>1 &lt;= i &lt;= n</code>). h : float, optional     Mesh size used in the finite difference approximation. Must be positive.     Default: <code>1e-6</code>. zero_tol : float, optional     A small threshold used to determine if the denominator <code>alpha + beta</code> is close to zero for numerical stability.     Default: <code>1e-8</code>.</p>"},{"location":"api/calculation/#specular.calculation.partial_derivative--returns","title":"Returns","text":"<p>float     The approximated <code>i</code>-th partial specular derivative of <code>f</code> at <code>x</code> as a scalar.</p>"},{"location":"api/calculation/#specular.calculation.partial_derivative--raises","title":"Raises","text":"<p>TypeError     If <code>i</code> is not an integer. ValueError     If <code>i</code> is out of the valid range (<code>1 &lt;= i &lt;= n</code>).</p>"},{"location":"api/calculation/#specular.calculation.partial_derivative--examples","title":"Examples","text":"<p>import specular import math  f = lambda x: math.sqrt(x[0]2 + x[1]2 + x[2]**2) specular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=2) 0.8859268982863702</p> Source code in <code>specular\\calculation.py</code> <pre><code>def partial_derivative(\n    f: Callable[[list | np.ndarray], int |float | np.number],\n    x: list | np.ndarray,\n    i: int | np.integer,\n    h: float = 1e-6,\n    zero_tol: float = 1e-8\n) -&gt; float:\n    \"\"\"\n    Approximates the i-th specular partial derivative of a real-valued function :math:`f:\\\\mathbb{R}^n \\\\to \\\\mathbb{R}` at point ``x`` for ``n &gt; 1``.\n\n    This is computed using :func:`specular_directional_derivative` with the direction of the ``i``-th standard basis vector of :math:`\\\\mathbb{R}^n`.\n\n    Parameters\n    ----------\n    f : callable\n        A real-valued function defined on :math:`\\\\mathbb{R}^n`.\n        ``n`` is the dimension of ``x``.\n    x : list | np.ndarray\n        The point at which the derivative is evaluated.\n    i : int | np.integer\n        The index of the specular partial derivative with respect to :math:`x_i` (``1 &lt;= i &lt;= n``).\n    h : float, optional\n        Mesh size used in the finite difference approximation. Must be positive.\n        Default: ``1e-6``.\n    zero_tol : float, optional\n        A small threshold used to determine if the denominator ``alpha + beta`` is close to zero for numerical stability.\n        Default: ``1e-8``.\n\n    Returns\n    -------\n    float\n        The approximated ``i``-th partial specular derivative of ``f`` at ``x`` as a scalar.\n\n    Raises\n    ------\n    TypeError\n        If ``i`` is not an integer.\n    ValueError\n        If ``i`` is out of the valid range (``1 &lt;= i &lt;= n``).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import specular\n    &gt;&gt;&gt; import math \n    &gt;&gt;&gt; f = lambda x: math.sqrt(x[0]**2 + x[1]**2 + x[2]**2)\n    &gt;&gt;&gt; specular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=2)\n    0.8859268982863702\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n\n    if not isinstance(i, (int, np.integer)):\n        raise TypeError(f\"Index 'i' must be an integer. Got {type(i).__name__}\")\n\n    n = x.size\n    if i &lt; 1 or i &gt; n:\n        raise ValueError(f\"Index 'i' must be between 1 and {n} (dimension of x). Got {i}\")\n\n    e_i = np.zeros_like(x)\n    e_i[i - 1] = 1.0\n\n    return directional_derivative(f, x, e_i, h, zero_tol)\n</code></pre>"},{"location":"api/calculation/#211-one-dimensional-euclidean-space-n1","title":"2.1.1 One-dimensional Euclidean Space (\\(n=1\\))","text":"<p>In \\(\u211d\\), the specular derivative can be calculated using the function <code>derivative</code>.</p> <pre><code>import specular\n\ndef f(x):\n    return max(x, 0.0)\n\nspecular.derivative(f, x=0.0)\n# Output: 0.41421356237309515\n</code></pre>"},{"location":"api/calculation/#212-the-n-dimensional-euclidean-space-n1","title":"2.1.2 the \\(n\\)-dimensional Euclidean space (\\(n&gt;1\\))","text":"<p>In \\(\u211d^n\\), the specular directional derivative of a function \\(f: \u211d^n \\to \u211d\\) at a point \\(x \\in \u211d^n\\) in the direction \\(v \\in \u211d^n\\) can be calculated using the function <code>directional_derivative</code>.</p> <pre><code>import specular\nimport math \n\nf = lambda x: math.sqrt(x[0]**2 + x[1]**2 + x[2]**2)\nspecular.directional_derivative(f, x=[0.0, 0.1, -0.1], v=[1.0, -1.0, 2.0])\n# Output: -2.1213203434708223\n</code></pre> <p>Let \\(e_1, e_2, \\ldots, e_n\\) be the standard basis of \\(\u211d^n\\). For each \\(i \\in \u2115\\) with \\(1 \\leq i \\leq n\\), the specular partial derivative with respect to a variable \\(x_i\\) can be calculated using the function <code>partial_derivative</code>, which yields the same result as <code>directional_derivative</code> with direction \\(v=e_i\\).</p> <pre><code>import specular\nimport math\n\ndef f(x):\n    return math.sqrt(x[0]**2 + x[1]**2 + x[2]**2)\n\nspecular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=2)\n# Output: 0.8859268982863702\nspecular.directional_derivative(f, x=[0.1, 2.3, -1.2], v=[0.0, 1.0, 0.0])\n# Output: 0.8859268982863702\n</code></pre> <p>Also, the specular gradient can be calculated using <code>gradient</code>.</p> <pre><code>import specular\nimport numpy as np\n\ndef f(x):\n    return np.linalg.norm(x)\n\nspecular.gradient(f, x=[0.1, 2.3, -1.2])\n# Output: [ 0.03851856  0.8859269  -0.46222273]\nspecular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=1)\n# Output: 0.03851856078540371\nspecular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=2)\n# Output: 0.8859268982863702\nspecular.partial_derivative(f, x=[0.1, 2.3, -1.2], i=3)\n# Output: -0.4622227292028128\n</code></pre>"},{"location":"api/jax/","title":"2.4. JAX backend","text":"<p>See the official homepage of JAX.</p>"},{"location":"api/jax/#241-for-now","title":"2.4.1 For now","text":"<ul> <li>This feature is currently experimental and undergoing verification.</li> <li>Benchmarks indicate significant speedups compared to the NumPy backend.</li> <li>Full GPU support and optimization are planned but not yet finalized.</li> <li>Requirement: Your objective function must use <code>jax.numpy</code> instead of standard <code>numpy</code> to avoid errors.</li> </ul>"},{"location":"api/jax/#242-why-jax","title":"2.4.2 Why JAX?","text":"<ul> <li>The JAX is chosen as the primary acceleration backend due to its high compatibility and similar syntax to NumPy. </li> <li>The core calculation logic is planned to be ported to other backends like PyTorch or TensorFlow to provide native GPU/TPU support and broader ecosystem integration.</li> </ul>"},{"location":"api/jax/#243-example","title":"2.4.3 Example","text":"<p>For a detailed comparison of the algorithms, please refer to the following scripts:</p> <ul> <li> <p><code>docs/api/jax_example.py</code>: A basic implementation using the JAX backend.</p> </li> <li> <p><code>examples/optimization/2026-Jung/main.py</code>: The full experimental setup used in the paper.</p> </li> </ul> <p>Note</p> <p>Preliminary tests show that the computation time is shorter than BFGS; however, the exact theoretical reasons for this performance gain are still being investigated and have not yet been fully proven.</p>"},{"location":"api/ode/","title":"2.2. Ordinary differential equations","text":""},{"location":"api/ode/#221-specular-euler-scheme","title":"2.2.1 Specular Euler scheme","text":"<p>All functions return an instance of the <code>ODEResult</code> class that encapsulates the numerical results.</p> <pre><code>import specular\n\ndef F(t, u):\n    return -2*u \n\nspecular.Euler_scheme(of_Type='1', F=F, t_0=0.0, u_0=1.0, T=2.5, h=0.1)\n# Output: Running the specular Euler scheme of Type 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00&lt;?, ?it/s]\n# Output: &lt;specular.ode.result.ODEResult at 0x1765982d8d0&gt;\n</code></pre> <p>To access the numerical results, call <code>.history()</code>. It returns a tuple containing the time grid and the numerical solution.</p> <pre><code>import specular\n\ndef F(t, u):\n    return -2*u \n\nspecular.Euler_scheme(of_Type=1, F=F, t_0=0.0, u_0=1.0, T=2.5, h=0.1).history()\n# Output: Running the specular Euler scheme of Type 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00&lt;?, ?it/s]\n# Output: (array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2,\n#        1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5]),\n# Output:  array([1.        , 0.8       , 0.62169432, 0.48101574, 0.37172557,\n#        0.2870388 , 0.22149069, 0.17081087, 0.13166787, 0.1014624 ,\n#        0.07816953, 0.06021577, 0.04638162, 0.0357239 , 0.02751427,\n#        0.02119088, 0.01632056, 0.0125695 , 0.00968054, 0.00745555,\n#        0.00574195, 0.00442221, 0.00340579, 0.00262299, 0.00202011,\n#        0.0015558 ]))\n</code></pre> <p>To visualize the numerical results, call <code>.visualization()</code>.</p> <pre><code>import specular\nimport numpy as np\n\ndef F(t, u):\n   return -2*u \n\ndef exact_sol(t):\n    return np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nspecular.Euler_scheme(of_Type='1', F=F, t_0=0.0, u_0=u_0, T=2.5, h=0.1).visualization(exact_sol=exact_sol, save_path=\"specular-Euler-scheme-of-Type-1\")\n# Output: Running the specular Euler scheme of Type 1: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00&lt;?, ?it/s]\n# Output: Figure saved: figures\\specular-Euler-scheme-of-Type-1\n</code></pre> <p></p> <p>To obtain the table of the numerical results, call <code>.table()</code>. </p> <pre><code>import specular\nimport numpy as np\n\ndef F(t, u):\n    return -2*u\n\ndef exact_sol(t):\n    return np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nspecular.Euler_scheme(of_Type=4, F=F, t_0=0.0, u_0=u_0, T=2.5, h=0.1).table(exact_sol=exact_sol, save_path=\"specular-Euler-scheme-of-type-4\")\n# Output: Running the specular Euler scheme of Type 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00&lt;?, ?it/s]\n# Output: Table saved: tables\\specular-Euler-scheme-of-type-4.csv\n</code></pre> <p><code>.visualization()</code> and <code>.table()</code> are are chainable.</p> <pre><code>import specular\nimport numpy as np\n\ndef F(t, u):\n    return -2*u\n\ndef exact_sol(t):\nreturn np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nspecular.Euler_scheme(of_Type=4, F=F, t_0=0.0, u_0=u_0, T=2.5, h=0.1).visualization(exact_sol=exact_sol).table(exact_sol=exact_sol)\n# Output: Running the specular Euler scheme of Type 4: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:00&lt;?, ?it/s]\n</code></pre> <p>To compute the total error of the numerical results, call <code>.total_error()</code>. The exact solution is required. The norm can be <code>max</code>, <code>l1</code>, or <code>l2</code>.</p> <pre><code>def F(t, u):\n    return -2*u \n\ndef exact_sol(t):\n    return np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nspecular.Euler_scheme(of_Type=5, F=F, t_0=0.0, u_0=u_0, T=10.0, h=0.1).total_error(exact_sol=exact_sol, norm='max')\n# Output: Running the specular Euler scheme of Type 5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 300882.64it/s]\n# Output: 0.0011409613137273178\n</code></pre>"},{"location":"api/ode/#222-specular-trigonometric-scheme","title":"2.2.2 Specular trigonometric scheme","text":"<pre><code>import specular\n\ndef F(t, u):\n    return -2*u \n\ndef exact_sol(t):\n    return np.exp(-2*t)\n\ndef u_0(t_0):\n    return exact_sol(t_0)\n\nu_1 = exact_sol(t_0 + h)\n\nspecular.trigonometric_scheme(F=F, t_0=0.0, u_0=u_0, u_1=u_1, T=2.5, h=0.1).visualization(exact_sol=exact_sol, save_path=\"specular-trigonometric\")\n# Output: Running specular trigonometric scheme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00&lt;?, ?it/s]\n# Output: Figure saved: figures\\specular-trigonometric\n</code></pre>"},{"location":"api/ode/#223-classical-schemes","title":"2.2.3 Classical schemes","text":"<p>The three classical schemes are available: the explicit Euler, the implicit Euler, and the Crank-Nicolson schemes.</p> <pre><code>import specular\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef F(t, u):\n    return -(t*u)/(1-t**2)\ndef exact_sol(t):\n    return np.sqrt(1 - t**2)\ndef u_0(t_0):\n    return exact_sol(t_0)\nt_0 = 0.0\nT = 0.9\nh = 0.05\n\nresult_EE = specular.ode.solver.classical_scheme(F=F, t_0=t_0, u_0=u_0, T=T, h=h, scheme=\"explicit Euler\").history()\nresult_IE = specular.ode.solver.classical_scheme(F=F, t_0=t_0, u_0=u_0, T=T, h=h, scheme=\"implicit Euler\").history()\nresult_CN = specular.ode.solver.classical_scheme(F=F, t_0=t_0, u_0=u_0, T=T, h=h, scheme=\"Crank-Nicolson\").history()\nexact_values = np.array([exact_sol(t) for t in result_EE[0]])\n\nplt.figure(figsize=(5.5, 2.5))\n\nplt.plot(result_EE[0], exact_values, color='black', label='Exact solution')\nplt.plot(result_EE[0], result_EE[1],  marker='x', linestyle='None', markerfacecolor='none', markeredgecolor='red', label='Explicit Euler') \nplt.plot(result_IE[0], result_IE[1],  marker='x', linestyle='None', markerfacecolor='none', markeredgecolor='blue', label='Implicit Euler') \nplt.plot(result_CN[0], result_CN[1],  marker='x', linestyle='None', markerfacecolor='none', markeredgecolor='purple', label='Crank-Nicolson')\n\nplt.xlabel(r\"Time\", fontsize=10)\nplt.ylabel(r\"Solution\", fontsize=10)\nplt.grid(True)\nplt.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), borderaxespad=0., fontsize=10)\nplt.savefig('figures/classical-schemes.png', dpi=1000, bbox_inches='tight')\nplt.show()\n# Output: Running the explicit Euler scheme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00&lt;?, ?it/s]\n# Output: Running the implicit Euler scheme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00&lt;?, ?it/s]\n# Output: Running Crank-Nicolson scheme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [00:00&lt;00:00, 17988.44it/s]\n</code></pre> <p></p>"},{"location":"api/optimization/","title":"2.3. Optimization","text":"<p>Step size rules for optimization methods.</p> Source code in <code>specular\\optimization\\step_size.py</code> <pre><code>class StepSize:\n    \"\"\"\n    Step size rules for optimization methods.\n    \"\"\"\n    __options__ = [\n        'constant',\n        'not_summable',\n        'square_summable_not_summable',\n        'geometric_series',\n        'user_defined'\n    ]\n\n    def __init__(\n        self,\n        name: str,\n        parameters: float | np.floating | int | Tuple | list | np.ndarray | Callable\n    ):\n        \"\"\"\n        The step size rules for optimization methods:\n\n        :math:`x_{k+1} = x_k - h_k s_k`,\n\n        where :math:`s_k` is the search direction and :math:`h_k &gt; 0` is the step size at iteration `k &gt;= 1`.\n\n        Parameters\n        ----------\n        name : str\n            Options: 'constant', 'not_summable', 'square_summable_not_summable', 'geometric_series', 'user_defined'\n        parameters : float | int | tuple | list | np.ndarray | Callable\n            The parameters required for the selected step size rule:\n\n            * 'constant': float or int\n\n                A number `a &gt; 0` for the rule :math:`h_k = a` for each `k`.\n\n            * 'not_summable': float or int\n\n                A number `a &gt; 0` for the rule :math:`h_k = a / sqrt{k}` for each `k`.\n\n            * 'square_summable_not_summable': list or tuple\n\n                A pair of numbers `[a, b]`, where `a &gt; 0` and `b &gt;= 0`, for the rule :math:`h_k = a / (b + k)` for each `k`.\n\n            * 'geometric_series': list or tuple\n\n                A pair of numbers `[a, r]`, where `a &gt; 0` and `0 &lt; r &lt; 1`, for the rule :math:`h_k = a * r^k` for each `k`.\n\n            * 'user_defined': Callable\n\n                A function that takes the current iteration `k` as input and returns the step size (float).\n\n        Examples\n        --------\n        &gt;&gt;&gt; from specular.optimization.step_size import StepSize\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'constant': h_k = a\n        &gt;&gt;&gt; step = StepSize(name='constant', parameters=0.5)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'not_summable' rule: h_k = a / sqrt(k)\n        &gt;&gt;&gt; # a = 2.0\n        &gt;&gt;&gt; step = StepSize(name='not_summable', parameters=2.0)\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'square_summable_not_summable' rule: h_k = a / (b + k\n        &gt;&gt;&gt; # a = 10, b = 2\n        &gt;&gt;&gt; step = StepSize(name='square_summable_not_summable', parameters=[10.0, 2.0])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'geometric_series' rule: h_k = a * r^k\n        &gt;&gt;&gt; # a = 1.0, r = 0.5\n        &gt;&gt;&gt; step = StepSize(name='geometric_series', parameters=[1.0, 0.5])\n        &gt;&gt;&gt; \n        &gt;&gt;&gt; # 'user_defined' callable.\n        &gt;&gt;&gt; # Custom rule: h_k = 1 / k^2\n        &gt;&gt;&gt; custom_rule = lambda k: 1.0 / (k**2)\n        &gt;&gt;&gt; step = StepSize(name='user_defined', parameters=custom_rule)\n        \"\"\"\n        self.step_size = name\n        self.parameters = parameters\n\n        init_methods = {\n            'constant': self._init_constant,\n            'not_summable': self._init_not_summable,\n            'square_summable_not_summable': self._init_square_summable,\n            'geometric_series': self._init_geometric,\n            'user_defined': self._init_user_defined\n        }\n\n        if name not in init_methods:\n             raise ValueError(f\"Invalid step size '{name}'. Options: {self.__options__}\")\n\n        init_methods[name]()\n\n    def __call__(self, k: int) -&gt; float:\n        \"\"\"\n        Returns the step size at iteration k.\n        \"\"\"\n        return self._rule(k)\n\n    # ==== Initialization Methods ====\n    def _init_constant(self):\n        if not isinstance(self.parameters, (float, int, np.floating)):\n            raise TypeError(f\"Invalid type: number required. Got {type(self.parameters)}\")\n\n        if self.parameters &lt;= 0:\n            raise ValueError(f\"Invalid value: positive number required. Got {self.parameters}\")\n\n        self.a = float(self.parameters)\n        self._rule = self._calc_constant\n\n    def _init_not_summable(self):\n        if not isinstance(self.parameters, (float, int, np.floating)):\n            raise TypeError(f\"Invalid type: number required. Got {type(self.parameters)}\")\n\n        if self.parameters &lt;= 0:\n            raise ValueError(f\"Invalid value: positive number required. Got {self.parameters}\")\n\n        self.a = float(self.parameters)\n        self._rule = self._calc_not_summable\n\n    def _init_square_summable(self):\n        if not isinstance(self.parameters, (tuple, list, np.ndarray)):\n            raise TypeError(f\"Invalid type: list/tuple required. Got {type(self.parameters)}\")\n\n        if len(self.parameters) != 2:\n            raise ValueError(f\"Invalid length: 2 parameters [a, b] required. Got {len(self.parameters)}\")\n\n        self.a, self.b = self.parameters[0], self.parameters[1]\n\n        if self.a &lt;= 0 or self.b &lt; 0:\n            raise ValueError(f\"Invalid parameters: a &gt; 0 and b &gt;= 0 required. Got a={self.a}, b={self.b}\")\n\n        self._rule = self._calc_square_summable_not_summable\n\n    def _init_geometric(self):\n        if not isinstance(self.parameters, (tuple, list, np.ndarray)):\n            raise TypeError(f\"Invalid type: list/tuple required. Got {type(self.parameters)}\")\n\n        if len(self.parameters) != 2:\n            raise ValueError(f\"Invalid length: 2 parameters [a, r] required. Got {len(self.parameters)}\")\n\n        self.a, self.r = self.parameters[0], self.parameters[1]\n\n        if self.a &lt;= 0 or not (0.0 &lt; self.r &lt; 1.0):\n            raise ValueError(f\"Invalid parameters: a &gt; 0 and 0 &lt; r &lt; 1 required. Got a={self.a}, r={self.r}\")\n\n        self._rule = self._calc_geometric_series\n\n    def _init_user_defined(self):\n        if not callable(self.parameters):\n            raise TypeError(\"Invalid type: callable function required.\")\n\n        self._rule = self.parameters\n\n    # ==== Calculation Methods ====\n    def _calc_constant(self, k: int) -&gt; float:\n        \"\"\"\n        h_k = a \n        \"\"\"\n        return self.a\n\n    def _calc_not_summable(self, k: int) -&gt; float:\n        \"\"\"\n        h_k = a / sqrt{k}\n        \"\"\"\n        return self.a / math.sqrt(k)\n\n    def _calc_square_summable_not_summable(self, k: int) -&gt; float:\n        \"\"\"\n        h_k = a / (b + k)\n        \"\"\"\n        return self.a / (self.b + k)\n\n    def _calc_geometric_series(self, k: int) -&gt; float:\n        \"\"\"\n        h_k = a * r**k\n        \"\"\"\n        return self.a * (self.r ** k)\n</code></pre>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__","title":"<code>__init__(name, parameters)</code>","text":"<p>The step size rules for optimization methods:</p> <p>:math:<code>x_{k+1} = x_k - h_k s_k</code>,</p> <p>where :math:<code>s_k</code> is the search direction and :math:<code>h_k &gt; 0</code> is the step size at iteration <code>k &gt;= 1</code>.</p>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--parameters","title":"Parameters","text":"<p>name : str     Options: 'constant', 'not_summable', 'square_summable_not_summable', 'geometric_series', 'user_defined' parameters : float | int | tuple | list | np.ndarray | Callable     The parameters required for the selected step size rule:</p> <pre><code>* 'constant': float or int\n\n    A number `a &gt; 0` for the rule :math:`h_k = a` for each `k`.\n\n* 'not_summable': float or int\n\n    A number `a &gt; 0` for the rule :math:`h_k = a / sqrt{k}` for each `k`.\n\n* 'square_summable_not_summable': list or tuple\n\n    A pair of numbers `[a, b]`, where `a &gt; 0` and `b &gt;= 0`, for the rule :math:`h_k = a / (b + k)` for each `k`.\n\n* 'geometric_series': list or tuple\n\n    A pair of numbers `[a, r]`, where `a &gt; 0` and `0 &lt; r &lt; 1`, for the rule :math:`h_k = a * r^k` for each `k`.\n\n* 'user_defined': Callable\n\n    A function that takes the current iteration `k` as input and returns the step size (float).\n</code></pre>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--examples","title":"Examples","text":"<p>from specular.optimization.step_size import StepSize</p> Source code in <code>specular\\optimization\\step_size.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    parameters: float | np.floating | int | Tuple | list | np.ndarray | Callable\n):\n    \"\"\"\n    The step size rules for optimization methods:\n\n    :math:`x_{k+1} = x_k - h_k s_k`,\n\n    where :math:`s_k` is the search direction and :math:`h_k &gt; 0` is the step size at iteration `k &gt;= 1`.\n\n    Parameters\n    ----------\n    name : str\n        Options: 'constant', 'not_summable', 'square_summable_not_summable', 'geometric_series', 'user_defined'\n    parameters : float | int | tuple | list | np.ndarray | Callable\n        The parameters required for the selected step size rule:\n\n        * 'constant': float or int\n\n            A number `a &gt; 0` for the rule :math:`h_k = a` for each `k`.\n\n        * 'not_summable': float or int\n\n            A number `a &gt; 0` for the rule :math:`h_k = a / sqrt{k}` for each `k`.\n\n        * 'square_summable_not_summable': list or tuple\n\n            A pair of numbers `[a, b]`, where `a &gt; 0` and `b &gt;= 0`, for the rule :math:`h_k = a / (b + k)` for each `k`.\n\n        * 'geometric_series': list or tuple\n\n            A pair of numbers `[a, r]`, where `a &gt; 0` and `0 &lt; r &lt; 1`, for the rule :math:`h_k = a * r^k` for each `k`.\n\n        * 'user_defined': Callable\n\n            A function that takes the current iteration `k` as input and returns the step size (float).\n\n    Examples\n    --------\n    &gt;&gt;&gt; from specular.optimization.step_size import StepSize\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # 'constant': h_k = a\n    &gt;&gt;&gt; step = StepSize(name='constant', parameters=0.5)\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # 'not_summable' rule: h_k = a / sqrt(k)\n    &gt;&gt;&gt; # a = 2.0\n    &gt;&gt;&gt; step = StepSize(name='not_summable', parameters=2.0)\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # 'square_summable_not_summable' rule: h_k = a / (b + k\n    &gt;&gt;&gt; # a = 10, b = 2\n    &gt;&gt;&gt; step = StepSize(name='square_summable_not_summable', parameters=[10.0, 2.0])\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # 'geometric_series' rule: h_k = a * r^k\n    &gt;&gt;&gt; # a = 1.0, r = 0.5\n    &gt;&gt;&gt; step = StepSize(name='geometric_series', parameters=[1.0, 0.5])\n    &gt;&gt;&gt; \n    &gt;&gt;&gt; # 'user_defined' callable.\n    &gt;&gt;&gt; # Custom rule: h_k = 1 / k^2\n    &gt;&gt;&gt; custom_rule = lambda k: 1.0 / (k**2)\n    &gt;&gt;&gt; step = StepSize(name='user_defined', parameters=custom_rule)\n    \"\"\"\n    self.step_size = name\n    self.parameters = parameters\n\n    init_methods = {\n        'constant': self._init_constant,\n        'not_summable': self._init_not_summable,\n        'square_summable_not_summable': self._init_square_summable,\n        'geometric_series': self._init_geometric,\n        'user_defined': self._init_user_defined\n    }\n\n    if name not in init_methods:\n         raise ValueError(f\"Invalid step size '{name}'. Options: {self.__options__}\")\n\n    init_methods[name]()\n</code></pre>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--constant-h_k-a","title":"'constant': h_k = a","text":"<p>step = StepSize(name='constant', parameters=0.5)</p>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--not_summable-rule-h_k-a-sqrtk","title":"'not_summable' rule: h_k = a / sqrt(k)","text":""},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--a-20","title":"a = 2.0","text":"<p>step = StepSize(name='not_summable', parameters=2.0)</p>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--square_summable_not_summable-rule-h_k-a-b-k","title":"'square_summable_not_summable' rule: h_k = a / (b + k","text":""},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--a-10-b-2","title":"a = 10, b = 2","text":"<p>step = StepSize(name='square_summable_not_summable', parameters=[10.0, 2.0])</p>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--geometric_series-rule-h_k-a-rk","title":"'geometric_series' rule: h_k = a * r^k","text":""},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--a-10-r-05","title":"a = 1.0, r = 0.5","text":"<p>step = StepSize(name='geometric_series', parameters=[1.0, 0.5])</p>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--user_defined-callable","title":"'user_defined' callable.","text":""},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__init__--custom-rule-h_k-1-k2","title":"Custom rule: h_k = 1 / k^2","text":"<p>custom_rule = lambda k: 1.0 / (k**2) step = StepSize(name='user_defined', parameters=custom_rule)</p>"},{"location":"api/optimization/#specular.optimization.step_size.StepSize.__call__","title":"<code>__call__(k)</code>","text":"<p>Returns the step size at iteration k.</p> Source code in <code>specular\\optimization\\step_size.py</code> <pre><code>def __call__(self, k: int) -&gt; float:\n    \"\"\"\n    Returns the step size at iteration k.\n    \"\"\"\n    return self._rule(k)\n</code></pre>"},{"location":"api/optimization/#quick-example","title":"Quick Example","text":"<pre><code>from specular.optimization.step_size import StepSize\n\n# Use a square-summable rule: h_k = 10 / (2 + k)\nstep = StepSize(name='square_summable_not_summable', parameters=[10.0, 2.0])\nh_1 = step(1)\nprint(h_1)\n# Output: 3.3333333333333335\n</code></pre>"},{"location":"api/optimization/#232-the-specular-gradient-method","title":"2.3.2 The specular gradient method","text":""},{"location":"api/optimization/#the-one-dimensional-case","title":"The one dimensional case","text":"<pre><code>import specular \n\n# Objective function: f(x) = |x|\ndef f(x):\n    return abs(x)\n\nstep_size = specular.StepSize('constant', 0.1) \n\n# Specular gradient method\nres = specular.gradient_method(f=f, x_0=1.0, step_size=step_size, form='specular gradient', max_iter=20)\n</code></pre>"},{"location":"api/optimization/#higher-dimensional-cases","title":"Higher dimensional cases","text":"<pre><code>import specular \n\n# Objective function: f(x) = sum(x^2)\ndef f(x):\n    return float(np.sum(np.array(x)**2))\n\n# Component functions for Stochastic test\n# f(x) = x1^2 + x2^2\n# f1(x) = x1^2, f2(x) = x2^2\ndef f_comp_1(x):\n    return x[0]**2\n\ndef f_comp_2(x):\n    return x[1]**2\n\nf_components = [f_comp_1, f_comp_2]\n\nx_0 = [1.0, 1.0]\nstep_size = specular.StepSize('square_summable_not_summable', [0.5, 1.0]) \n\n# Specular gradient method\nres1 = specular.gradient_method(f=f, x_0=x_0, step_size=step_size, form='specular gradient', max_iter=50)\n\n# Stochastic specular gradient method\nres2 = specular.gradient_method(f=f, x_0=x_0, step_size=step_size, form='stochastic', f_j=f_components, max_iter=100)\n\n# hybrid specular gradient method\nres3 = specular.gradient_method(f=f_quad_vector, x_0=x_0, step_size=step_size, form='hybrid', f_j=f_components, switch_iter=5, max_iter=20)\n</code></pre>"},{"location":"api/optimization/#233-optimizationresult","title":"2.3.3 <code>OptimizationResult</code>","text":"<p>The class <code>OptimizationResult</code> collects the optimization results. To get history of optimization, call <code>history()</code>.</p> <pre><code>import specular \n\n# Objective function: f(x) = sum(x^2)\ndef f(x):\n    return float(np.sum(np.array(x)**2))\n\nx_0 = [1.0, 1.0]\nstep_size = specular.StepSize('square_summable_not_summable', [0.5, 1.0]) \n\n# Specular gradient method\nres_x, res_f, res_time = specular.gradient_method(f=f, x_0=x_0, step_size=step_size, form='specular gradient', max_iter=50).history()\n</code></pre>"},{"location":"examples/","title":"3. Examples","text":"<p>This directory includes applications of specular differentiation. Each subdirectory is based on a paper.</p>"},{"location":"examples/#31-ordinary-differential-equation","title":"3.1. Ordinary differential equation","text":"<ul> <li>Directory: <code>examples/ode/</code></li> </ul>"},{"location":"examples/#2026-jung","title":"2026-Jung","text":"<ul> <li>Directory: <code>examples/ode/2026-Jung/</code></li> <li>Keywords: generalized differentiation, Mean Value Theorem, Fermat's Theorem, explicit Euler scheme, implicit Euler scheme, Crank-Nicolson scheme</li> <li>Reference: TBA</li> </ul>"},{"location":"examples/#32-optimization","title":"3.2. Optimization","text":"<ul> <li>Directory: <code>examples/optimization/</code></li> <li>Requirements: </li> <li><code>scipy</code> &gt;= 1.10.0</li> <li><code>torch</code> &gt;= 2.0.0</li> </ul>"},{"location":"examples/#2024-jung-oh","title":"2024-Jung-Oh","text":"<ul> <li>Directory: <code>examples/optimization/2024-Jung-Oh/</code></li> <li>Keywords: nonsmooth convex optimization, subgradient methods, non-differentiable convex functions, generalization of derivatives, convergence rate</li> <li>Reference: K. Jung and J. Oh. Nonsmooth convex optimization using the specular gradient method with root-linear convergence. arXiv preprint arXiv:2210.06933, 2024</li> </ul>"},{"location":"examples/#2026-jung_1","title":"2026-Jung","text":"<ul> <li>Directory: <code>examples/optimization/2026-Jung/</code></li> <li>Keywords: TBA</li> <li>Reference: TBA</li> </ul>"}]}